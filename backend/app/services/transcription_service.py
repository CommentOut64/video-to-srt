"""
è½¬å½•å¤„ç†æœåŠ¡
æ•´åˆäº†processor.pyå’ŒåŸtranscription_service.pyçš„æ‰€æœ‰åŠŸèƒ½
"""
import os, subprocess, uuid, threading, json, math, gc, logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from pydub import AudioSegment, silence
import whisperx
import torch
import shutil

from models.job_models import JobSettings, JobState
from models.hardware_models import HardwareInfo, OptimizationConfig
from services.hardware_service import get_hardware_detector, get_hardware_optimizer
from services.cpu_affinity_service import CPUAffinityManager, CPUAffinityConfig
from services.job_index_service import get_job_index_service
from core.config import config  # å¯¼å…¥ç»Ÿä¸€é…ç½®

# å…¨å±€æ¨¡å‹ç¼“å­˜ (æŒ‰ (model, compute_type, device) é”®)
_model_cache: Dict[Tuple[str, str, str], object] = {}
_align_model_cache: Dict[str, Tuple[object, object]] = {}
_model_lock = threading.Lock()
_align_lock = threading.Lock()


class TranscriptionService:
    """
    è½¬å½•å¤„ç†æœåŠ¡
    æ•´åˆäº†æ‰€æœ‰è½¬å½•ç›¸å…³åŠŸèƒ½
    """

    def __init__(self, jobs_root: str):
        """
        åˆå§‹åŒ–è½¬å½•æœåŠ¡

        Args:
            jobs_root: ä»»åŠ¡å·¥ä½œç›®å½•æ ¹è·¯å¾„
        """
        self.jobs_root = Path(jobs_root)
        self.jobs_root.mkdir(parents=True, exist_ok=True)

        self.jobs: Dict[str, JobState] = {}
        self.lock = threading.Lock()
        self.logger = logging.getLogger(__name__)

        # é›†æˆCPUäº²å’Œæ€§ç®¡ç†å™¨
        self.cpu_manager = CPUAffinityManager()

        # é›†æˆç¡¬ä»¶æ£€æµ‹
        self.hardware_detector = get_hardware_detector()
        self.hardware_optimizer = get_hardware_optimizer()
        self._hardware_info: Optional[HardwareInfo] = None
        self._optimization_config: Optional[OptimizationConfig] = None

        # é›†æˆä»»åŠ¡ç´¢å¼•æœåŠ¡
        self.job_index = get_job_index_service(jobs_root)
        # å¯åŠ¨æ—¶æ¸…ç†æ— æ•ˆæ˜ å°„
        self.job_index.cleanup_invalid_mappings()

        # è®°å½•CPUä¿¡æ¯
        sys_info = self.cpu_manager.get_system_info()
        if sys_info.get('supported', False):
            self.logger.info(
                f"ğŸ’» CPUä¿¡æ¯: {sys_info['logical_cores']}ä¸ªé€»è¾‘æ ¸å¿ƒ, "
                f"{sys_info.get('physical_cores', '?')}ä¸ªç‰©ç†æ ¸å¿ƒ, "
                f"å¹³å°: {sys_info.get('platform', '?')}"
            )
        else:
            self.logger.warning("âš ï¸ CPUäº²å’Œæ€§åŠŸèƒ½ä¸å¯ç”¨")

        # æ‰§è¡Œç¡¬ä»¶æ£€æµ‹
        self._detect_hardware()

    def _detect_hardware(self):
        """æ‰§è¡Œç¡¬ä»¶æ£€æµ‹å¹¶ç”Ÿæˆä¼˜åŒ–é…ç½®"""
        try:
            self.logger.info("å¼€å§‹ç¡¬ä»¶æ£€æµ‹...")
            self._hardware_info = self.hardware_detector.detect()
            self._optimization_config = self.hardware_optimizer.get_optimization_config(self._hardware_info)
            
            # è®°å½•æ£€æµ‹ç»“æœ
            hw = self._hardware_info
            opt = self._optimization_config
            self.logger.info(f"ç¡¬ä»¶æ£€æµ‹å®Œæˆ - GPU: {'âœ“' if hw.cuda_available else 'âœ—'}, "
                           f"CPU: {hw.cpu_cores}æ ¸/{hw.cpu_threads}çº¿ç¨‹, "
                           f"å†…å­˜: {hw.memory_total_mb}MB, "
                           f"ä¼˜åŒ–é…ç½®: batch={opt.batch_size}, device={opt.recommended_device}")
        except Exception as e:
            self.logger.error(f"ç¡¬ä»¶æ£€æµ‹å¤±è´¥: {e}")
    
    def get_hardware_info(self) -> Optional[HardwareInfo]:
        """è·å–ç¡¬ä»¶ä¿¡æ¯"""
        return self._hardware_info
    
    def get_optimization_config(self) -> Optional[OptimizationConfig]:
        """è·å–ä¼˜åŒ–é…ç½®"""  
        return self._optimization_config
    
    def get_optimized_job_settings(self, base_settings: Optional[JobSettings] = None) -> JobSettings:
        """è·å–åŸºäºç¡¬ä»¶ä¼˜åŒ–çš„ä»»åŠ¡è®¾ç½®"""
        # ä½¿ç”¨ç¡¬ä»¶ä¼˜åŒ–é…ç½®ä½œä¸ºé»˜è®¤å€¼
        if self._optimization_config:
            optimized = JobSettings(
                model=base_settings.model if base_settings else "medium",
                compute_type=base_settings.compute_type if base_settings else "float16",
                device=self._optimization_config.recommended_device,
                batch_size=self._optimization_config.batch_size,
                word_timestamps=base_settings.word_timestamps if base_settings else False
            )
            return optimized
        
        # å¦‚æœæ²¡æœ‰ç¡¬ä»¶ä¿¡æ¯ï¼Œä½¿ç”¨ä¼ å…¥çš„è®¾ç½®æˆ–é»˜è®¤è®¾ç½®
        return base_settings or JobSettings()

    def create_job(
        self,
        filename: str,
        src_path: str,
        settings: JobSettings,
        job_id: Optional[str] = None
    ) -> JobState:
        """
        åˆ›å»ºè½¬å½•ä»»åŠ¡

        Args:
            filename: æ–‡ä»¶å
            src_path: æºæ–‡ä»¶è·¯å¾„
            settings: ä»»åŠ¡è®¾ç½®
            job_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰

        Returns:
            JobState: åˆ›å»ºçš„ä»»åŠ¡çŠ¶æ€å¯¹è±¡
        """
        job_id = job_id or uuid.uuid4().hex
        job_dir = self.jobs_root / job_id
        job_dir.mkdir(parents=True, exist_ok=True)

        dest_path = job_dir / filename

        # å¤åˆ¶æ–‡ä»¶åˆ°ä»»åŠ¡ç›®å½•
        if os.path.abspath(src_path) != os.path.abspath(dest_path):
            try:
                shutil.copyfile(src_path, dest_path)
                self.logger.debug(f"æ–‡ä»¶å·²å¤åˆ¶: {src_path} -> {dest_path}")
            except Exception as e:
                self.logger.warning(f"æ–‡ä»¶å¤åˆ¶å¤±è´¥: {e}")

        # åˆ›å»ºä»»åŠ¡çŠ¶æ€å¯¹è±¡
        job = JobState(
            job_id=job_id,
            filename=filename,
            dir=str(job_dir),
            input_path=src_path,
            settings=settings,
            status="uploaded",
            phase="pending",
            message="æ–‡ä»¶å·²ä¸Šä¼ "
        )

        with self.lock:
            self.jobs[job_id] = job

        # æ·»åŠ æ–‡ä»¶è·¯å¾„åˆ°ä»»åŠ¡IDçš„æ˜ å°„
        self.job_index.add_mapping(src_path, job_id)

        self.logger.info(f"âœ… ä»»åŠ¡å·²åˆ›å»º: {job_id} - {filename}")
        return job

    def get_job(self, job_id: str) -> Optional[JobState]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            Optional[JobState]: ä»»åŠ¡çŠ¶æ€å¯¹è±¡ï¼Œä¸å­˜åœ¨åˆ™è¿”å›None
        """
        with self.lock:
            return self.jobs.get(job_id)

    def scan_incomplete_jobs(self) -> List[Dict]:
        """
        æ‰«ææ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ˆæœ‰checkpoint.jsonçš„ä»»åŠ¡ï¼‰

        Returns:
            List[Dict]: æœªå®Œæˆä»»åŠ¡åˆ—è¡¨
        """
        incomplete_jobs = []

        try:
            # éå†æ‰€æœ‰ä»»åŠ¡ç›®å½•
            for job_dir in self.jobs_root.iterdir():
                if not job_dir.is_dir():
                    continue

                checkpoint_path = job_dir / "checkpoint.json"
                if not checkpoint_path.exists():
                    continue

                try:
                    # åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®
                    with open(checkpoint_path, 'r', encoding='utf-8') as f:
                        checkpoint_data = json.load(f)

                    job_id = checkpoint_data.get('job_id') or job_dir.name
                    total_segments = checkpoint_data.get('total_segments', 0)
                    processed_indices = checkpoint_data.get('processed_indices', [])
                    processed_count = len(processed_indices)

                    # è®¡ç®—è¿›åº¦
                    if total_segments > 0:
                        progress = (processed_count / total_segments) * 100
                    else:
                        progress = 0

                    # ä»ç´¢å¼•ä¸­æŸ¥æ‰¾æ–‡ä»¶å
                    file_path = self.job_index.get_file_path(job_id)
                    filename = os.path.basename(file_path) if file_path else "æœªçŸ¥æ–‡ä»¶"

                    incomplete_jobs.append({
                        'job_id': job_id,
                        'filename': filename,
                        'file_path': file_path,  # æ·»åŠ æ–‡ä»¶è·¯å¾„
                        'progress': round(progress, 2),
                        'processed_segments': processed_count,
                        'total_segments': total_segments,
                        'phase': checkpoint_data.get('phase', 'unknown'),
                        'dir': str(job_dir)
                    })

                except Exception as e:
                    self.logger.warning(f"è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥ {checkpoint_path}: {e}")
                    continue

            self.logger.info(f"æ‰«æåˆ° {len(incomplete_jobs)} ä¸ªæœªå®Œæˆä»»åŠ¡")
            return incomplete_jobs

        except Exception as e:
            self.logger.error(f"æ‰«ææœªå®Œæˆä»»åŠ¡å¤±è´¥: {e}")
            return []

    def restore_job_from_checkpoint(self, job_id: str) -> Optional[JobState]:
        """
        ä»æ£€æŸ¥ç‚¹æ¢å¤ä»»åŠ¡çŠ¶æ€

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            Optional[JobState]: æ¢å¤çš„ä»»åŠ¡çŠ¶æ€å¯¹è±¡
        """
        job_dir = self.jobs_root / job_id
        if not job_dir.exists():
            return None

        checkpoint = self._load_checkpoint(job_dir)
        if not checkpoint:
            return None

        try:
            # æŸ¥æ‰¾åŸæ–‡ä»¶
            filename = "unknown"
            input_path = None

            # ä»ç›®å½•ä¸­æŸ¥æ‰¾è§†é¢‘/éŸ³é¢‘æ–‡ä»¶
            for ext in ['.mp4', '.avi', '.mkv', '.mov', '.flv', '.wmv', '.mp3', '.wav', '.m4a']:
                matches = list(job_dir.glob(f"*{ext}"))
                if matches:
                    filename = matches[0].name
                    input_path = str(matches[0])
                    break

            if not input_path:
                self.logger.warning(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡ {job_id} çš„è¾“å…¥æ–‡ä»¶")
                return None

            # åˆ›å»ºé»˜è®¤çš„CPUäº²å’Œæ€§é…ç½®
            from services.cpu_affinity_service import CPUAffinityConfig
            default_cpu_config = CPUAffinityConfig(
                enabled=True,
                strategy="auto",
                custom_cores=None,
                exclude_cores=None
            )

            # åˆ›å»ºä»»åŠ¡çŠ¶æ€å¯¹è±¡
            job = JobState(
                job_id=job_id,
                filename=filename,
                dir=str(job_dir),
                input_path=input_path,
                settings=JobSettings(cpu_affinity=default_cpu_config),  # æä¾›é»˜è®¤çš„cpu_affinity
                status="paused",
                phase=checkpoint.get('phase', 'pending'),
                message=f"å·²æš‚åœ ({len(checkpoint.get('processed_indices', []))}/{checkpoint.get('total_segments', 0)}æ®µ)",
                total=checkpoint.get('total_segments', 0),
                processed=len(checkpoint.get('processed_indices', [])),
                progress=round((len(checkpoint.get('processed_indices', [])) / max(1, checkpoint.get('total_segments', 1))) * 100, 2)
            )

            with self.lock:
                self.jobs[job_id] = job

            self.logger.info(f"âœ… ä»æ£€æŸ¥ç‚¹æ¢å¤ä»»åŠ¡: {job_id}")
            return job

        except Exception as e:
            self.logger.error(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ä»»åŠ¡å¤±è´¥: {e}")
            return None

    def check_file_checkpoint(self, file_path: str) -> Optional[Dict]:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å¯ç”¨çš„æ–­ç‚¹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            Optional[Dict]: æ–­ç‚¹ä¿¡æ¯ï¼Œæ— æ–­ç‚¹åˆ™è¿”å›None
        """
        # ä»ç´¢å¼•ä¸­æŸ¥æ‰¾ä»»åŠ¡ID
        job_id = self.job_index.get_job_id(file_path)
        if not job_id:
            return None

        # æ£€æŸ¥ä»»åŠ¡ç›®å½•å’Œcheckpointæ˜¯å¦å­˜åœ¨
        job_dir = self.jobs_root / job_id
        if not job_dir.exists():
            # æ¸…ç†æ— æ•ˆæ˜ å°„
            self.job_index.remove_mapping(file_path)
            return None

        checkpoint = self._load_checkpoint(job_dir)
        if not checkpoint:
            return None

        # è¿”å›æ–­ç‚¹ä¿¡æ¯
        total_segments = checkpoint.get('total_segments', 0)
        processed_indices = checkpoint.get('processed_indices', [])
        processed_count = len(processed_indices)

        if total_segments > 0:
            progress = (processed_count / total_segments) * 100
        else:
            progress = 0

        return {
            'job_id': job_id,
            'progress': round(progress, 2),
            'processed_segments': processed_count,
            'total_segments': total_segments,
            'phase': checkpoint.get('phase', 'unknown'),
            'can_resume': True
        }

    def start_job(self, job_id: str):
        """
        å¯åŠ¨è½¬å½•ä»»åŠ¡ï¼ˆæ”¯æŒä»pausedçŠ¶æ€æ¢å¤ï¼‰

        Args:
            job_id: ä»»åŠ¡ID
        """
        job = self.get_job(job_id)
        if not job or job.status not in ("uploaded", "failed", "paused"):
            self.logger.warning(f"ä»»åŠ¡æ— æ³•å¯åŠ¨: {job_id}, çŠ¶æ€: {job.status if job else 'not found'}")
            return

        job.canceled = False
        job.paused = False  # æ¸…é™¤æš‚åœæ ‡å¿—
        job.error = None
        job.status = "processing"
        job.message = "å¼€å§‹å¤„ç†" if job.status != "paused" else "æ¢å¤å¤„ç†"

        # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­æ‰§è¡Œè½¬å½•
        threading.Thread(
            target=self._run_pipeline,
            args=(job,),
            daemon=True,
            name=f"Transcription-{job_id[:8]}"
        ).start()

        self.logger.info(f"ğŸš€ ä»»åŠ¡å·²å¯åŠ¨: {job_id}")

    def pause_job(self, job_id: str) -> bool:
        """
        æš‚åœè½¬å½•ä»»åŠ¡ï¼ˆä¿å­˜æ–­ç‚¹ï¼‰

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸè®¾ç½®æš‚åœæ ‡å¿—
        """
        job = self.get_job(job_id)
        if not job:
            return False

        job.paused = True
        job.message = "æš‚åœä¸­..."
        self.logger.info(f"â¸ï¸ ä»»åŠ¡æš‚åœè¯·æ±‚: {job_id}")
        return True

    def cancel_job(self, job_id: str, delete_data: bool = False) -> bool:
        """
        å–æ¶ˆè½¬å½•ä»»åŠ¡

        Args:
            job_id: ä»»åŠ¡ID
            delete_data: æ˜¯å¦åˆ é™¤ä»»åŠ¡æ•°æ®

        Returns:
            bool: æ˜¯å¦æˆåŠŸè®¾ç½®å–æ¶ˆæ ‡å¿—
        """
        job = self.get_job(job_id)
        if not job:
            return False

        job.canceled = True
        job.message = "å–æ¶ˆä¸­..."
        self.logger.info(f"ğŸ›‘ ä»»åŠ¡å–æ¶ˆè¯·æ±‚: {job_id}, åˆ é™¤æ•°æ®: {delete_data}")

        # å¦‚æœéœ€è¦åˆ é™¤æ•°æ®
        if delete_data:
            try:
                job_dir = Path(job.dir)
                # ç§»é™¤æ–‡ä»¶è·¯å¾„æ˜ å°„
                if job.input_path:
                    self.job_index.remove_mapping(job.input_path)

                if job_dir.exists():
                    # åˆ é™¤æ•´ä¸ªä»»åŠ¡ç›®å½•
                    shutil.rmtree(job_dir)
                    self.logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤ä»»åŠ¡æ•°æ®: {job_id}")
                    # ä»å†…å­˜ä¸­ç§»é™¤ä»»åŠ¡
                    with self.lock:
                        if job_id in self.jobs:
                            del self.jobs[job_id]
            except Exception as e:
                self.logger.error(f"åˆ é™¤ä»»åŠ¡æ•°æ®å¤±è´¥: {e}")

        return True

    def _update_progress(
        self,
        job: JobState,
        phase: str,
        phase_ratio: float,
        message: str = ""
    ):
        """
        æ›´æ–°ä»»åŠ¡è¿›åº¦

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            phase: å½“å‰é˜¶æ®µ (extract/split/transcribe/srt)
            phase_ratio: å½“å‰é˜¶æ®µå®Œæˆæ¯”ä¾‹ (0.0-1.0)
            message: è¿›åº¦æ¶ˆæ¯
        """
        job.phase = phase

        # ä½¿ç”¨é…ç½®ä¸­çš„è¿›åº¦æƒé‡
        phase_weights = config.PHASE_WEIGHTS
        total_weight = config.TOTAL_WEIGHT

        # è®¡ç®—ç´¯è®¡è¿›åº¦
        done_weight = 0
        for p, w in phase_weights.items():
            if p == phase:
                break
            done_weight += w

        current_weight = phase_weights.get(phase, 0) * max(0.0, min(1.0, phase_ratio))
        job.progress = round((done_weight + current_weight) / total_weight * 100, 2)

        if message:
            job.message = message

    def _save_checkpoint(self, job_dir: Path, data: dict):
        """
        åŸå­æ€§ä¿å­˜æ£€æŸ¥ç‚¹
        ä½¿ç”¨"å†™ä¸´æ—¶æ–‡ä»¶ -> é‡å‘½å"ç­–ç•¥ï¼Œç¡®ä¿æ–‡ä»¶è¦ä¹ˆå®Œæ•´å†™å…¥ï¼Œè¦ä¹ˆä¿æŒåŸæ ·

        Args:
            job_dir: ä»»åŠ¡ç›®å½•
            data: æ£€æŸ¥ç‚¹æ•°æ®
        """
        checkpoint_path = job_dir / "checkpoint.json"
        temp_path = checkpoint_path.with_suffix(".tmp")

        try:
            # 1. å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            # 2. åŸå­æ›¿æ¢ï¼ˆWindows/Linux/macOS å‡æ”¯æŒï¼‰
            # å¦‚æœç¨‹åºåœ¨è¿™é‡Œå´©æºƒï¼Œcheckpoint.json ä¾ç„¶æ˜¯æ—§ç‰ˆæœ¬ï¼Œä¸ä¼šæŸå
            os.replace(temp_path, checkpoint_path)

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            # ä¿å­˜å¤±è´¥ä¸åº”ä¸­æ–­ä¸»æµç¨‹ï¼Œä»…è®°å½•æ—¥å¿—

    def _load_checkpoint(self, job_dir: Path) -> Optional[dict]:
        """
        åŠ è½½æ£€æŸ¥ç‚¹ï¼Œå¦‚æœæ–‡ä»¶æŸååˆ™è¿”å› None

        Args:
            job_dir: ä»»åŠ¡ç›®å½•

        Returns:
            Optional[dict]: æ£€æŸ¥ç‚¹æ•°æ®ï¼Œä¸å­˜åœ¨æˆ–æŸååˆ™è¿”å› None
        """
        checkpoint_path = job_dir / "checkpoint.json"
        if not checkpoint_path.exists():
            return None

        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, OSError) as e:
            self.logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶æŸåï¼Œå°†é‡æ–°å¼€å§‹ä»»åŠ¡: {checkpoint_path} - {e}")
            return None

    def _run_pipeline(self, job: JobState):
        """
        æ‰§è¡Œè½¬å½•å¤„ç†ç®¡é“ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
        """
        # åº”ç”¨CPUäº²å’Œæ€§è®¾ç½®
        cpu_applied = False
        if job.settings.cpu_affinity and job.settings.cpu_affinity.enabled:
            cpu_applied = self.cpu_manager.apply_cpu_affinity(
                job.settings.cpu_affinity
            )
            if cpu_applied:
                self.logger.info(f"ğŸ“Œ ä»»åŠ¡ {job.job_id} å·²åº”ç”¨CPUäº²å’Œæ€§è®¾ç½®")

        try:
            # æ£€æŸ¥å–æ¶ˆå’Œæš‚åœæ ‡å¿—
            if job.canceled:
                job.status = 'canceled'
                job.message = 'å·²å–æ¶ˆ'
                return

            if job.paused:
                job.status = 'paused'
                job.message = 'å·²æš‚åœ'
                self.logger.info(f"â¸ï¸ ä»»åŠ¡å·²æš‚åœ: {job.job_id}")
                return

            job_dir = Path(job.dir)
            input_path = job_dir / job.filename
            audio_path = job_dir / 'audio.wav'

            # ==========================================
            # 1. å°è¯•æ¢å¤çŠ¶æ€ï¼ˆæ–­ç‚¹ç»­ä¼ æ ¸å¿ƒï¼‰
            # ==========================================
            checkpoint = self._load_checkpoint(job_dir)

            # åˆå§‹åŒ–å†…å­˜çŠ¶æ€
            processed_indices = set()
            processed_results = []
            current_segments = []

            if checkpoint:
                self.logger.info(f"ğŸ”„ å‘ç°æ£€æŸ¥ç‚¹ï¼Œä» {checkpoint.get('phase', 'unknown')} é˜¶æ®µæ¢å¤")
                # æ¢å¤æ•°æ®åˆ°å†…å­˜
                processed_indices = set(checkpoint.get('processed_indices', []))
                processed_results = checkpoint.get('results', [])
                current_segments = checkpoint.get('segments', [])
                # æ¢å¤ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
                job.total = checkpoint.get('total_segments', 0)
                job.processed = len(processed_indices)
                self.logger.info(f"ğŸ“Š å·²å¤„ç† {job.processed}/{job.total} æ®µ")

            # ==========================================
            # 2. é˜¶æ®µ1: æå–éŸ³é¢‘
            # ==========================================
            # åªæœ‰å½“éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…ä»å¤´å¼€å§‹æ—¶ï¼Œæ‰æ‰§è¡Œæå–
            if not audio_path.exists() or (checkpoint is None):
                self._update_progress(job, 'extract', 0, 'æå–éŸ³é¢‘ä¸­')
                if job.canceled:
                    raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

                if not self._extract_audio(str(input_path), str(audio_path)):
                    raise RuntimeError('FFmpeg æå–éŸ³é¢‘å¤±è´¥')

                self._update_progress(job, 'extract', 1, 'éŸ³é¢‘æå–å®Œæˆ')
            else:
                self.logger.info("âœ… è·³è¿‡éŸ³é¢‘æå–ï¼Œä½¿ç”¨å·²æœ‰æ–‡ä»¶")

            if job.canceled:
                raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

            # ==========================================
            # 3. é˜¶æ®µ2: æ™ºèƒ½åˆ†æ®µ
            # ==========================================
            # å¦‚æœæ£€æŸ¥ç‚¹é‡Œæ²¡æœ‰åˆ†æ®µä¿¡æ¯ï¼Œè¯´æ˜ä¸Šæ¬¡æ²¡è·‘åˆ°åˆ†æ®µå®Œæˆ
            if not current_segments:
                self._update_progress(job, 'split', 0, 'éŸ³é¢‘åˆ†æ®µä¸­')
                current_segments = self._split_audio(str(audio_path))
                if job.canceled:
                    raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

                job.segments = current_segments
                job.total = len(current_segments)
                self._update_progress(job, 'split', 1, f'åˆ†æ®µå®Œæˆ å…±{job.total}æ®µ')

                # ã€å…³é”®åŸ‹ç‚¹1ã€‘åˆ†æ®µå®Œæˆåç«‹å³ä¿å­˜
                checkpoint_data = {
                    "job_id": job.job_id,
                    "phase": "split",
                    "total_segments": job.total,
                    "processed_indices": [],
                    "segments": current_segments,  # ä¿å­˜åˆ†æ®µç»“æœ
                    "results": []
                }
                self._save_checkpoint(job_dir, checkpoint_data)
                self.logger.info("ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: åˆ†æ®µå®Œæˆ")
            else:
                self.logger.info(f"âœ… è·³è¿‡åˆ†æ®µï¼Œä½¿ç”¨æ£€æŸ¥ç‚¹æ•°æ®ï¼ˆå…±{len(current_segments)}æ®µï¼‰")
                job.segments = current_segments  # æ¢å¤åˆ° job å¯¹è±¡
                job.total = len(current_segments)

            # ==========================================
            # 4. é˜¶æ®µ3: è½¬å½•å¤„ç†ï¼ˆæ ¸å¿ƒå¾ªç¯ï¼‰
            # ==========================================
            self._update_progress(job, 'transcribe', 0, 'åŠ è½½æ¨¡å‹ä¸­')
            if job.canceled:
                raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

            model = self._get_model(job.settings, job)
            align_cache = {}

            # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„æ®µ
            todo_segments = [
                seg for i, seg in enumerate(current_segments)
                if i not in processed_indices
            ]

            self.logger.info(f"ğŸ“ å‰©ä½™ {len(todo_segments)}/{len(current_segments)} æ®µéœ€è¦è½¬å½•")

            for idx, seg in enumerate(current_segments):
                # å¦‚æœå·²ç»åœ¨ processed_indices é‡Œï¼Œç›´æ¥è·³è¿‡
                if idx in processed_indices:
                    self.logger.debug(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ®µ {idx}")
                    continue

                # æ£€æŸ¥å–æ¶ˆå’Œæš‚åœæ ‡å¿—
                if job.canceled:
                    raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

                if job.paused:
                    raise RuntimeError('ä»»åŠ¡å·²æš‚åœ')

                ratio = len(processed_indices) / max(1, len(current_segments))
                self._update_progress(
                    job,
                    'transcribe',
                    ratio,
                    f'è½¬å½• {len(processed_indices)+1}/{len(current_segments)}'
                )

                seg_result = self._transcribe_segment(seg, model, job, align_cache)

                # --- æ›´æ–°å†…å­˜çŠ¶æ€ ---
                if seg_result:
                    processed_results.append(seg_result)
                processed_indices.add(idx)
                job.processed = len(processed_indices)

                # --- æ›´æ–°è¿›åº¦æ¡ ---
                progress = len(processed_indices) / len(current_segments)
                self._update_progress(
                    job,
                    'transcribe',
                    progress,
                    f'è½¬å½•ä¸­ {len(processed_indices)}/{len(current_segments)}'
                )

                # ã€å…³é”®åŸ‹ç‚¹2ã€‘æ¯å¤„ç†ä¸€æ®µä¿å­˜ä¸€æ¬¡
                # å•æœºç‰ˆæ¯æ®µä¿å­˜å¼€é”€å¾ˆå°ï¼Œå»ºè®®ç›´æ¥æ¯æ®µä¿å­˜ï¼Œä½“éªŒæœ€å¥½
                checkpoint_data = {
                    "job_id": job.job_id,
                    "phase": "transcribe",
                    "total_segments": len(current_segments),
                    "processed_indices": list(processed_indices),  # setè½¬list
                    "segments": current_segments,
                    "results": processed_results
                }
                self._save_checkpoint(job_dir, checkpoint_data)
                self.logger.debug(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {len(processed_indices)}/{len(current_segments)}")

            self._update_progress(job, 'transcribe', 1, 'è½¬å½•å®Œæˆ ç”Ÿæˆå­—å¹•ä¸­')
            if job.canceled:
                raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

            # ==========================================
            # 5. é˜¶æ®µ4: ç”ŸæˆSRT
            # ==========================================
            base_name = os.path.splitext(job.filename)[0]
            srt_path = job_dir / f'{base_name}.srt'
            self._update_progress(job, 'srt', 0, 'å†™å…¥ SRT...')
            self._generate_srt(
                processed_results,
                str(srt_path),
                job.settings.word_timestamps
            )
            self._update_progress(job, 'srt', 1, 'å¤„ç†å®Œæˆ')

            job.srt_path = str(srt_path)

            # ã€æ¸…ç†ã€‘ä»»åŠ¡æˆåŠŸå®Œæˆåï¼Œåˆ é™¤ checkpoint
            try:
                checkpoint_file = job_dir / "checkpoint.json"
                checkpoint_file.unlink(missing_ok=True)
                self.logger.info("ğŸ§¹ æ£€æŸ¥ç‚¹å·²æ¸…ç†")
            except Exception as e:
                self.logger.warning(f"æ¸…ç†æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

            if job.canceled:
                job.status = 'canceled'
                job.message = 'å·²å–æ¶ˆ'
            else:
                job.status = 'finished'
                job.message = 'å®Œæˆ'
                self.logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {job.job_id}")

        except Exception as e:
            if job.canceled and 'å–æ¶ˆ' in str(e):
                job.status = 'canceled'
                job.message = 'å·²å–æ¶ˆ'
                self.logger.info(f"ğŸ›‘ ä»»åŠ¡å·²å–æ¶ˆ: {job.job_id}")
            elif job.paused and 'æš‚åœ' in str(e):
                job.status = 'paused'
                job.message = 'å·²æš‚åœ'
                self.logger.info(f"â¸ï¸ ä»»åŠ¡å·²æš‚åœ: {job.job_id}")
            else:
                job.status = 'failed'
                job.message = f'å¤±è´¥: {e}'
                job.error = str(e)
                self.logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {job.job_id} - {e}", exc_info=True)

        finally:
            # æ¢å¤CPUäº²å’Œæ€§è®¾ç½®
            if cpu_applied:
                restored = self.cpu_manager.restore_cpu_affinity()
                if restored:
                    self.logger.info(f"ğŸ”„ ä»»åŠ¡ {job.job_id} å·²æ¢å¤CPUäº²å’Œæ€§è®¾ç½®")

            # é‡Šæ”¾å†…å­˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    # ========== æ ¸å¿ƒå¤„ç†æ–¹æ³• ==========

    def _extract_audio(self, input_file: str, audio_out: str) -> bool:
        """
        ä½¿ç”¨FFmpegæå–éŸ³é¢‘

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            audio_out: è¾“å‡ºéŸ³é¢‘è·¯å¾„

        Returns:
            bool: æ˜¯å¦æå–æˆåŠŸ
        """
        if os.path.exists(audio_out):
            self.logger.debug(f"éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æå–: {audio_out}")
            return True

        # ä½¿ç”¨é…ç½®ä¸­çš„FFmpegå‘½ä»¤ï¼ˆæ”¯æŒç‹¬ç«‹æ‰“åŒ…ï¼‰
        ffmpeg_cmd = config.get_ffmpeg_command()
        self.logger.debug(f"ä½¿ç”¨FFmpeg: {ffmpeg_cmd}")

        cmd = [
            ffmpeg_cmd, '-y', '-i', input_file,
            '-vn',                    # ä»…éŸ³é¢‘
            '-ac', '1',               # å•å£°é“
            '-ar', '16000',           # 16kHz é‡‡æ ·ç‡
            '-acodec', 'pcm_s16le',   # PCM ç¼–ç 
            audio_out
        ]

        try:
            proc = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )

            if proc.returncode == 0 and os.path.exists(audio_out):
                self.logger.debug(f"âœ… éŸ³é¢‘æå–æˆåŠŸ: {audio_out}")
                return True
            else:
                error_msg = proc.stderr.decode('utf-8', errors='ignore')
                self.logger.error(f"âŒ FFmpegæ‰§è¡Œå¤±è´¥: {error_msg}")
                return False

        except subprocess.TimeoutExpired:
            self.logger.error("âŒ FFmpegè¶…æ—¶")
            return False
        except Exception as e:
            self.logger.error(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
            return False

    def _split_audio(self, audio_path: str) -> List[Dict]:
        """
        æ™ºèƒ½åˆ†æ®µéŸ³é¢‘ï¼ˆåŸºäºé™éŸ³æ£€æµ‹ï¼‰

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            List[Dict]: æ®µä¿¡æ¯åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« file å’Œ start_ms
        """
        self.logger.debug(f"å¼€å§‹éŸ³é¢‘åˆ†æ®µ: {audio_path}")

        # ä½¿ç”¨é…ç½®ä¸­çš„éŸ³é¢‘å¤„ç†å‚æ•°
        audio_config = config.get_audio_config()
        SEGMENT_LEN_MS = audio_config['segment_length_ms']
        SILENCE_SEARCH_MS = audio_config['silence_search_ms']
        MIN_SILENCE_LEN_MS = audio_config['min_silence_len_ms']
        SILENCE_THRESH_DBFS = audio_config['silence_threshold_dbfs']

        audio = AudioSegment.from_wav(audio_path)
        length = len(audio)
        segments = []
        pos = 0
        idx = 0

        while pos < length:
            end = min(pos + SEGMENT_LEN_MS, length)

            # æ™ºèƒ½å¯»æ‰¾é™éŸ³ç‚¹ï¼ˆé¿å…åœ¨å¥å­ä¸­é—´åˆ†å‰²ï¼‰
            if end < length and (end - pos) > SILENCE_SEARCH_MS:
                search_start = max(pos, end - SILENCE_SEARCH_MS)
                search_chunk = audio[search_start:end]

                try:
                    silences = silence.detect_silence(
                        search_chunk,
                        min_silence_len=MIN_SILENCE_LEN_MS,
                        silence_thresh=SILENCE_THRESH_DBFS
                    )

                    if silences:
                        # ä½¿ç”¨ç¬¬ä¸€ä¸ªé™éŸ³ç‚¹
                        silence_start = silences[0][0]
                        new_end = search_start + silence_start
                        if new_end - pos > MIN_SILENCE_LEN_MS:
                            end = new_end
                except Exception as e:
                    self.logger.warning(f"é™éŸ³æ£€æµ‹å¤±è´¥: {e}")

            # å¯¼å‡ºåˆ†æ®µ
            chunk = audio[pos:end]
            seg_file = os.path.join(os.path.dirname(audio_path), f'segment_{idx}.wav')
            chunk.export(seg_file, format='wav')

            segments.append({
                'file': seg_file,
                'start_ms': pos,
                'duration_ms': end - pos
            })

            pos = end
            idx += 1

        self.logger.debug(f"âœ… éŸ³é¢‘åˆ†æ®µå®Œæˆ: å…±{len(segments)}æ®µ")
        return segments

    def _get_model(self, settings: JobSettings, job: Optional[JobState] = None):
        """
        è·å–WhisperXæ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰

        ä¼˜å…ˆä½¿ç”¨æ¨¡å‹ç®¡ç†æœåŠ¡æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨ç®€å•ç¼“å­˜

        Args:
            settings: ä»»åŠ¡è®¾ç½®
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡(å¯é€‰,ç”¨äºæ›´æ–°ä¸‹è½½è¿›åº¦)

        Returns:
            æ¨¡å‹å¯¹è±¡
        """
        # å°è¯•ä½¿ç”¨æ¨¡å‹ç®¡ç†æœåŠ¡æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹
        try:
            from services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()
            whisper_model_info = model_mgr.whisper_models.get(settings.model)

            if whisper_model_info:
                # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
                if whisper_model_info.status == "not_downloaded" or whisper_model_info.status == "incomplete":
                    self.logger.warning(f"âš ï¸ Whisperæ¨¡å‹æœªä¸‹è½½æˆ–ä¸å®Œæ•´: {settings.model}")

                    # è·å–æ¨¡å‹å¤§å°ä¿¡æ¯
                    model_size_mb = whisper_model_info.size_mb

                    # å¦‚æœæ¨¡å‹å¤§å°>=1GB,ç»™å‡ºç‰¹æ®Šæç¤º
                    download_msg = ""
                    if model_size_mb >= 1024:
                        size_gb = model_size_mb / 1024
                        download_msg = f"å½“å‰ä¸‹è½½æ¨¡å‹å¤§äº1GB ({size_gb:.1f}GB),è¯·è€å¿ƒç­‰å¾…"
                        self.logger.info(f"ğŸ“¦ {download_msg}")
                    else:
                        download_msg = f"å¼€å§‹ä¸‹è½½æ¨¡å‹ {settings.model} ({model_size_mb}MB)"

                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    if job:
                        job.message = download_msg

                    self.logger.info(f"ğŸš€ è‡ªåŠ¨è§¦å‘ä¸‹è½½Whisperæ¨¡å‹: {settings.model} ({model_size_mb}MB)")

                    # è§¦å‘ä¸‹è½½
                    success = model_mgr.download_whisper_model(settings.model)
                    if not success:
                        self.logger.warning(f"âš ï¸ æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥æˆ–å·²åœ¨ä¸‹è½½ä¸­,å›é€€åˆ°whisperx")
                        raise RuntimeError("æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥")

                    # ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾…10åˆ†é’Ÿï¼‰
                    import time
                    max_wait_time = 600  # 10åˆ†é’Ÿ
                    wait_interval = 5  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                    elapsed = 0

                    while elapsed < max_wait_time:
                        time.sleep(wait_interval)
                        elapsed += wait_interval

                        current_status = model_mgr.whisper_models[settings.model].status
                        progress = model_mgr.whisper_models[settings.model].download_progress

                        if current_status == "ready":
                            self.logger.info(f"âœ… Whisperæ¨¡å‹ä¸‹è½½å®Œæˆ: {settings.model}")
                            if job:
                                job.message = f"æ¨¡å‹ä¸‹è½½å®Œæˆ,å‡†å¤‡åŠ è½½"
                            break
                        elif current_status == "error":
                            self.logger.error(f"âŒ æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥,å›é€€åˆ°whisperx")
                            raise RuntimeError(f"Whisperæ¨¡å‹ä¸‹è½½å¤±è´¥: {settings.model}")
                        else:
                            # å¦‚æœæ¨¡å‹å¤§å°>=1GB,å®šæœŸæé†’ç”¨æˆ·è€å¿ƒç­‰å¾…
                            if model_size_mb >= 1024 and elapsed % 30 == 0:  # æ¯30ç§’æé†’ä¸€æ¬¡
                                wait_msg = f"å½“å‰ä¸‹è½½æ¨¡å‹å¤§äº1GB,è¯·è€å¿ƒç­‰å¾…... {progress:.1f}% ({elapsed}s/{max_wait_time}s)"
                                self.logger.info(f"â³ {wait_msg}")
                                if job:
                                    job.message = wait_msg
                            else:
                                wait_msg = f"ç­‰å¾…æ¨¡å‹ä¸‹è½½... {progress:.1f}%"
                                self.logger.info(f"â³ {wait_msg} ({elapsed}s/{max_wait_time}s)")
                                # æ›´æ–°ä»»åŠ¡çŠ¶æ€(æ¯æ¬¡éƒ½æ›´æ–°,è¿™æ ·ç”¨æˆ·å¯ä»¥çœ‹åˆ°è¿›åº¦å˜åŒ–)
                                if job:
                                    job.message = wait_msg

                    if elapsed >= max_wait_time:
                        self.logger.error(f"âŒ æ¨¡å‹ä¸‹è½½è¶…æ—¶,å›é€€åˆ°whisperx")
                        raise TimeoutError(f"Whisperæ¨¡å‹ä¸‹è½½è¶…æ—¶: {settings.model}")

        except Exception as e:
            self.logger.warning(f"âš ï¸ æ¨¡å‹ç®¡ç†æœåŠ¡æ£€æŸ¥å¤±è´¥,å›é€€åˆ°whisperx: {e}")

        # å°è¯•ä½¿ç”¨æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨
        try:
            from services.model_preload_manager import get_model_manager as get_preload_manager
            model_manager = get_preload_manager()
            if model_manager:
                self.logger.debug("âœ… ä½¿ç”¨æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨è·å–æ¨¡å‹")
                if job:
                    job.message = "åŠ è½½æ¨¡å‹ä¸­"
                return model_manager.get_model(settings)
        except Exception as e:
            self.logger.debug(f"âš ï¸ æ— æ³•ä½¿ç”¨æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨ï¼Œå›é€€åˆ°æœ¬åœ°ç¼“å­˜: {e}")
            pass

        # å›é€€åˆ°ç®€å•ç¼“å­˜æœºåˆ¶
        key = (settings.model, settings.compute_type, settings.device)
        with _model_lock:
            if key in _model_cache:
                self.logger.debug(f"âœ… å‘½ä¸­æ¨¡å‹ç¼“å­˜: {key}")
                if job:
                    job.message = "ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹"
                return _model_cache[key]

            self.logger.info(f"ğŸ” åŠ è½½æ¨¡å‹: {key}")
            if job:
                job.message = f"åŠ è½½æ¨¡å‹ {settings.model}"

            # é¦–å…ˆå°è¯•ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            try:
                from core.config import config
                m = whisperx.load_model(
                    settings.model,
                    settings.device,
                    compute_type=settings.compute_type,
                    download_root=str(config.HF_CACHE_DIR),  # æŒ‡å®šç¼“å­˜è·¯å¾„
                    local_files_only=True  # ç¦æ­¢è‡ªåŠ¨ä¸‹è½½ï¼Œåªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                )
                _model_cache[key] = m
                if job:
                    job.message = "æ¨¡å‹åŠ è½½å®Œæˆ"
                return m
            except Exception as e:
                self.logger.warning(f"âš ï¸ æœ¬åœ°åŠ è½½å¤±è´¥,å…è®¸whisperxä¸‹è½½: {e}")
                if job:
                    job.message = "æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨,ä½¿ç”¨whisperxä¸‹è½½"
                # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥,å…è®¸whisperxä¸‹è½½
                m = whisperx.load_model(
                    settings.model,
                    settings.device,
                    compute_type=settings.compute_type,
                    download_root=str(config.HF_CACHE_DIR),  # æŒ‡å®šç¼“å­˜è·¯å¾„
                    local_files_only=False  # å…è®¸ä¸‹è½½
                )
                _model_cache[key] = m
                if job:
                    job.message = "æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ"
                return m

    def _get_align_model(self, lang: str, device: str, job: Optional[JobState] = None):
        """
        è·å–å¯¹é½æ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰

        é›†æˆæ¨¡å‹ç®¡ç†å™¨ï¼šå¦‚æœæ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å®Œæ•´ï¼Œä¼šè‡ªåŠ¨è§¦å‘ä¸‹è½½å¹¶ç­‰å¾…å®Œæˆ

        Args:
            lang: è¯­è¨€ä»£ç 
            device: è®¾å¤‡ (cuda/cpu)
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡(å¯é€‰,ç”¨äºæ›´æ–°ä¸‹è½½è¿›åº¦)

        Returns:
            Tuple[model, metadata]: å¯¹é½æ¨¡å‹å’Œå…ƒæ•°æ®
        """
        with _align_lock:
            # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
            if lang in _align_model_cache:
                self.logger.debug(f"âœ… å‘½ä¸­å¯¹é½æ¨¡å‹ç¼“å­˜: {lang}")
                if job:
                    job.message = "ä½¿ç”¨ç¼“å­˜çš„å¯¹é½æ¨¡å‹"
                return _align_model_cache[lang]

            # å°è¯•ä½¿ç”¨æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨ï¼ˆä¼˜å…ˆä»LRUç¼“å­˜è·å–ï¼‰
            try:
                from services.model_preload_manager import get_model_manager as get_preload_manager
                preload_mgr = get_preload_manager()
                if preload_mgr:
                    self.logger.debug("âœ… å°è¯•ä»é¢„åŠ è½½ç®¡ç†å™¨è·å–å¯¹é½æ¨¡å‹")
                    if job:
                        job.message = "åŠ è½½å¯¹é½æ¨¡å‹"
                    am, meta = preload_mgr.get_align_model(lang, device)
                    _align_model_cache[lang] = (am, meta)
                    return am, meta
            except Exception as e:
                self.logger.debug(f"é¢„åŠ è½½ç®¡ç†å™¨è·å–å¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥åŠ è½½: {e}")

            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦ä¸‹è½½ï¼ˆä½¿ç”¨æ¨¡å‹ç®¡ç†æœåŠ¡ï¼‰
            try:
                from services.model_manager_service import get_model_manager
                model_mgr = get_model_manager()
                align_model_info = model_mgr.align_models.get(lang)

                if align_model_info and (align_model_info.status == "not_downloaded" or align_model_info.status == "incomplete"):
                    # æ£€æŸ¥æ¨¡å‹çŠ¶æ€,å¦‚æœæœªä¸‹è½½æˆ–ä¸å®Œæ•´åˆ™è§¦å‘ä¸‹è½½
                    if align_model_info.status == "incomplete":
                        self.logger.warning(f"âš ï¸ å¯¹é½æ¨¡å‹ä¸å®Œæ•´: {lang}")
                    else:
                        self.logger.warning(f"âš ï¸ å¯¹é½æ¨¡å‹æœªä¸‹è½½: {lang}")

                    # å¯¹é½æ¨¡å‹é€šå¸¸ä¸º1.2GBå·¦å³,ç»™å‡ºå¤§æ¨¡å‹æç¤º
                    download_msg = "å½“å‰ä¸‹è½½æ¨¡å‹å¤§äº1GB (çº¦1.2GB),è¯·è€å¿ƒç­‰å¾…"
                    self.logger.info(f"ğŸ“¦ {download_msg}")
                    self.logger.info(f"ğŸš€ è‡ªåŠ¨è§¦å‘ä¸‹è½½å¯¹é½æ¨¡å‹: {lang}")

                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    if job:
                        job.message = download_msg

                    # è§¦å‘ä¸‹è½½
                    success = model_mgr.download_align_model(lang)
                    if not success:
                        self.logger.warning(f"âš ï¸ æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥æˆ–å·²åœ¨ä¸‹è½½ä¸­,å›é€€åˆ°whisperx")
                        raise RuntimeError("æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥")

                    # ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾…10åˆ†é’Ÿ,å¯¹é½æ¨¡å‹è¾ƒå¤§ï¼‰
                    import time
                    max_wait_time = 600  # 10åˆ†é’Ÿ
                    wait_interval = 5  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                    elapsed = 0

                    while elapsed < max_wait_time:
                        time.sleep(wait_interval)
                        elapsed += wait_interval

                        current_status = model_mgr.align_models[lang].status
                        progress = model_mgr.align_models[lang].download_progress

                        if current_status == "ready":
                            self.logger.info(f"âœ… å¯¹é½æ¨¡å‹ä¸‹è½½å®Œæˆ: {lang}")
                            if job:
                                job.message = "å¯¹é½æ¨¡å‹ä¸‹è½½å®Œæˆ,å‡†å¤‡åŠ è½½"
                            break
                        elif current_status == "error":
                            self.logger.error(f"âŒ æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥,å›é€€åˆ°whisperx")
                            raise RuntimeError(f"å¯¹é½æ¨¡å‹ä¸‹è½½å¤±è´¥: {lang}")
                        else:
                            # å®šæœŸæé†’ç”¨æˆ·è€å¿ƒç­‰å¾…(æ¯30ç§’)
                            if elapsed % 30 == 0:
                                wait_msg = f"å½“å‰ä¸‹è½½æ¨¡å‹å¤§äº1GB,è¯·è€å¿ƒç­‰å¾…... {progress:.1f}% ({elapsed}s/{max_wait_time}s)"
                                self.logger.info(f"â³ {wait_msg}")
                                if job:
                                    job.message = wait_msg
                            else:
                                wait_msg = f"ç­‰å¾…å¯¹é½æ¨¡å‹ä¸‹è½½... {progress:.1f}%"
                                self.logger.info(f"â³ {wait_msg} ({elapsed}s/{max_wait_time}s)")
                                # æ›´æ–°ä»»åŠ¡çŠ¶æ€(æ¯æ¬¡éƒ½æ›´æ–°,è¿™æ ·ç”¨æˆ·å¯ä»¥çœ‹åˆ°è¿›åº¦å˜åŒ–)
                                if job:
                                    job.message = wait_msg

                    if elapsed >= max_wait_time:
                        self.logger.error(f"âŒ æ¨¡å‹ä¸‹è½½è¶…æ—¶,å›é€€åˆ°whisperx")
                        raise TimeoutError(f"å¯¹é½æ¨¡å‹ä¸‹è½½è¶…æ—¶: {lang}")

            except Exception as e:
                self.logger.warning(f"âš ï¸ æ¨¡å‹ç®¡ç†æœåŠ¡æ£€æŸ¥å¤±è´¥,å›é€€åˆ°whisperx: {e}")

            # ç›´æ¥åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå·²ä¸‹è½½æˆ–ä¸‹è½½å®Œæˆï¼‰
            self.logger.info(f"ğŸ” åŠ è½½å¯¹é½æ¨¡å‹: {lang}")
            if job:
                job.message = f"åŠ è½½å¯¹é½æ¨¡å‹ {lang}"

            # é¦–å…ˆå°è¯•ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            try:
                from core.config import config
                am, meta = whisperx.load_align_model(
                    language_code=lang,
                    device=device,
                    model_dir=str(config.HF_CACHE_DIR)  # æŒ‡å®šç¼“å­˜è·¯å¾„
                )
                _align_model_cache[lang] = (am, meta)
                if job:
                    job.message = "å¯¹é½æ¨¡å‹åŠ è½½å®Œæˆ"
                return am, meta
            except Exception as e:
                self.logger.warning(f"âš ï¸ æœ¬åœ°åŠ è½½å¯¹é½æ¨¡å‹å¤±è´¥,å…è®¸whisperxä¸‹è½½: {e}")
                if job:
                    job.message = "æœ¬åœ°å¯¹é½æ¨¡å‹ä¸å­˜åœ¨,ä½¿ç”¨whisperxä¸‹è½½"
                # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥,å…è®¸whisperxä¸‹è½½
                am, meta = whisperx.load_align_model(
                    language_code=lang,
                    device=device
                )
                _align_model_cache[lang] = (am, meta)
                if job:
                    job.message = "å¯¹é½æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ"
                return am, meta

    def _transcribe_segment(
        self,
        seg: Dict,
        model,
        job: JobState,
        align_cache: Dict
    ):
        """
        è½¬å½•å•ä¸ªéŸ³é¢‘æ®µ

        Args:
            seg: æ®µä¿¡æ¯ {file, start_ms, duration_ms}
            model: Whisperæ¨¡å‹
            job: ä»»åŠ¡çŠ¶æ€
            align_cache: å¯¹é½æ¨¡å‹ç¼“å­˜

        Returns:
            Dict: è½¬å½•ç»“æœï¼ˆåŒ…å«segmentså’Œword_segmentsï¼‰
        """
        audio = whisperx.load_audio(seg['file'])

        try:
            # Whisperè½¬å½•
            rs = model.transcribe(
                audio,
                batch_size=job.settings.batch_size,
                verbose=False,
                language=job.language
            )

            if not rs or 'segments' not in rs:
                return None

            # æ£€æµ‹è¯­è¨€
            if not job.language and 'language' in rs:
                job.language = rs['language']
                self.logger.info(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {job.language}")

            lang = job.language or rs.get('language')

            # åŠ è½½å¯¹é½æ¨¡å‹
            if lang not in align_cache:
                am, meta = self._get_align_model(lang, job.settings.device, job)
                align_cache[lang] = (am, meta)

            am, meta = align_cache[lang]

            # è¯çº§å¯¹é½
            aligned = whisperx.align(
                rs['segments'],
                am,
                meta,
                audio,
                job.settings.device
            )

            # æ—¶é—´åç§»æ ¡æ­£ï¼ˆé‡è¦ï¼ï¼‰
            start_offset = seg['start_ms'] / 1000.0
            final = {'segments': []}

            if 'segments' in aligned:
                for s in aligned['segments']:
                    if 'start' in s:
                        s['start'] += start_offset
                    if 'end' in s:
                        s['end'] += start_offset
                    final['segments'].append(s)

            if 'word_segments' in aligned:
                final['word_segments'] = []
                for w in aligned['word_segments']:
                    if 'start' in w:
                        w['start'] += start_offset
                    if 'end' in w:
                        w['end'] += start_offset
                    final['word_segments'].append(w)

            return final

        finally:
            del audio
            gc.collect()

    def _format_ts(self, sec: float) -> str:
        """
        æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºSRTæ ¼å¼

        Args:
            sec: ç§’æ•°

        Returns:
            str: SRTæ—¶é—´æˆ³ (HH:MM:SS,mmm)
        """
        if sec < 0:
            sec = 0

        ms = int(round(sec * 1000))
        h = ms // 3600000
        ms %= 3600000
        m = ms // 60000
        ms %= 60000
        s = ms // 1000
        ms %= 1000

        return f"{h:02}:{m:02}:{s:02},{ms:03}"

    def _generate_srt(self, results: List[Dict], path: str, word_level: bool):
        """
        ç”ŸæˆSRTå­—å¹•æ–‡ä»¶

        Args:
            results: è½¬å½•ç»“æœåˆ—è¡¨
            path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            word_level: æ˜¯å¦ä½¿ç”¨è¯çº§æ—¶é—´æˆ³
        """
        lines = []
        n = 1  # å­—å¹•åºå·

        for r in results:
            if not r:
                continue

            entries = []

            # è¯çº§æ—¶é—´æˆ³æ¨¡å¼
            if word_level and r.get('word_segments'):
                for w in r['word_segments']:
                    if w.get('start') is not None and w.get('end') is not None:
                        txt = (w.get('word') or '').strip()
                        if txt:
                            entries.append({
                                'start': w['start'],
                                'end': w['end'],
                                'text': txt
                            })

            # å¥å­çº§æ—¶é—´æˆ³æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            elif r.get('segments'):
                for s in r['segments']:
                    if s.get('start') is not None and s.get('end') is not None:
                        txt = (s.get('text') or '').strip()
                        if txt:
                            entries.append({
                                'start': s['start'],
                                'end': s['end'],
                                'text': txt
                            })

            # å†™å…¥SRTæ ¼å¼
            for e in entries:
                if e['end'] <= e['start']:
                    continue  # è·³è¿‡æ— æ•ˆæ—¶é—´æˆ³

                lines.append(str(n))  # åºå·
                lines.append(
                    f"{self._format_ts(e['start'])} --> {self._format_ts(e['end'])}"
                )  # æ—¶é—´æˆ³
                lines.append(e['text'])  # å­—å¹•æ–‡æœ¬
                lines.append("")  # ç©ºè¡Œ
                n += 1

        # å†™å…¥æ–‡ä»¶
        with open(path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        self.logger.info(f"âœ… SRTæ–‡ä»¶å·²ç”Ÿæˆ: {path}, å…±{n-1}æ¡å­—å¹•")


# å•ä¾‹å¤„ç†å™¨
_service_instance: Optional[TranscriptionService] = None


def get_transcription_service(root: str) -> TranscriptionService:
    """è·å–è½¬å½•æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _service_instance
    if _service_instance is None:
        _service_instance = TranscriptionService(root)
    return _service_instance