"""
è½¬å½•å¤„ç†æœåŠ¡
æ•´åˆäº†processor.pyå’ŒåŸtranscription_service.pyçš„æ‰€æœ‰åŠŸèƒ½
"""
import os, subprocess, uuid, threading, json, math, gc, logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from enum import Enum
from dataclasses import dataclass, field
from collections import OrderedDict  # æ–°å¢å¯¼å…¥
from pydub import AudioSegment, silence
from .whisper_service import get_whisper_service, load_audio as whisper_load_audio
import torch
import shutil
import psutil
import numpy as np


class ProcessingMode(Enum):
    """
    å¤„ç†æ¨¡å¼æšä¸¾
    ç”¨äºæ™ºèƒ½å†³ç­–ä½¿ç”¨å†…å­˜æ¨¡å¼è¿˜æ˜¯ç¡¬ç›˜æ¨¡å¼è¿›è¡ŒéŸ³é¢‘å¤„ç†
    """
    MEMORY = "memory"  # å†…å­˜æ¨¡å¼ï¼ˆé»˜è®¤ï¼Œé«˜æ€§èƒ½ï¼‰
    DISK = "disk"      # ç¡¬ç›˜æ¨¡å¼ï¼ˆé™çº§ï¼Œç¨³å®šæ€§ä¼˜å…ˆï¼‰


class VADMethod(Enum):
    """
    VADæ¨¡å‹é€‰æ‹©æšä¸¾
    ç”¨äºé€‰æ‹©è¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVoice Activity Detectionï¼‰æ¨¡å‹
    """
    SILERO = "silero"      # é»˜è®¤ï¼Œæ— éœ€è®¤è¯ï¼Œé€Ÿåº¦å¿«
    PYANNOTE = "pyannote"  # å¯é€‰ï¼Œéœ€è¦HF Tokenï¼Œç²¾åº¦æ›´é«˜


@dataclass
class VADConfig:
    """
    VADé…ç½®æ•°æ®ç±»
    ç”¨äºé…ç½®è¯­éŸ³æ´»åŠ¨æ£€æµ‹çš„å‚æ•°
    
    å‚æ•°è¯´æ˜ï¼š
    - onset (0.0-1.0)ï¼šè¯­éŸ³å¼€å§‹é˜ˆå€¼ï¼Œè¶Šé«˜è¶Šä¸¥æ ¼ï¼Œæ¨è0.6-0.7ä»¥è¿‡æ»¤èƒŒæ™¯éŸ³ä¹
    - offset (0.0-1.0)ï¼šè¯­éŸ³ç»“æŸé˜ˆå€¼ï¼Œé€šå¸¸ä¸ºonsetçš„70%å·¦å³
    - min_speech_duration_msï¼šæœ€å°è¯­éŸ³æ®µé•¿åº¦ï¼Œé¿å…è¯¯æ£€ç¢ç‰‡éŸ³ï¼ˆæ¨è300-500msï¼‰
    - min_silence_duration_msï¼šæœ€å°é™éŸ³é•¿åº¦ï¼Œè¶Šé•¿è¶Šèƒ½è¿‡æ»¤èƒŒæ™¯éŸ³ä¹ï¼ˆæ¨è300-500msï¼‰
    """
    method: VADMethod = VADMethod.SILERO  # é»˜è®¤ä½¿ç”¨Silero
    hf_token: Optional[str] = None         # Pyannoteéœ€è¦çš„HF Token
    onset: float = 0.7                     # è¯­éŸ³å¼€å§‹é˜ˆå€¼ï¼ˆæå‡è‡³0.7ä»¥æ›´ä¸¥æ ¼è¿‡æ»¤ï¼‰
    offset: float = 0.5                    # è¯­éŸ³ç»“æŸé˜ˆå€¼ï¼ˆå¯¹åº”onset=0.7çš„è°ƒæ•´ï¼‰
    chunk_size: int = 30                   # æœ€å¤§æ®µé•¿ï¼ˆç§’ï¼‰
    min_speech_duration_ms: int = 500      # æœ€å°è¯­éŸ³æ®µé•¿åº¦ï¼ˆæå‡è‡³500msï¼Œè¿‡æ»¤çŸ­å™ªéŸ³ï¼‰
    min_silence_duration_ms: int = 500     # æœ€å°é™éŸ³é•¿åº¦ï¼ˆæå‡è‡³500msï¼Œç¡®ä¿æ–­å¥æ¸…æ™°ï¼‰

    def validate(self) -> bool:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        if self.method == VADMethod.PYANNOTE and not self.hf_token:
            return False  # Pyannoteéœ€è¦Token
        if not (0.0 <= self.onset <= 1.0) or not (0.0 <= self.offset <= 1.0):
            return False  # é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´
        return True


class BreakToGlobalSeparation(Exception):
    """ç†”æ–­å¼‚å¸¸ï¼šè§¦å‘æ—¶éœ€è¦å‡çº§ä¸ºå…¨å±€äººå£°åˆ†ç¦»æ¨¡å¼"""
    pass


@dataclass
class CircuitBreakerState:
    """
    ç†”æ–­å™¨çŠ¶æ€ï¼ˆæ”¯æŒæ¨¡å‹å‡çº§ï¼‰

    ç”¨äºç›‘æ§è½¬å½•è´¨é‡ï¼Œå½“å¤§é‡æ®µè½éœ€è¦é‡è¯•æ—¶ï¼š
    1. ä¼˜å…ˆå°è¯•å‡çº§æ¨¡å‹ï¼ˆå¦‚æœå…è®¸ä¸”æœªè¾¾ä¸Šé™ï¼‰
    2. æ— æ³•å‡çº§æ—¶æ‰è§¦å‘ç†”æ–­
    """
    consecutive_retries: int = 0        # è¿ç»­é‡è¯•è®¡æ•°
    total_retries: int = 0              # æ€»é‡è¯•æ¬¡æ•°
    total_segments: int = 0             # æ€»æ®µè½æ•°
    processed_segments: int = 0         # å·²å¤„ç†æ®µè½æ•°

    # === Phase 3: å‡çº§è·Ÿè¸ª ===
    escalation_count: int = 0                           # å·²å‡çº§æ¬¡æ•°
    current_model: Optional[str] = None                 # å½“å‰ä½¿ç”¨çš„æ¨¡å‹
    escalation_history: List[str] = field(default_factory=list)  # å‡çº§å†å²

    def record_retry(self):
        """è®°å½•ä¸€æ¬¡é‡è¯•"""
        self.consecutive_retries += 1
        self.total_retries += 1

    def record_success(self):
        """è®°å½•ä¸€æ¬¡æˆåŠŸï¼ˆé‡ç½®è¿ç»­è®¡æ•°ï¼‰"""
        self.consecutive_retries = 0
        self.processed_segments += 1

    def record_escalation(self, new_model: str):
        """
        è®°å½•ä¸€æ¬¡æ¨¡å‹å‡çº§

        Args:
            new_model: å‡çº§åçš„æ¨¡å‹åç§°
        """
        if self.current_model:
            self.escalation_history.append(f"{self.current_model} -> {new_model}")
        self.current_model = new_model
        self.escalation_count += 1
        # å‡çº§åé‡ç½®è¿ç»­é‡è¯•è®¡æ•°ï¼Œç»™æ–°æ¨¡å‹æœºä¼š
        self.consecutive_retries = 0

    def should_escalate(self, demucs_settings) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å‡çº§æ¨¡å‹ï¼ˆä¼˜å…ˆäºç†”æ–­ï¼‰

        å‡çº§æ¡ä»¶ï¼š
        1. å…è®¸è‡ªåŠ¨å‡çº§ (auto_escalation=True)
        2. æœªè¾¾åˆ°æœ€å¤§å‡çº§æ¬¡æ•°
        3. æ»¡è¶³ç†”æ–­æ¡ä»¶ï¼ˆè¿ç»­é‡è¯•æˆ–æ¯”ä¾‹è¿‡é«˜ï¼‰

        Args:
            demucs_settings: Demucsé…ç½®å¯¹è±¡

        Returns:
            bool: Trueè¡¨ç¤ºåº”è¯¥å‡çº§æ¨¡å‹
        """
        if not demucs_settings.auto_escalation:
            return False

        if self.escalation_count >= demucs_settings.max_escalations:
            return False

        # æ»¡è¶³ç†”æ–­æ¡ä»¶æ—¶ï¼Œä¼˜å…ˆå‡çº§
        return self._check_break_condition(demucs_settings)

    def should_break(self, demucs_settings) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘ç†”æ–­

        æ³¨æ„ï¼šåªæœ‰åœ¨æ— æ³•å‡çº§æ—¶æ‰è§¦å‘ç†”æ–­

        Args:
            demucs_settings: Demucsé…ç½®å¯¹è±¡

        Returns:
            bool: Trueè¡¨ç¤ºåº”è¯¥è§¦å‘ç†”æ–­
        """
        if not demucs_settings.circuit_breaker_enabled:
            return False

        # å¦‚æœè¿˜èƒ½å‡çº§ï¼Œä¸è§¦å‘ç†”æ–­
        if self.should_escalate(demucs_settings):
            return False

        return self._check_break_condition(demucs_settings)

    def _check_break_condition(self, demucs_settings) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç†”æ–­/å‡çº§æ¡ä»¶

        ç†”æ–­æ¡ä»¶ï¼ˆæ»¡è¶³ä»»ä¸€å³è§¦å‘ï¼‰ï¼š
        1. è¿ç»­ N ä¸ª segment éƒ½è§¦å‘é‡è¯•ï¼ˆé»˜è®¤N=3ï¼‰
        2. æ€»é‡è¯•æ¯”ä¾‹è¶…è¿‡é˜ˆå€¼ï¼ˆé»˜è®¤20%ï¼‰

        Args:
            demucs_settings: Demucsé…ç½®å¯¹è±¡

        Returns:
            bool: Trueè¡¨ç¤ºæ»¡è¶³ç†”æ–­/å‡çº§æ¡ä»¶
        """
        # æ¡ä»¶1ï¼šè¿ç»­é‡è¯•æ¬¡æ•°
        if self.consecutive_retries >= demucs_settings.consecutive_threshold:
            return True

        # æ¡ä»¶2ï¼šæ€»é‡è¯•æ¯”ä¾‹ï¼ˆè‡³å°‘å¤„ç†5ä¸ªsegmentåæ‰æ£€æŸ¥ï¼‰
        if self.processed_segments >= 5:
            retry_ratio = self.total_retries / self.processed_segments
            if retry_ratio >= demucs_settings.ratio_threshold:
                return True

        return False

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ‰©å±•ï¼‰"""
        return {
            "consecutive_retries": self.consecutive_retries,
            "total_retries": self.total_retries,
            "total_segments": self.total_segments,
            "processed_segments": self.processed_segments,
            "retry_ratio": self.total_retries / max(1, self.processed_segments),
            # Phase 3 æ–°å¢
            "escalation_count": self.escalation_count,
            "current_model": self.current_model,
            "escalation_history": self.escalation_history,
        }


class CircuitBreakAction(Enum):
    """ç†”æ–­åçš„å¤„ç†åŠ¨ä½œ"""
    CONTINUE = "continue"           # ç»§ç»­å¤„ç†ï¼Œæ ‡è®°é—®é¢˜æ®µè½
    FALLBACK_ORIGINAL = "fallback"  # é™çº§ä½¿ç”¨åŸå§‹éŸ³é¢‘
    FAIL = "fail"                   # ä»»åŠ¡å¤±è´¥
    PAUSE = "pause"                 # æš‚åœç­‰å¾…äººå·¥ä»‹å…¥


class CircuitBreakHandler:
    """
    ç†”æ–­å¼‚å¸¸å¤„ç†å™¨

    è´Ÿè´£åœ¨ç†”æ–­è§¦å‘æ—¶æ‰§è¡Œç”¨æˆ·é…ç½®çš„å¤„ç†ç­–ç•¥
    """

    def __init__(self, job: "JobState", settings):
        """
        åˆå§‹åŒ–ç†”æ–­å¤„ç†å™¨

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            settings: DemucsSettings é…ç½®å¯¹è±¡
        """
        self.job = job
        self.settings = settings
        self.logger = logging.getLogger(__name__)
        self.problem_segments: List[int] = []  # è®°å½•é—®é¢˜æ®µè½ç´¢å¼•

    def handle(
        self,
        breaker_state: CircuitBreakerState,
        current_segment_idx: int,
        sse_manager = None
    ) -> CircuitBreakAction:
        """
        å¤„ç†ç†”æ–­å¼‚å¸¸

        Args:
            breaker_state: ç†”æ–­å™¨çŠ¶æ€
            current_segment_idx: å½“å‰æ®µè½ç´¢å¼•
            sse_manager: SSEç®¡ç†å™¨ï¼ˆç”¨äºæ¨é€äº‹ä»¶ï¼‰

        Returns:
            CircuitBreakAction: å¤„ç†åŠ¨ä½œ
        """
        action_str = self.settings.on_break

        # è§£æå¤„ç†åŠ¨ä½œ
        try:
            action = CircuitBreakAction(action_str)
        except ValueError:
            # å¦‚æœé…ç½®å€¼æ— æ•ˆï¼Œé»˜è®¤ä½¿ç”¨ CONTINUE
            self.logger.warning(f"æ— æ•ˆçš„ç†”æ–­å¤„ç†ç­–ç•¥: {action_str}ï¼Œä½¿ç”¨é»˜è®¤å€¼ continue")
            action = CircuitBreakAction.CONTINUE

        # è®°å½•é—®é¢˜æ®µè½
        self.problem_segments.append(current_segment_idx)

        # æ¨é€ SSE äº‹ä»¶
        if sse_manager:
            self._push_circuit_break_event(breaker_state, action, sse_manager)

        # æ ¹æ®ç­–ç•¥æ‰§è¡Œæ“ä½œ
        if action == CircuitBreakAction.FAIL:
            self.logger.error(
                f"ç†”æ–­è§¦å‘ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚é—®é¢˜æ®µè½: {self.problem_segments}"
            )
            raise BreakToGlobalSeparation(
                f"ç†”æ–­è§¦å‘ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚é—®é¢˜æ®µè½: {self.problem_segments}"
            )

        elif action == CircuitBreakAction.PAUSE:
            self.logger.warning(
                f"ç†”æ–­è§¦å‘ï¼Œç­‰å¾…äººå·¥ä»‹å…¥ã€‚é—®é¢˜æ®µè½: {self.problem_segments}"
            )
            self.job.paused = True
            self.job.status = "paused"
            self.job.message = f"ç†”æ–­è§¦å‘ï¼Œç­‰å¾…äººå·¥ä»‹å…¥ã€‚é—®é¢˜æ®µè½: {self.problem_segments}"
            raise BreakToGlobalSeparation(self.job.message)

        else:  # CONTINUE æˆ– FALLBACK_ORIGINAL
            self.logger.warning(
                f"ç†”æ–­è§¦å‘ï¼Œé‡‡ç”¨ {action.value} ç­–ç•¥ç»§ç»­å¤„ç†ã€‚"
                f"é—®é¢˜æ®µè½: {self.problem_segments}"
            )

        return action

    def get_problem_report(self) -> Dict:
        """
        è·å–é—®é¢˜æŠ¥å‘Š

        Returns:
            åŒ…å«é—®é¢˜ç»Ÿè®¡å’Œå»ºè®®çš„å­—å…¸
        """
        return {
            "total_problem_segments": len(self.problem_segments),
            "problem_indices": self.problem_segments,
            "suggestion": self._get_suggestion()
        }

    def _get_suggestion(self) -> str:
        """
        æ ¹æ®é—®é¢˜æ®µè½æ•°é‡ç»™å‡ºå»ºè®®

        Returns:
            å»ºè®®æ–‡æœ¬
        """
        count = len(self.problem_segments)
        if count == 0:
            return "æ‰€æœ‰æ®µè½å¤„ç†æ­£å¸¸"
        elif count <= 3:
            return "å°‘é‡æ®µè½å¯èƒ½éœ€è¦æ‰‹åŠ¨è°ƒæ•´æ—¶é—´è½´"
        elif count <= 10:
            return "å»ºè®®æ£€æŸ¥è¿™äº›æ®µè½çš„å­—å¹•å‡†ç¡®æ€§"
        else:
            return "å¤§é‡æ®µè½æœ‰é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨æ›´é«˜è´¨é‡çš„æ¨¡å‹é‡æ–°å¤„ç†"

    def _push_circuit_break_event(
        self,
        state: CircuitBreakerState,
        action: CircuitBreakAction,
        sse_manager
    ):
        """
        æ¨é€ç†”æ–­å¤„ç†äº‹ä»¶

        Args:
            state: ç†”æ–­å™¨çŠ¶æ€
            action: å¤„ç†åŠ¨ä½œ
            sse_manager: SSEç®¡ç†å™¨
        """
        try:
            sse_manager.push_event(
                self.job.job_id,
                "circuit_breaker_handled",
                {
                    "action": action.value,
                    "problem_segments": self.problem_segments,
                    "stats": state.get_stats(),
                    "suggestion": self._get_suggestion()
                }
            )
        except Exception as e:
            self.logger.debug(f"SSEæ¨é€å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")


from models.job_models import JobSettings, JobState
from models.hardware_models import HardwareInfo, OptimizationConfig
from services.hardware_service import get_hardware_detector, get_hardware_optimizer
from services.cpu_affinity_service import CPUAffinityManager, CPUAffinityConfig
from services.job_index_service import get_job_index_service
from core.config import config  # å¯¼å…¥ç»Ÿä¸€é…ç½®

# å…¨å±€æ¨¡å‹ç¼“å­˜ (æŒ‰ (model, compute_type, device) é”®)
_model_cache: Dict[Tuple[str, str, str], object] = {}


_model_lock = threading.Lock()


class TranscriptionService:
    """
    è½¬å½•å¤„ç†æœåŠ¡
    æ•´åˆäº†æ‰€æœ‰è½¬å½•ç›¸å…³åŠŸèƒ½
    """

    def __init__(self, jobs_root: str):
        """
        åˆå§‹åŒ–è½¬å½•æœåŠ¡

        Args:
            jobs_root: ä»»åŠ¡å·¥ä½œç›®å½•æ ¹è·¯å¾„
        """
        self.jobs_root = Path(jobs_root)
        self.jobs_root.mkdir(parents=True, exist_ok=True)

        self.jobs: Dict[str, JobState] = {}
        self.lock = threading.Lock()
        self.logger = logging.getLogger(__name__)

        # é›†æˆCPUäº²å’Œæ€§ç®¡ç†å™¨
        self.cpu_manager = CPUAffinityManager()

        # é›†æˆç¡¬ä»¶æ£€æµ‹
        self.hardware_detector = get_hardware_detector()
        self.hardware_optimizer = get_hardware_optimizer()
        self._hardware_info: Optional[HardwareInfo] = None
        self._optimization_config: Optional[OptimizationConfig] = None

        # é›†æˆä»»åŠ¡ç´¢å¼•æœåŠ¡
        self.job_index = get_job_index_service(jobs_root)
        # å¯åŠ¨æ—¶æ¸…ç†æ— æ•ˆæ˜ å°„
        self.job_index.cleanup_invalid_mappings()

        # é›†æˆSSEç®¡ç†å™¨ï¼ˆç”¨äºå®æ—¶è¿›åº¦æ¨é€ï¼‰
        from services.sse_service import get_sse_manager
        self.sse_manager = get_sse_manager()
        self.logger.info("SSEç®¡ç†å™¨å·²é›†æˆ")

        # è®°å½•CPUä¿¡æ¯
        sys_info = self.cpu_manager.get_system_info()
        if sys_info.get('supported', False):
            self.logger.info(
                f" CPUä¿¡æ¯: {sys_info['logical_cores']}ä¸ªé€»è¾‘æ ¸å¿ƒ, "
                f"{sys_info.get('physical_cores', '?')}ä¸ªç‰©ç†æ ¸å¿ƒ, "
                f"å¹³å°: {sys_info.get('platform', '?')}"
            )
        else:
            self.logger.warning("CPUäº²å’Œæ€§åŠŸèƒ½ä¸å¯ç”¨")

        # æ‰§è¡Œç¡¬ä»¶æ£€æµ‹
        self._detect_hardware()

        # å¯åŠ¨æ—¶æ‰«æå¹¶åŠ è½½æ‰€æœ‰ä»»åŠ¡ï¼ˆä¿®å¤é‡å¯åæ— æ³•æ‰“å¼€æ—§ä»»åŠ¡çš„é—®é¢˜ï¼‰
        self._load_all_jobs_from_disk()

    def _detect_hardware(self):
        """æ‰§è¡Œç¡¬ä»¶æ£€æµ‹å¹¶ç”Ÿæˆä¼˜åŒ–é…ç½®"""
        try:
            self.logger.info("å¼€å§‹ç¡¬ä»¶æ£€æµ‹...")
            self._hardware_info = self.hardware_detector.detect()
            self._optimization_config = self.hardware_optimizer.get_optimization_config(self._hardware_info)

            # è®°å½•æ£€æµ‹ç»“æœ
            hw = self._hardware_info
            opt = self._optimization_config
            self.logger.info(f"ç¡¬ä»¶æ£€æµ‹å®ŒæˆGPU: {'' if hw.cuda_available else ''}, "
                           f"CPU: {hw.cpu_cores}æ ¸/{hw.cpu_threads}çº¿ç¨‹, "
                           f"å†…å­˜: {hw.memory_total_mb}MB, "
                           f"ä¼˜åŒ–é…ç½®: batch={opt.batch_size}, device={opt.recommended_device}")
        except Exception as e:
            self.logger.error(f"ç¡¬ä»¶æ£€æµ‹å¤±è´¥: {e}")

    def _load_all_jobs_from_disk(self):
        """
        å¯åŠ¨æ—¶æ‰«æå¹¶åŠ è½½æ‰€æœ‰ä»»åŠ¡åˆ°å†…å­˜ï¼ˆä¿®å¤é‡å¯åæ— æ³•æ‰“å¼€æ—§ä»»åŠ¡çš„é—®é¢˜ï¼‰

        è¿™ä¸ªæ–¹æ³•ä¼šæ‰«æ jobs ç›®å½•ä¸­çš„æ‰€æœ‰ä»»åŠ¡ï¼Œå¹¶åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œ
        é¿å…é‡å¯åå› ä¸ºå†…å­˜ä¸ºç©ºå¯¼è‡´æ— æ³•è®¿é—®æ—§ä»»åŠ¡
        """
        try:
            loaded_count = 0
            for job_dir in self.jobs_root.iterdir():
                if not job_dir.is_dir():
                    continue

                job_id = job_dir.name
                if job_id in self.jobs:
                    # å·²åŠ è½½ï¼Œè·³è¿‡
                    continue

                # å°è¯•åŠ è½½ä»»åŠ¡
                job = self.get_job(job_id)
                if job:
                    loaded_count += 1

            if loaded_count > 0:
                self.logger.info(f"å¯åŠ¨æ—¶å·²åŠ è½½ {loaded_count} ä¸ªå†å²ä»»åŠ¡åˆ°å†…å­˜")
        except Exception as e:
            self.logger.error(f"åŠ è½½å†å²ä»»åŠ¡å¤±è´¥: {e}")
    
    def get_hardware_info(self) -> Optional[HardwareInfo]:
        """è·å–ç¡¬ä»¶ä¿¡æ¯"""
        return self._hardware_info
    
    def get_optimization_config(self) -> Optional[OptimizationConfig]:
        """è·å–ä¼˜åŒ–é…ç½®"""  
        return self._optimization_config
    
    def get_optimized_job_settings(self, base_settings: Optional[JobSettings] = None) -> JobSettings:
        """è·å–åŸºäºç¡¬ä»¶ä¼˜åŒ–çš„ä»»åŠ¡è®¾ç½®"""
        # ä½¿ç”¨ç¡¬ä»¶ä¼˜åŒ–é…ç½®ä½œä¸ºé»˜è®¤å€¼
        if self._optimization_config:
            optimized = JobSettings(
                model=base_settings.model if base_settings else "medium",
                compute_type=base_settings.compute_type if base_settings else "float16",
                device=self._optimization_config.recommended_device,
                batch_size=self._optimization_config.batch_size,
                word_timestamps=base_settings.word_timestamps if base_settings else False
            )
            return optimized
        
        # å¦‚æœæ²¡æœ‰ç¡¬ä»¶ä¿¡æ¯ï¼Œä½¿ç”¨ä¼ å…¥çš„è®¾ç½®æˆ–é»˜è®¤è®¾ç½®
        return base_settings or JobSettings()

    def create_job(
        self,
        filename: str,
        src_path: str,
        settings: JobSettings,
        job_id: Optional[str] = None
    ) -> JobState:
        """
        åˆ›å»ºè½¬å½•ä»»åŠ¡

        Args:
            filename: æ–‡ä»¶å
            src_path: æºæ–‡ä»¶è·¯å¾„
            settings: ä»»åŠ¡è®¾ç½®
            job_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰

        Returns:
            JobState: åˆ›å»ºçš„ä»»åŠ¡çŠ¶æ€å¯¹è±¡
        """
        job_id = job_id or uuid.uuid4().hex
        job_dir = self.jobs_root / job_id
        job_dir.mkdir(parents=True, exist_ok=True)

        dest_path = job_dir / filename

        # å¤åˆ¶æ–‡ä»¶åˆ°ä»»åŠ¡ç›®å½•
        if os.path.abspath(src_path) != os.path.abspath(dest_path):
            try:
                shutil.copyfile(src_path, dest_path)
                self.logger.debug(f"æ–‡ä»¶å·²å¤åˆ¶: {src_path} -> {dest_path}")
            except Exception as e:
                self.logger.warning(f"æ–‡ä»¶å¤åˆ¶å¤±è´¥: {e}")

        # åˆ›å»ºä»»åŠ¡çŠ¶æ€å¯¹è±¡
        job = JobState(
            job_id=job_id,
            filename=filename,
            dir=str(job_dir),
            input_path=src_path,
            settings=settings,
            status="uploaded",
            phase="pending",
            message="æ–‡ä»¶å·²ä¸Šä¼ "
        )

        with self.lock:
            self.jobs[job_id] = job

        # æ·»åŠ æ–‡ä»¶è·¯å¾„åˆ°ä»»åŠ¡IDçš„æ˜ å°„
        self.job_index.add_mapping(src_path, job_id)

        # æŒä¹…åŒ–ä»»åŠ¡å…ƒä¿¡æ¯ï¼ˆé‡å¯åå¯æ¢å¤ï¼‰
        self.save_job_meta(job)

        self.logger.info(f"ä»»åŠ¡å·²åˆ›å»º: {job_id} - {filename}")
        return job

    def save_job_meta(self, job: JobState) -> bool:
        """
        ä¿å­˜ä»»åŠ¡å…ƒä¿¡æ¯åˆ° job_meta.jsonï¼ˆç”¨äºé‡å¯åæ¢å¤ï¼‰

        ä½¿ç”¨åŸå­å†™å…¥ç¡®ä¿æ–­ç”µå®‰å…¨ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†renameæ›¿æ¢

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡

        Returns:
            bool: æ˜¯å¦æˆåŠŸä¿å­˜
        """
        job_dir = Path(job.dir)
        meta_file = job_dir / "job_meta.json"

        try:
            job_dir.mkdir(parents=True, exist_ok=True)

            # åŸå­å†™å…¥ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†renameæ›¿æ¢
            temp_file = meta_file.with_suffix(".tmp")
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(job.to_meta_dict(), f, indent=2, ensure_ascii=False)

            # åŸå­æ›¿æ¢
            temp_file.replace(meta_file)
            self.logger.debug(f"ä»»åŠ¡å…ƒä¿¡æ¯å·²ä¿å­˜: {job.job_id}")
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜ä»»åŠ¡å…ƒä¿¡æ¯å¤±è´¥ {job.job_id}: {e}")
            return False

    def load_job_meta(self, job_id: str) -> Optional[JobState]:
        """
        ä» job_meta.json åŠ è½½ä»»åŠ¡å…ƒä¿¡æ¯

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            Optional[JobState]: æ¢å¤çš„ä»»åŠ¡çŠ¶æ€å¯¹è±¡
        """
        job_dir = self.jobs_root / job_id
        meta_file = job_dir / "job_meta.json"

        if not meta_file.exists():
            return None

        try:
            with open(meta_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            job = JobState.from_meta_dict(data)

            # ç¡®ä¿ dir è·¯å¾„æ­£ç¡®ï¼ˆå¯èƒ½å› ä¸ºé¡¹ç›®è¿ç§»è€Œæ”¹å˜ï¼‰
            job.dir = str(job_dir)

            self.logger.debug(f"ä» job_meta.json åŠ è½½ä»»åŠ¡: {job_id}")
            return job
        except Exception as e:
            self.logger.error(f"åŠ è½½ä»»åŠ¡å…ƒä¿¡æ¯å¤±è´¥ {job_id}: {e}")
            return None

    def get_job(self, job_id: str) -> Optional[JobState]:
        """
        è·å–ä»»åŠ¡çŠ¶æ€

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            Optional[JobState]: ä»»åŠ¡çŠ¶æ€å¯¹è±¡ï¼Œä¸å­˜åœ¨åˆ™è¿”å›None
        """
        with self.lock:
            # é¦–å…ˆä»å†…å­˜ä¸­æŸ¥æ‰¾
            if job_id in self.jobs:
                return self.jobs[job_id]

        # å¦‚æœå†…å­˜ä¸­æ²¡æœ‰ï¼Œå°è¯•ä» jobs ç›®å½•è¯»å–ï¼ˆä¿®å¤é‡å¯åæ— æ³•æ‰“å¼€æ—§ä»»åŠ¡çš„é—®é¢˜ï¼‰
        job_dir = self.jobs_root / job_id
        if not job_dir.exists():
            return None

        try:
            # ä¼˜å…ˆä» job_meta.json åŠ è½½ï¼ˆåŒ…å«å®Œæ•´çš„ä»»åŠ¡çŠ¶æ€ï¼‰
            job = self.load_job_meta(job_id)
            if job:
                # ç¼“å­˜åˆ°å†…å­˜
                with self.lock:
                    self.jobs[job_id] = job
                self.logger.info(f"ä» job_meta.json æ¢å¤ä»»åŠ¡: {job_id}")
                return job

            # é™çº§ï¼šä»ç›®å½•æ–‡ä»¶æ¨æ–­çŠ¶æ€ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            # å°è¯•æ‰¾åˆ°åŸå§‹æ–‡ä»¶
            filename = "æœªçŸ¥æ–‡ä»¶"
            input_path = None

            # ä»ç›®å½•ä¸­æŸ¥æ‰¾è§†é¢‘/éŸ³é¢‘æ–‡ä»¶
            for ext in ['.mp4', '.avi', '.mkv', '.mov', '.flv', '.wmv', '.mp3', '.wav', '.m4a']:
                matches = list(job_dir.glob(f"*{ext}"))
                if matches:
                    filename = matches[0].name
                    input_path = str(matches[0])
                    break

            # æ£€æŸ¥æ˜¯å¦æœ‰ SRT æ–‡ä»¶ï¼ˆè¡¨ç¤ºå·²å®Œæˆï¼‰
            srt_files = list(job_dir.glob("*.srt"))
            is_finished = len(srt_files) > 0

            # åˆ›å»º JobState å¯¹è±¡
            job = JobState(
                job_id=job_id,
                filename=filename,
                dir=str(job_dir),
                input_path=input_path,
                status='finished' if is_finished else 'processing',
                phase='editing' if is_finished else 'transcribing',
                progress=100 if is_finished else 0,
                message='å·²å®Œæˆ' if is_finished else 'å¤„ç†ä¸­',
                srt_path=str(srt_files[0]) if srt_files else None
            )

            # å°è¯•ä» checkpoint è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
            checkpoint_path = job_dir / "checkpoint.json"
            if checkpoint_path.exists():
                try:
                    with open(checkpoint_path, 'r', encoding='utf-8') as f:
                        checkpoint_data = json.load(f)
                        total_segments = checkpoint_data.get('total_segments', 0)
                        processed_indices = checkpoint_data.get('processed_indices', [])
                        if total_segments > 0:
                            job.progress = min((len(processed_indices) / total_segments) * 100, 100)
                        job.phase = checkpoint_data.get('phase', 'transcribing')
                        job.language = checkpoint_data.get('language')
                        # ä»checkpointæ¢å¤segments
                        if 'unaligned_results' in checkpoint_data:
                            job.segments = checkpoint_data['unaligned_results']
                except Exception as e:
                    self.logger.warning(f"è¯»å–checkpointå¤±è´¥ {checkpoint_path}: {e}")

            # ç¼“å­˜åˆ°å†…å­˜
            with self.lock:
                self.jobs[job_id] = job

            # åŒæ—¶ä¿å­˜ job_meta.json ä»¥ä¾¿ä¸‹æ¬¡ç›´æ¥åŠ è½½
            self.save_job_meta(job)

            self.logger.info(f"ä»ç£ç›˜æ¢å¤ä»»åŠ¡ï¼ˆæ—§ç‰ˆå…¼å®¹ï¼‰: {job_id}")
            return job

        except Exception as e:
            self.logger.error(f"ä»ç£ç›˜è¯»å–ä»»åŠ¡å¤±è´¥ {job_id}: {e}")
            return None

    def scan_incomplete_jobs(self) -> List[Dict]:
        """
        æ‰«ææ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ˆæœ‰checkpoint.jsonçš„ä»»åŠ¡ï¼‰

        Returns:
            List[Dict]: æœªå®Œæˆä»»åŠ¡åˆ—è¡¨
        """
        incomplete_jobs = []

        try:
            # éå†æ‰€æœ‰ä»»åŠ¡ç›®å½•
            for job_dir in self.jobs_root.iterdir():
                if not job_dir.is_dir():
                    continue

                checkpoint_path = job_dir / "checkpoint.json"
                if not checkpoint_path.exists():
                    continue

                try:
                    # åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®
                    with open(checkpoint_path, 'r', encoding='utf-8') as f:
                        checkpoint_data = json.load(f)

                    job_id = checkpoint_data.get('job_id') or job_dir.name
                    total_segments = checkpoint_data.get('total_segments', 0)
                    processed_indices = checkpoint_data.get('processed_indices', [])
                    processed_count = len(processed_indices)

                    # è®¡ç®—è¿›åº¦
                    if total_segments > 0:
                        progress = (processed_count / total_segments) * 100
                    else:
                        progress = 0

                    # ä»ç´¢å¼•ä¸­æŸ¥æ‰¾æ–‡ä»¶å
                    file_path = self.job_index.get_file_path(job_id)
                    filename = os.path.basename(file_path) if file_path else "æœªçŸ¥æ–‡ä»¶"

                    incomplete_jobs.append({
                        'job_id': job_id,
                        'filename': filename,
                        'file_path': file_path,  # æ·»åŠ æ–‡ä»¶è·¯å¾„
                        'progress': round(progress, 2),
                        'processed_segments': processed_count,
                        'total_segments': total_segments,
                        'phase': checkpoint_data.get('phase', 'unknown'),
                        'dir': str(job_dir)
                    })

                except Exception as e:
                    self.logger.warning(f"è¯»å–æ£€æŸ¥ç‚¹å¤±è´¥ {checkpoint_path}: {e}")
                    continue

            self.logger.info(f"æ‰«æåˆ° {len(incomplete_jobs)} ä¸ªæœªå®Œæˆä»»åŠ¡")
            return incomplete_jobs

        except Exception as e:
            self.logger.error(f"æ‰«ææœªå®Œæˆä»»åŠ¡å¤±è´¥: {e}")
            return []

    def restore_job_from_checkpoint(self, job_id: str) -> Optional[JobState]:
        """
        ä»æ£€æŸ¥ç‚¹æ¢å¤ä»»åŠ¡çŠ¶æ€ï¼ˆæ—  checkpoint æ—¶ä»å¤´å¼€å§‹ï¼‰

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            Optional[JobState]: æ¢å¤çš„ä»»åŠ¡çŠ¶æ€å¯¹è±¡
        """
        job_dir = self.jobs_root / job_id
        if not job_dir.exists():
            return None

        # å°è¯•åŠ è½½ checkpointï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
        checkpoint = self._load_checkpoint(job_dir)

        try:
            # æŸ¥æ‰¾åŸæ–‡ä»¶
            filename = "unknown"
            input_path = None

            # ä»ç›®å½•ä¸­æŸ¥æ‰¾è§†é¢‘/éŸ³é¢‘æ–‡ä»¶
            for ext in ['.mp4', '.avi', '.mkv', '.mov', '.flv', '.wmv', '.mp3', '.wav', '.m4a']:
                matches = list(job_dir.glob(f"*{ext}"))
                if matches:
                    filename = matches[0].name
                    input_path = str(matches[0])
                    break

            if not input_path:
                self.logger.warning(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡ {job_id} çš„è¾“å…¥æ–‡ä»¶")
                return None

            # åˆ›å»ºé»˜è®¤çš„CPUäº²å’Œæ€§é…ç½®
            from services.cpu_affinity_service import CPUAffinityConfig
            default_cpu_config = CPUAffinityConfig(
                enabled=True,
                strategy="auto",
                custom_cores=None,
                exclude_cores=None
            )

            # æ ¹æ®æ˜¯å¦æœ‰ checkpoint å†³å®šæ¢å¤çŠ¶æ€
            if checkpoint:
                # æœ‰ checkpointï¼Œä»æ–­ç‚¹æ¢å¤
                phase = checkpoint.get('phase', 'pending')
                total_segments = checkpoint.get('total_segments', 0)
                processed_indices = checkpoint.get('processed_indices', [])
                processed = len(processed_indices)
                progress = round((processed / max(1, total_segments)) * 100, 2)
                message = f"å·²æš‚åœ ({processed}/{total_segments}æ®µ)"
                self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ä»»åŠ¡: {job_id}")
            else:
                # æ—  checkpointï¼Œä»å¤´å¼€å§‹
                phase = 'pending'
                total_segments = 0
                processed = 0
                progress = 0
                message = "ç¨‹åºé‡å¯ï¼Œä»»åŠ¡å°†ä»å¤´å¼€å§‹"
                self.logger.info(f"æ— æ£€æŸ¥ç‚¹ï¼Œä»»åŠ¡å°†ä»å¤´å¼€å§‹: {job_id}")

            # åˆ›å»ºä»»åŠ¡çŠ¶æ€å¯¹è±¡
            job = JobState(
                job_id=job_id,
                filename=filename,
                dir=str(job_dir),
                input_path=input_path,
                settings=JobSettings(cpu_affinity=default_cpu_config),
                status="paused",
                phase=phase,
                message=message,
                total=total_segments,
                processed=processed,
                progress=progress
            )

            with self.lock:
                self.jobs[job_id] = job

            return job

        except Exception as e:
            self.logger.error(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ä»»åŠ¡å¤±è´¥: {e}")
            return None

    def check_file_checkpoint(self, file_path: str) -> Optional[Dict]:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰å¯ç”¨çš„æ–­ç‚¹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            Optional[Dict]: æ–­ç‚¹ä¿¡æ¯ï¼Œæ— æ–­ç‚¹åˆ™è¿”å›None
        """
        # ä»ç´¢å¼•ä¸­æŸ¥æ‰¾ä»»åŠ¡ID
        job_id = self.job_index.get_job_id(file_path)
        if not job_id:
            return None

        # æ£€æŸ¥ä»»åŠ¡ç›®å½•å’Œcheckpointæ˜¯å¦å­˜åœ¨
        job_dir = self.jobs_root / job_id
        if not job_dir.exists():
            # æ¸…ç†æ— æ•ˆæ˜ å°„
            self.job_index.remove_mapping(file_path)
            return None

        checkpoint = self._load_checkpoint(job_dir)
        if not checkpoint:
            return None

        # è¿”å›æ–­ç‚¹ä¿¡æ¯
        total_segments = checkpoint.get('total_segments', 0)
        processed_indices = checkpoint.get('processed_indices', [])
        processed_count = len(processed_indices)

        if total_segments > 0:
            progress = (processed_count / total_segments) * 100
        else:
            progress = 0

        return {
            'job_id': job_id,
            'progress': round(progress, 2),
            'processed_segments': processed_count,
            'total_segments': total_segments,
            'phase': checkpoint.get('phase', 'unknown'),
            'can_resume': True
        }

    def start_job(self, job_id: str):
        """
        å¯åŠ¨è½¬å½•ä»»åŠ¡ï¼ˆV2.2: åºŸå¼ƒï¼Œç”±é˜Ÿåˆ—æœåŠ¡è°ƒç”¨_run_pipelineï¼‰

        æ³¨æ„: æ­¤æ–¹æ³•ä¿ç•™æ˜¯ä¸ºäº†å‘åå…¼å®¹ï¼Œä½†ä¸å†è‡ªåŠ¨åˆ›å»ºçº¿ç¨‹

        Args:
            job_id: ä»»åŠ¡ID
        """
        #  å…³é”®æ”¹åŠ¨: ä¸å†è‡ªåŠ¨åˆ›å»ºçº¿ç¨‹ï¼Œç”±é˜Ÿåˆ—æœåŠ¡ç»Ÿä¸€ç®¡ç†
        # åŸæœ‰ä»£ç :
        # threading.Thread(target=self._run_pipeline, args=(job,), daemon=True).start()

        # æ–°é€»è¾‘: åªæ›´æ–°çŠ¶æ€ï¼Œå®é™…æ‰§è¡Œç”±é˜Ÿåˆ—æœåŠ¡æ§åˆ¶
        job = self.get_job(job_id)
        if not job:
            self.logger.warning(f"ä»»åŠ¡æœªæ‰¾åˆ°: {job_id}")
            return

        if job.status not in ("uploaded", "failed", "paused", "created"):
            self.logger.warning(f"ä»»åŠ¡æ— æ³•å¯åŠ¨: {job_id}, çŠ¶æ€: {job.status}")
            return

        job.canceled = False
        job.paused = False
        job.error = None
        # çŠ¶æ€ç”±é˜Ÿåˆ—æœåŠ¡è®¾ç½®ï¼Œè¿™é‡Œä¸æ”¹

        self.logger.warning(f"start_jobå·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨é˜Ÿåˆ—æœåŠ¡: {job_id}")

    def pause_job(self, job_id: str) -> bool:
        """
        æš‚åœè½¬å½•ä»»åŠ¡ï¼ˆä¿å­˜æ–­ç‚¹ï¼‰

        Args:
            job_id: ä»»åŠ¡ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸè®¾ç½®æš‚åœæ ‡å¿—
        """
        job = self.get_job(job_id)
        if not job:
            return False

        job.paused = True
        job.message = "æš‚åœä¸­..."
        self.logger.info(f"â¸ï¸ ä»»åŠ¡æš‚åœè¯·æ±‚: {job_id}")
        return True

    def cancel_job(self, job_id: str, delete_data: bool = False) -> bool:
        """
        å–æ¶ˆè½¬å½•ä»»åŠ¡

        Args:
            job_id: ä»»åŠ¡ID
            delete_data: æ˜¯å¦åˆ é™¤ä»»åŠ¡æ•°æ®

        Returns:
            bool: æ˜¯å¦æˆåŠŸè®¾ç½®å–æ¶ˆæ ‡å¿—
        """
        job = self.get_job(job_id)
        if not job:
            return False

        job.canceled = True
        job.message = "å–æ¶ˆä¸­..."
        self.logger.info(f"ğŸ›‘ ä»»åŠ¡å–æ¶ˆè¯·æ±‚: {job_id}, åˆ é™¤æ•°æ®: {delete_data}")

        # å¦‚æœéœ€è¦åˆ é™¤æ•°æ®
        if delete_data:
            try:
                job_dir = Path(job.dir)
                # ç§»é™¤æ–‡ä»¶è·¯å¾„æ˜ å°„
                if job.input_path:
                    self.job_index.remove_mapping(job.input_path)

                if job_dir.exists():
                    # åˆ é™¤æ•´ä¸ªä»»åŠ¡ç›®å½•
                    shutil.rmtree(job_dir)
                    self.logger.info(f"å·²åˆ é™¤ä»»åŠ¡æ•°æ®: {job_id}")
                    # ä»å†…å­˜ä¸­ç§»é™¤ä»»åŠ¡
                    with self.lock:
                        if job_id in self.jobs:
                            del self.jobs[job_id]
            except Exception as e:
                self.logger.error(f"åˆ é™¤ä»»åŠ¡æ•°æ®å¤±è´¥: {e}")

        return True

    def _update_progress(
        self,
        job: JobState,
        phase: str,
        phase_ratio: float,
        message: str = ""
    ):
        """
        æ›´æ–°ä»»åŠ¡è¿›åº¦

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            phase: å½“å‰é˜¶æ®µ (extract/split/transcribe/srt)
            phase_ratio: å½“å‰é˜¶æ®µå®Œæˆæ¯”ä¾‹ (0.0-1.0)
            message: è¿›åº¦æ¶ˆæ¯
        """
        job.phase = phase

        # è®¡ç®—é˜¶æ®µå†…è¿›åº¦ï¼ˆ0-100ï¼Œä¿ç•™1ä½å°æ•°ï¼‰
        job.phase_percent = round(max(0.0, min(1.0, phase_ratio)) * 100, 1)

        # ä½¿ç”¨é…ç½®ä¸­çš„è¿›åº¦æƒé‡
        phase_weights = config.PHASE_WEIGHTS
        total_weight = config.TOTAL_WEIGHT

        # è®¡ç®—ç´¯è®¡è¿›åº¦
        done_weight = 0
        for p, w in phase_weights.items():
            if p == phase:
                break
            done_weight += w

        current_weight = phase_weights.get(phase, 0) * max(0.0, min(1.0, phase_ratio))
        # æ”¹ä¸º1ä½å°æ•°
        job.progress = round((done_weight + current_weight) / total_weight * 100, 1)

        if message:
            job.message = message

        # æ¨é€SSEè¿›åº¦æ›´æ–°ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        self._push_sse_progress(job)

    def _push_sse_progress(self, job: JobState):
        """
        æ¨é€SSEè¿›åº¦æ›´æ–°ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        åŒæ—¶æ¨é€åˆ°å•ä»»åŠ¡é¢‘é“å’Œå…¨å±€é¢‘é“ï¼Œç¡®ä¿ TaskMonitor å®æ—¶æ›´æ–°

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
        """
        try:
            # åŠ¨æ€è·å–SSEç®¡ç†å™¨ï¼ˆç¡®ä¿è·å–åˆ°å·²è®¾ç½®loopçš„å®ä¾‹ï¼‰
            from services.sse_service import get_sse_manager
            sse_manager = get_sse_manager()

            # 1. æ¨é€åˆ°å•ä»»åŠ¡é¢‘é“ï¼ˆEditorView ä½¿ç”¨ï¼‰
            channel_id = f"job:{job.job_id}"
            progress_data = {
                "job_id": job.job_id,
                "phase": job.phase,
                "percent": job.progress,
                "phase_percent": job.phase_percent,  # æ–°å¢ï¼šé˜¶æ®µå†…è¿›åº¦
                "message": job.message,
                "status": job.status,
                "processed": job.processed,
                "total": job.total,
                "language": job.language or ""
            }
            sse_manager.broadcast_sync(channel_id, "progress", progress_data)

            # 2. æ¨é€åˆ°å…¨å±€é¢‘é“ï¼ˆTaskMonitor ä½¿ç”¨ï¼‰
            global_progress_data = {
                "id": job.job_id,  # å…¨å±€é¢‘é“ä½¿ç”¨ "id"
                "percent": job.progress,
                "phase_percent": job.phase_percent,  # æ–°å¢ï¼šé˜¶æ®µå†…è¿›åº¦
                "message": job.message,
                "status": job.status,
                "phase": job.phase,
                "processed": job.processed,
                "total": job.total
            }
            sse_manager.broadcast_sync("global", "job_progress", global_progress_data)

        except Exception as e:
            # SSEæ¨é€å¤±è´¥ä¸åº”å½±å“è½¬å½•æµç¨‹
            self.logger.debug(f"SSEæ¨é€å¤±è´¥: {e}")

    def _push_sse_signal(self, job: JobState, signal_code: str, message: str = ""):
        """
        æ¨é€SSEä¿¡å·äº‹ä»¶ï¼ˆç”¨äºå…³é”®èŠ‚ç‚¹é€šçŸ¥ï¼‰

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            signal_code: ä¿¡å·ä»£ç ï¼ˆå¦‚ "job_complete", "job_failed", "job_canceled"ï¼‰
            message: é™„åŠ æ¶ˆæ¯
        """
        try:
            # åŠ¨æ€è·å–SSEç®¡ç†å™¨ï¼ˆç¡®ä¿è·å–åˆ°å·²è®¾ç½®loopçš„å®ä¾‹ï¼‰
            from services.sse_service import get_sse_manager
            sse_manager = get_sse_manager()

            channel_id = f"job:{job.job_id}"
            sse_manager.broadcast_sync(
                channel_id,
                "signal",
                {
                    "job_id": job.job_id,
                    "signal": signal_code,  # ç»Ÿä¸€ä½¿ç”¨ "signal" å­—æ®µ
                    "message": message or job.message,
                    "status": job.status,
                    "percent": job.progress
                }
            )
        except Exception as e:
            self.logger.debug(f"SSEä¿¡å·æ¨é€å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

    def _trigger_media_post_process(self, job_id: str):
        """
        å¼‚æ­¥è§¦å‘åª’ä½“é¢„å¤„ç†ï¼ˆè½¬å½•å®Œæˆåè°ƒç”¨ï¼‰
        ç”Ÿæˆæ³¢å½¢å³°å€¼ã€è§†é¢‘ç¼©ç•¥å›¾ã€Proxyè§†é¢‘ç­‰ï¼Œä¸ºç¼–è¾‘å™¨åšå‡†å¤‡

        Args:
            job_id: ä»»åŠ¡ID
        """
        try:
            import asyncio
            import aiohttp

            async def do_post_process():
                """å¼‚æ­¥æ‰§è¡Œé¢„å¤„ç†è¯·æ±‚"""
                try:
                    # è°ƒç”¨åª’ä½“é¢„å¤„ç†æ¥å£
                    async with aiohttp.ClientSession() as session:
                        url = f"http://127.0.0.1:8000/api/media/{job_id}/post-process"
                        async with session.post(url, timeout=aiohttp.ClientTimeout(total=300)) as resp:
                            if resp.status == 200:
                                result = await resp.json()
                                self.logger.info(f"åª’ä½“é¢„å¤„ç†å®Œæˆ: peaks={result.get('peaks')}, thumbnails={result.get('thumbnails')}, proxy={result.get('proxy')}")
                            else:
                                self.logger.warning(f"åª’ä½“é¢„å¤„ç†è¯·æ±‚å¤±è´¥: {resp.status}")
                except asyncio.TimeoutError:
                    self.logger.warning(f"åª’ä½“é¢„å¤„ç†è¶…æ—¶: {job_id}")
                except Exception as e:
                    self.logger.warning(f"åª’ä½“é¢„å¤„ç†å¼‚å¸¸: {e}")

            # å°è¯•åœ¨ç°æœ‰äº‹ä»¶å¾ªç¯ä¸­æ‰§è¡Œ
            try:
                loop = asyncio.get_running_loop()
                asyncio.ensure_future(do_post_process(), loop=loop)
            except RuntimeError:
                # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çº¿ç¨‹æ‰§è¡Œ
                import threading
                def run_in_thread():
                    asyncio.run(do_post_process())
                thread = threading.Thread(target=run_in_thread, daemon=True)
                thread.start()

            self.logger.info(f"å·²è§¦å‘åª’ä½“é¢„å¤„ç†ä»»åŠ¡: {job_id}")

        except Exception as e:
            # é¢„å¤„ç†å¤±è´¥ä¸å½±å“è½¬å½•ç»“æœ
            self.logger.warning(f"è§¦å‘åª’ä½“é¢„å¤„ç†å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

    def _push_sse_segment(self, job: JobState, segment_result: dict, processed: int, total: int):
        """
        æ¨é€å•ä¸ªsegmentçš„è½¬å½•ç»“æœï¼ˆæµå¼è¾“å‡ºï¼‰

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            segment_result: å•ä¸ªsegmentçš„è½¬å½•ç»“æœï¼ˆæœªå¯¹é½ï¼‰
            processed: å·²å¤„ç†çš„segmentæ•°é‡
            total: æ€»segmentæ•°é‡
        """
        try:
            # åŠ¨æ€è·å–SSEç®¡ç†å™¨
            from services.sse_service import get_sse_manager
            sse_manager = get_sse_manager()

            channel_id = f"job:{job.job_id}"
            sse_manager.broadcast_sync(
                channel_id,
                "segment",
                {
                    "segment_index": segment_result.get('segment_index', 0),
                    "segments": segment_result.get('segments', []),
                    "language": segment_result.get('language', job.language),
                    "progress": {
                        "processed": processed,
                        "total": total,
                        "percentage": round(processed / max(1, total) * 100, 2)
                    }
                }
            )
            self.logger.debug(f"æ¨é€segment #{segment_result.get('segment_index', 0)} è½¬å½•ç»“æœ")
        except Exception as e:
            # SSEæ¨é€å¤±è´¥ä¸åº”å½±å“è½¬å½•æµç¨‹
            self.logger.debug(f"SSE segmentæ¨é€å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

    def _push_sse_aligned(self, job: JobState, aligned_results: List[Dict]):
        """
        æ¨é€å¯¹é½å®Œæˆäº‹ä»¶ï¼ˆæµå¼è¾“å‡ºï¼‰

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            aligned_results: å¯¹é½åçš„ç»“æœåˆ—è¡¨
        """
        try:
            # åŠ¨æ€è·å–SSEç®¡ç†å™¨
            from services.sse_service import get_sse_manager
            sse_manager = get_sse_manager()

            channel_id = f"job:{job.job_id}"

            # æå–å¯¹é½åçš„segments
            segments = []
            word_segments = []
            if aligned_results and len(aligned_results) > 0:
                segments = aligned_results[0].get('segments', [])
                word_segments = aligned_results[0].get('word_segments', [])

            sse_manager.broadcast_sync(
                channel_id,
                "aligned",
                {
                    "segments": segments,
                    "word_segments": word_segments,
                    "message": "å¯¹é½å®Œæˆ"
                }
            )
            self.logger.info(f"æ¨é€å¯¹é½å®Œæˆäº‹ä»¶ï¼Œå…± {len(segments)} æ¡å­—å¹•")
        except Exception as e:
            # SSEæ¨é€å¤±è´¥ä¸åº”å½±å“è½¬å½•æµç¨‹
            self.logger.debug(f"SSE alignedæ¨é€å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

    def _save_checkpoint(self, job_dir: Path, data: dict, job: JobState):
        """
        åŸå­æ€§ä¿å­˜æ£€æŸ¥ç‚¹
        ä½¿ç”¨"å†™ä¸´æ—¶æ–‡ä»¶ -> é‡å‘½å"ç­–ç•¥ï¼Œç¡®ä¿æ–‡ä»¶è¦ä¹ˆå®Œæ•´å†™å…¥ï¼Œè¦ä¹ˆä¿æŒåŸæ ·

        Args:
            job_dir: ä»»åŠ¡ç›®å½•
            data: æ£€æŸ¥ç‚¹æ•°æ®
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡ï¼ˆç”¨äºè·å–settingsï¼‰
        """
        # æ·»åŠ åŸå§‹è®¾ç½®åˆ°checkpointï¼ˆç”¨äºæ ¡éªŒå‚æ•°å…¼å®¹æ€§ï¼‰
        data["original_settings"] = {
            "model": job.settings.model,
            "device": job.settings.device,
            "word_timestamps": job.settings.word_timestamps,
            "compute_type": job.settings.compute_type,
            "batch_size": job.settings.batch_size,
            "demucs": {
                "enabled": job.settings.demucs.enabled,
                "mode": job.settings.demucs.mode,
            }
        }

        # ç¡®ä¿ demucs å­—æ®µå­˜åœ¨ï¼ˆå‘åå…¼å®¹ï¼‰
        if "demucs" not in data:
            data["demucs"] = {
                "enabled": job.settings.demucs.enabled,
                "mode": job.settings.demucs.mode,
                "bgm_level": "none",
                "bgm_ratios": [],
                "global_separation_done": False,
                "vocals_path": None,
                "circuit_breaker": None,
                "retry_triggered": False
            }

        checkpoint_path = job_dir / "checkpoint.json"
        temp_path = checkpoint_path.with_suffix(".tmp")

        try:
            # 1. å†™å…¥ä¸´æ—¶æ–‡ä»¶
            with open(temp_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            # 2. åŸå­æ›¿æ¢ï¼ˆWindows/Linux/macOS å‡æ”¯æŒï¼‰
            # å¦‚æœç¨‹åºåœ¨è¿™é‡Œå´©æºƒï¼Œcheckpoint.json ä¾ç„¶æ˜¯æ—§ç‰ˆæœ¬ï¼Œä¸ä¼šæŸå
            os.replace(temp_path, checkpoint_path)

            # 3. åŒæ­¥ä¿å­˜ä»»åŠ¡å…ƒä¿¡æ¯ï¼ˆç”¨äºé‡å¯åæ¢å¤ä»»åŠ¡çŠ¶æ€ï¼‰
            # è¿™æ ·æ¯æ¬¡ä¿å­˜æ£€æŸ¥ç‚¹æ—¶ï¼Œä»»åŠ¡çš„è¿›åº¦å’ŒçŠ¶æ€éƒ½ä¼šè¢«æŒä¹…åŒ–
            self.save_job_meta(job)

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            # ä¿å­˜å¤±è´¥ä¸åº”ä¸­æ–­ä¸»æµç¨‹ï¼Œä»…è®°å½•æ—¥å¿—

    def _load_checkpoint(self, job_dir: Path) -> Optional[dict]:
        """
        åŠ è½½æ£€æŸ¥ç‚¹ï¼Œå¦‚æœæ–‡ä»¶æŸååˆ™è¿”å› None

        Args:
            job_dir: ä»»åŠ¡ç›®å½•

        Returns:
            Optional[dict]: æ£€æŸ¥ç‚¹æ•°æ®ï¼Œä¸å­˜åœ¨æˆ–æŸååˆ™è¿”å› None
        """
        checkpoint_path = job_dir / "checkpoint.json"
        if not checkpoint_path.exists():
            return None

        try:
            with open(checkpoint_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, OSError) as e:
            self.logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶æŸåï¼Œå°†é‡æ–°å¼€å§‹ä»»åŠ¡: {checkpoint_path} - {e}")
            return None

    def _flush_checkpoint_after_split(
        self,
        job_dir: Path,
        job: JobState,
        segments: List[Dict],
        processing_mode: ProcessingMode,
        demucs_state: Dict = None
    ):
        """
        åˆ†æ®µå®Œæˆåå¼ºåˆ¶åˆ·æ–°checkpointï¼ˆç¡®ä¿æ–­ç‚¹ç»­ä¼ ä¸€è‡´æ€§ï¼‰

        è¿™æ˜¯æ–­ç‚¹ç»­ä¼ çš„å…³é”®èŠ‚ç‚¹ï¼
        åªæœ‰åˆ†æ®µå…ƒæ•°æ®è¢«æŒä¹…åŒ–åï¼Œåç»­çš„è½¬å½•ç´¢å¼•æ‰æœ‰æ„ä¹‰ã€‚

        Args:
            job_dir: ä»»åŠ¡ç›®å½•
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            segments: åˆ†æ®µå…ƒæ•°æ®åˆ—è¡¨
            processing_mode: å½“å‰å¤„ç†æ¨¡å¼
            demucs_state: DemucsçŠ¶æ€æ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        import time

        checkpoint_data = {
            "job_id": job.job_id,
            "phase": "split_complete",  # æ˜ç¡®æ ‡è®°åˆ†æ®µå®Œæˆ
            "processing_mode": processing_mode.value,  # è®°å½•æ¨¡å¼
            "total_segments": len(segments),
            "processed_indices": [],
            "segments": segments,
            "unaligned_results": [],
            "timestamp": time.time()  # æ—¶é—´æˆ³ç”¨äºè°ƒè¯•
        }

        # æ·»åŠ  demucs çŠ¶æ€ï¼ˆå¦‚æœæä¾›ï¼‰
        if demucs_state:
            checkpoint_data["demucs"] = demucs_state

        # å¼ºåˆ¶åŒæ­¥å†™å…¥ï¼ˆç¡®ä¿æ•°æ®è½ç›˜ï¼‰
        self._save_checkpoint(job_dir, checkpoint_data, job)

        # éªŒè¯å†™å…¥æˆåŠŸ
        saved_checkpoint = self._load_checkpoint(job_dir)
        if saved_checkpoint is None:
            raise RuntimeError("checkpoint write verification failed: file not readable")

        if saved_checkpoint.get('phase') != 'split_complete':
            raise RuntimeError("checkpoint write verification failed: phase mismatch")

        if len(saved_checkpoint.get('segments', [])) != len(segments):
            raise RuntimeError("checkpoint write verification failed: segments count mismatch")

        self.logger.info(f"checkpoint flushed and verified after split (mode: {processing_mode.value}, segments: {len(segments)})")

    def _run_pipeline(self, job: JobState):
        """
        æ‰§è¡Œè½¬å½•å¤„ç†ç®¡é“ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
        """
        # åº”ç”¨CPUäº²å’Œæ€§è®¾ç½®
        cpu_applied = False
        if job.settings.cpu_affinity and job.settings.cpu_affinity.enabled:
            cpu_applied = self.cpu_manager.apply_cpu_affinity(
                job.settings.cpu_affinity
            )
            if cpu_applied:
                self.logger.info(f"ğŸ“Œ ä»»åŠ¡ {job.job_id} å·²åº”ç”¨CPUäº²å’Œæ€§è®¾ç½®")

        try:
            # æ£€æŸ¥å–æ¶ˆå’Œæš‚åœæ ‡å¿—
            if job.canceled:
                job.status = 'canceled'
                job.message = 'å·²å–æ¶ˆ'
                return

            if job.paused:
                job.status = 'paused'
                job.message = 'å·²æš‚åœ'
                self.logger.info(f"â¸ï¸ ä»»åŠ¡å·²æš‚åœ: {job.job_id}")
                return

            job_dir = Path(job.dir)
            input_path = job_dir / job.filename
            audio_path = job_dir / 'audio.wav'

            # ==========================================
            # 1. å°è¯•æ¢å¤çŠ¶æ€ï¼ˆæ–­ç‚¹ç»­ä¼ æ ¸å¿ƒï¼‰
            # ==========================================
            checkpoint = self._load_checkpoint(job_dir)

            # åˆå§‹åŒ–å†…å­˜çŠ¶æ€
            processed_indices = set()
            unaligned_results = []  # æœªå¯¹é½çš„è½¬å½•ç»“æœ
            current_segments = []

            if checkpoint:
                self.logger.info(f"å‘ç°æ£€æŸ¥ç‚¹ï¼Œä» {checkpoint.get('phase', 'unknown')} é˜¶æ®µæ¢å¤")
                # æ¢å¤æ•°æ®åˆ°å†…å­˜
                processed_indices = set(checkpoint.get('processed_indices', []))

                # ã€å…¼å®¹æ€§å¤„ç†ã€‘æ”¯æŒæ—§æ ¼å¼checkpoint
                if 'unaligned_results' in checkpoint:
                    # æ–°æ ¼å¼ï¼šunaligned_resultså­—æ®µ
                    unaligned_results = checkpoint.get('unaligned_results', [])
                    self.logger.info("æ£€æµ‹åˆ°æ–°æ ¼å¼checkpointï¼ˆæœªå¯¹é½ç»“æœï¼‰")
                elif 'results' in checkpoint:
                    # æ—§æ ¼å¼ï¼šresultså­—æ®µï¼ˆå·²å¯¹é½ï¼‰
                    self.logger.warning("æ£€æµ‹åˆ°æ—§ç‰ˆcheckpointæ ¼å¼ï¼Œå°†ç›´æ¥ä½¿ç”¨å·²å¯¹é½ç»“æœ")
                    # å°†æ—§æ ¼å¼è½¬æ¢ä¸ºæ–°æ ¼å¼ï¼ˆè·³è¿‡å¯¹é½é˜¶æ®µï¼‰
                    # è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ç›´æ¥ä½¿ç”¨resultsä½œä¸ºæœ€ç»ˆç»“æœ
                    pass

                current_segments = checkpoint.get('segments', [])
                # æ¢å¤ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
                job.total = checkpoint.get('total_segments', 0)
                job.processed = len(processed_indices)
                self.logger.info(f"å·²å¤„ç† {job.processed}/{job.total} æ®µ")

            # ==========================================
            # 2. é˜¶æ®µ1: æå–éŸ³é¢‘
            # ==========================================
            # åªæœ‰å½“éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…ä»å¤´å¼€å§‹æ—¶ï¼Œæ‰æ‰§è¡Œæå–
            if not audio_path.exists() or (checkpoint is None):
                self._update_progress(job, 'extract', 0, 'æå–éŸ³é¢‘ä¸­')
                if job.canceled:
                    raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

                if not self._extract_audio(str(input_path), str(audio_path)):
                    raise RuntimeError('FFmpeg æå–éŸ³é¢‘å¤±è´¥')

                self._update_progress(job, 'extract', 1, 'éŸ³é¢‘æå–å®Œæˆ')
            else:
                self.logger.info("è·³è¿‡éŸ³é¢‘æå–ï¼Œä½¿ç”¨å·²æœ‰æ–‡ä»¶")

            if job.canceled:
                raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

            # ==========================================
            # 2. é˜¶æ®µ2: BGMæ£€æµ‹ï¼ˆå¯é€‰ï¼‰
            # ==========================================
            from services.demucs_service import BGMLevel

            bgm_level = BGMLevel.NONE
            bgm_ratios = []
            demucs_settings = job.settings.demucs

            # ä»checkpointæ¢å¤BGMæ£€æµ‹ç»“æœ
            if checkpoint and 'demucs' in checkpoint:
                demucs_state = checkpoint['demucs']
                bgm_level_str = demucs_state.get('bgm_level', 'none')
                bgm_level = BGMLevel(bgm_level_str)
                bgm_ratios = demucs_state.get('bgm_ratios', [])
                self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤BGMæ£€æµ‹ç»“æœ: {bgm_level.value}")
            else:
                # æ‰§è¡ŒBGMæ£€æµ‹ï¼ˆå¦‚æœå¯ç”¨ä¸”æ¨¡å¼éœ€è¦ï¼‰
                if demucs_settings.enabled and demucs_settings.mode in ["auto", "always"]:
                    bgm_level, bgm_ratios = self._detect_bgm(str(audio_path), job)
                    self.logger.info(f"BGMæ£€æµ‹å®Œæˆ: {bgm_level.value}")

            # ==========================================
            # 3. é˜¶æ®µ3: å…¨å±€äººå£°åˆ†ç¦»ï¼ˆä½¿ç”¨åˆ†çº§ç­–ç•¥ï¼‰
            # ==========================================
            use_vocals = False
            vocals_path = None
            separation_strategy = None

            # åªæœ‰å¯ç”¨ Demucs æ—¶æ‰ç”Ÿæˆåˆ†ç¦»ç­–ç•¥
            if demucs_settings.enabled:
                # ã€æ–°å¢ã€‘ä½¿ç”¨ç­–ç•¥è§£æå™¨å†³å®šåˆ†ç¦»ç­–ç•¥
                from services.demucs_service import SeparationStrategyResolver, get_demucs_service
                strategy_resolver = SeparationStrategyResolver(demucs_settings)
                separation_strategy = strategy_resolver.resolve(bgm_level)

                # æ¨é€åˆ†ç¦»ç­–ç•¥SSEäº‹ä»¶
                self._push_sse_separation_strategy(job, separation_strategy)

                self.logger.info(
                    f"åˆ†ç¦»ç­–ç•¥: should_separate={separation_strategy.should_separate}, "
                    f"model={separation_strategy.initial_model}, "
                    f"reason={separation_strategy.reason}"
                )

            # ä»checkpointæ¢å¤åˆ†ç¦»çŠ¶æ€
            if checkpoint and 'demucs' in checkpoint:
                demucs_state = checkpoint['demucs']
                if demucs_state.get('global_separation_done'):
                    vocals_path = demucs_state.get('vocals_path')
                    use_vocals = True
                    self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼šä½¿ç”¨å·²åˆ†ç¦»çš„äººå£° {vocals_path}")

            # å¦‚æœç­–ç•¥è¦æ±‚åˆ†ç¦»ä¸”å°šæœªå®Œæˆ
            if separation_strategy and separation_strategy.should_separate and not use_vocals:
                # ã€å…³é”®ã€‘æ ¹æ®ç­–ç•¥é€‰æ‹©æ¨¡å‹
                from services.demucs_service import get_demucs_service
                demucs_service = get_demucs_service()
                demucs_service.set_model_for_strategy(separation_strategy)

                vocals_path = self._separate_vocals_global(str(audio_path), job)
                use_vocals = True
                self.logger.info(f"å…¨å±€äººå£°åˆ†ç¦»å®Œæˆ: {vocals_path} (æ¨¡å‹: {separation_strategy.initial_model})")

            # å†³å®šåç»­ä½¿ç”¨å“ªä¸ªéŸ³é¢‘æ–‡ä»¶
            active_audio_path = Path(vocals_path) if use_vocals else audio_path

            if job.canceled:
                raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

            # ==========================================
            # 4. é˜¶æ®µ1.5: æ™ºèƒ½æ¨¡å¼å†³ç­–
            # ==========================================
            processing_mode = None
            audio_array = None  # å†…å­˜æ¨¡å¼ä¸‹çš„éŸ³é¢‘æ•°ç»„

            # ä»checkpointæ¢å¤æ¨¡å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if checkpoint and 'processing_mode' in checkpoint:
                mode_value = checkpoint['processing_mode']
                processing_mode = ProcessingMode(mode_value)
                self.logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤å¤„ç†æ¨¡å¼: {processing_mode.value}")

            # å¦‚æœæ²¡æœ‰æ£€æŸ¥ç‚¹æˆ–æ²¡æœ‰æ¨¡å¼ä¿¡æ¯ï¼Œè¿›è¡Œæ™ºèƒ½å†³ç­–
            if processing_mode is None:
                processing_mode = self._decide_processing_mode(str(active_audio_path), job)
                self.logger.info(f"æ™ºèƒ½é€‰æ‹©å¤„ç†æ¨¡å¼: {processing_mode.value}")

            # ==========================================
            # 5. é˜¶æ®µ1.6: éŸ³é¢‘åŠ è½½ï¼ˆå†…å­˜æ¨¡å¼ï¼‰
            # ==========================================
            if processing_mode == ProcessingMode.MEMORY:
                # å†…å­˜æ¨¡å¼ï¼šå°è¯•åŠ è½½å®Œæ•´éŸ³é¢‘åˆ°å†…å­˜ï¼ˆä½¿ç”¨å¯èƒ½åˆ†ç¦»åçš„éŸ³é¢‘ï¼‰
                try:
                    audio_array = self._safe_load_audio(str(active_audio_path), job)
                    self.logger.info("éŸ³é¢‘å·²åŠ è½½åˆ°å†…å­˜ï¼ˆå†…å­˜æ¨¡å¼ï¼‰")
                except RuntimeError as e:
                    # åŠ è½½å¤±è´¥ï¼Œé™çº§åˆ°ç¡¬ç›˜æ¨¡å¼
                    self.logger.warning(f"å†…å­˜åŠ è½½å¤±è´¥ï¼Œé™çº§åˆ°ç¡¬ç›˜æ¨¡å¼: {e}")
                    processing_mode = ProcessingMode.DISK
                    audio_array = None

            # ==========================================
            # 6. é˜¶æ®µ2: æ™ºèƒ½åˆ†æ®µï¼ˆæ¨¡å¼æ„ŸçŸ¥ï¼‰
            # ==========================================
            # å¦‚æœæ£€æŸ¥ç‚¹é‡Œæ²¡æœ‰åˆ†æ®µä¿¡æ¯ï¼Œè¯´æ˜ä¸Šæ¬¡æ²¡è·‘åˆ°åˆ†æ®µå®Œæˆ
            if not current_segments:
                self._update_progress(job, 'split', 0, 'éŸ³é¢‘åˆ†æ®µä¸­')

                # æ ¹æ®æ¨¡å¼é€‰æ‹©åˆ†æ®µæ–¹æ³•
                if processing_mode == ProcessingMode.MEMORY and audio_array is not None:
                    # å†…å­˜æ¨¡å¼ï¼šVADåˆ†æ®µï¼ˆä¸äº§ç”Ÿç£ç›˜IOï¼‰
                    self.logger.info("ä½¿ç”¨å†…å­˜VADåˆ†æ®µï¼ˆé«˜æ€§èƒ½æ¨¡å¼ï¼‰")
                    from services.transcription_service import VADConfig
                    current_segments = self._split_audio_in_memory(
                        audio_array,
                        sr=16000,
                        vad_config=VADConfig()  # ä½¿ç”¨é»˜è®¤Silero VAD
                    )
                else:
                    # ç¡¬ç›˜æ¨¡å¼ï¼šä¼ ç»Ÿpydubåˆ†æ®µï¼ˆä½¿ç”¨å¯èƒ½åˆ†ç¦»åçš„éŸ³é¢‘ï¼‰
                    self.logger.info("ä½¿ç”¨ç¡¬ç›˜åˆ†æ®µï¼ˆç¨³å®šæ¨¡å¼ï¼‰")
                    current_segments = self._split_audio_to_disk(str(active_audio_path))

                if job.canceled:
                    raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

                job.segments = current_segments
                job.total = len(current_segments)
                self._update_progress(job, 'split', 1, f'åˆ†æ®µå®Œæˆ å…±{job.total}æ®µ')

                # ã€å…³é”®åŸ‹ç‚¹1ã€‘åˆ†æ®µå®Œæˆåå¼ºåˆ¶åˆ·æ–°checkpointï¼ˆä½¿ç”¨æ–°æ–¹æ³•ï¼ŒåŒ…å«demucsçŠ¶æ€ï¼‰
                self._flush_checkpoint_after_split(
                    job_dir,
                    job,
                    current_segments,
                    processing_mode,
                    demucs_state={
                        "enabled": demucs_settings.enabled,
                        "mode": demucs_settings.mode,
                        "bgm_level": bgm_level.value,
                        "bgm_ratios": bgm_ratios,
                        "global_separation_done": use_vocals,
                        "vocals_path": str(vocals_path) if vocals_path else None,
                        "used_model": separation_strategy.initial_model if separation_strategy else None,
                        "circuit_breaker": None,
                        "retry_triggered": False
                    }
                )
                self.logger.info("æ£€æŸ¥ç‚¹å·²å¼ºåˆ¶åˆ·æ–°: åˆ†æ®µå®Œæˆï¼ˆå«demucsçŠ¶æ€ï¼‰")
            else:
                self.logger.info(f"è·³è¿‡åˆ†æ®µï¼Œä½¿ç”¨æ£€æŸ¥ç‚¹æ•°æ®ï¼ˆå…±{len(current_segments)}æ®µï¼‰")
                job.segments = current_segments  # æ¢å¤åˆ° job å¯¹è±¡
                job.total = len(current_segments)

            # ==========================================
            # 6. é˜¶æ®µ3: è½¬å½•å¤„ç†ï¼ˆåŒæ¨¡å¼ç»Ÿä¸€å¾ªç¯ + Demucsç†”æ–­ï¼‰
            # ==========================================
            self._update_progress(job, 'transcribe', 0, 'åŠ è½½æ¨¡å‹ä¸­')
            if job.canceled:
                raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

            model = self._get_model(job.settings, job)

            # è¿‡æ»¤å‡ºéœ€è¦å¤„ç†çš„æ®µ
            todo_segments = [
                seg for i, seg in enumerate(current_segments)
                if i not in processed_indices
            ]

            self.logger.info(f"å‰©ä½™ {len(todo_segments)}/{len(current_segments)} æ®µéœ€è¦è½¬å½•")
            self.logger.info(f"å¤„ç†æ¨¡å¼: {processing_mode.value}")

            # åˆå§‹åŒ–ç†”æ–­å™¨ï¼ˆå¦‚æœå¯ç”¨ä¸”å¤„äºæŒ‰éœ€åˆ†ç¦»æ¨¡å¼ï¼‰
            circuit_breaker = None
            demucs_settings = job.settings.demucs
            if (demucs_settings.enabled and
                demucs_settings.mode in ["auto", "on_demand"] and
                not use_vocals):  # åªæœ‰åœ¨æœªè¿›è¡Œå…¨å±€åˆ†ç¦»æ—¶æ‰å¯ç”¨ç†”æ–­
                circuit_breaker = CircuitBreakerState()
                circuit_breaker.total_segments = len(current_segments)
                self.logger.info("ç†”æ–­æœºåˆ¶å·²å¯ç”¨")

            try:
                for idx, seg in enumerate(current_segments):
                    # å¦‚æœå·²ç»åœ¨ processed_indices é‡Œï¼Œç›´æ¥è·³è¿‡
                    if idx in processed_indices:
                        self.logger.debug(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ®µ {idx}")
                        continue

                    # æ£€æŸ¥å–æ¶ˆå’Œæš‚åœæ ‡å¿—
                    if job.canceled:
                        raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

                    if job.paused:
                        raise RuntimeError('ä»»åŠ¡å·²æš‚åœ')

                    # ã€å†…å­˜ç›‘æ§ã€‘å®šæœŸæ£€æŸ¥å†…å­˜çŠ¶æ€ï¼ˆæ¯10æ®µæ£€æŸ¥ä¸€æ¬¡ï¼‰
                    if idx % 10 == 0 and processing_mode == ProcessingMode.MEMORY:
                        if not self._check_memory_during_transcription(job):
                            # å†…å­˜ä¸¥é‡ä¸è¶³ï¼Œä»»åŠ¡å·²æš‚åœ
                            raise RuntimeError('å†…å­˜ä¸è¶³ï¼Œä»»åŠ¡å·²æš‚åœ')

                    ratio = len(processed_indices) / max(1, len(current_segments))
                    self._update_progress(
                        job,
                        'transcribe',
                        ratio,
                        f'è½¬å½• {len(processed_indices)+1}/{len(current_segments)}'
                    )

                    # ç¡®ä¿segmentæœ‰indexå­—æ®µ
                    if 'index' not in seg:
                        seg['index'] = idx

                    # ã€ç»Ÿä¸€å…¥å£ã€‘ä½¿ç”¨å¸¦é‡è¯•çš„è½¬å½•æ–¹æ³•ï¼ˆæ”¯æŒç†”æ–­ï¼‰
                    if circuit_breaker:
                        # å¯ç”¨ç†”æ–­æ¨¡å¼ï¼šä½¿ç”¨ _transcribe_segment_with_retry
                        seg_result = self._transcribe_segment_with_retry(
                            seg,
                            model,
                            job,
                            audio_array=audio_array,
                            circuit_breaker=circuit_breaker
                        )
                    else:
                        # æœªå¯ç”¨ç†”æ–­ï¼šä½¿ç”¨åŸå§‹æ–¹æ³•
                        seg_result = self._transcribe_segment(
                            seg,
                            model,
                            job,
                            audio_array=audio_array
                        )

                    # --- æ›´æ–°å†…å­˜çŠ¶æ€ ---
                    if seg_result:
                        unaligned_results.append(seg_result)
                    processed_indices.add(idx)
                    job.processed = len(processed_indices)

                    # --- æ›´æ–°è¿›åº¦æ¡ ---
                    progress = len(processed_indices) / len(current_segments)
                    self._update_progress(
                        job,
                        'transcribe',
                        progress,
                        f'è½¬å½•ä¸­ {len(processed_indices)}/{len(current_segments)}'
                    )

                    # ã€æµå¼è¾“å‡ºã€‘ç«‹å³æ¨é€å•ä¸ªsegmentçš„è½¬å½•ç»“æœ
                    if seg_result:
                        self._push_sse_segment(job, seg_result, len(processed_indices), len(current_segments))

                    # ã€å…³é”®åŸ‹ç‚¹2ã€‘æ¯å¤„ç†ä¸€æ®µä¿å­˜ä¸€æ¬¡ï¼ˆä¿å­˜æœªå¯¹é½ç»“æœï¼‰
                    checkpoint_data = {
                        "job_id": job.job_id,
                        "phase": "transcribe",
                        "processing_mode": processing_mode.value,  # ä¿å­˜æ¨¡å¼ä¿¡æ¯
                        "total_segments": len(current_segments),
                        "processed_indices": list(processed_indices),  # setè½¬list
                        "segments": current_segments,
                        "unaligned_results": unaligned_results  # ä¿å­˜æœªå¯¹é½ç»“æœ
                    }
                    self._save_checkpoint(job_dir, checkpoint_data, job)
                    self.logger.debug(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {len(processed_indices)}/{len(current_segments)}")

                self._update_progress(job, 'transcribe', 1, 'è½¬å½•å®Œæˆ')

            except BreakToGlobalSeparation as e:
                # ç†”æ–­è§¦å‘ï¼šå‡çº§æ¨¡å‹å¹¶æ‰§è¡Œå…¨å±€äººå£°åˆ†ç¦»
                self.logger.warning(f"ç†”æ–­è§¦å‘: {e}")

                # æ¨é€ SSE äº‹ä»¶
                self._push_sse_circuit_breaker_triggered(job, circuit_breaker)

                # ã€æ–°å¢ã€‘æ£€æŸ¥æ˜¯å¦å…è®¸æ¨¡å‹å‡çº§
                if separation_strategy and separation_strategy.allow_escalation:
                    # å‡çº§åˆ° fallback æ¨¡å‹
                    fallback = separation_strategy.fallback_model
                    if fallback:
                        self.logger.info(f"æ¨¡å‹å‡çº§: {separation_strategy.initial_model} â†’ {fallback}")
                        from services.demucs_service import get_demucs_service
                        demucs_service = get_demucs_service()
                        demucs_service.set_model(fallback)

                        # æ¨é€æ¨¡å‹å‡çº§äº‹ä»¶
                        self._push_sse_model_escalated(
                            job,
                            separation_strategy.initial_model,
                            fallback,
                            "ç†”æ–­è§¦å‘ï¼Œå‡çº§åˆ°å…œåº•æ¨¡å‹",
                            circuit_breaker
                        )

                # æ‰§è¡Œå…¨å±€äººå£°åˆ†ç¦»ï¼ˆä½¿ç”¨å‡çº§åçš„æ¨¡å‹ï¼‰
                self.logger.info("å¼€å§‹æ‰§è¡Œå…¨å±€äººå£°åˆ†ç¦»ï¼ˆç†”æ–­å‡çº§ï¼‰...")
                vocals_path = self._separate_vocals_global(str(audio_path), job)

                # æ›´æ–° active_audio_path
                active_audio_path = Path(vocals_path)

                # é‡æ–°åŠ è½½åˆ†ç¦»åçš„éŸ³é¢‘
                if processing_mode == ProcessingMode.MEMORY:
                    self.logger.info("é‡æ–°åŠ è½½åˆ†ç¦»åçš„éŸ³é¢‘åˆ°å†…å­˜...")
                    audio_array = self._safe_load_audio(str(active_audio_path), job)

                # é‡ç½®çŠ¶æ€ï¼Œç»§ç»­å¤„ç†å‰©ä½™æ®µè½
                self.logger.info("ç»§ç»­å¤„ç†å‰©ä½™æ®µè½ï¼ˆä½¿ç”¨åˆ†ç¦»åçš„éŸ³é¢‘ï¼‰...")

                # ç¦ç”¨ç†”æ–­å™¨ï¼ˆé¿å…äºŒæ¬¡è§¦å‘ï¼‰
                circuit_breaker = None

                # ç»§ç»­è½¬å½•å‰©ä½™æ®µè½
                for idx, seg in enumerate(current_segments):
                    if idx in processed_indices:
                        continue

                    if job.canceled:
                        raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

                    if job.paused:
                        raise RuntimeError('ä»»åŠ¡å·²æš‚åœ')

                    # ç¡®ä¿segmentæœ‰indexå­—æ®µ
                    if 'index' not in seg:
                        seg['index'] = idx

                    # ä½¿ç”¨åˆ†ç¦»åçš„éŸ³é¢‘è½¬å½•
                    seg_result = self._transcribe_segment(
                        seg,
                        model,
                        job,
                        audio_array=audio_array
                    )

                    if seg_result:
                        unaligned_results.append(seg_result)
                    processed_indices.add(idx)
                    job.processed = len(processed_indices)

                    progress = len(processed_indices) / len(current_segments)
                    self._update_progress(
                        job,
                        'transcribe',
                        progress,
                        f'è½¬å½•ä¸­ï¼ˆå·²å‡çº§ï¼‰ {len(processed_indices)}/{len(current_segments)}'
                    )

                    if seg_result:
                        self._push_sse_segment(job, seg_result, len(processed_indices), len(current_segments))

                    # ä¿å­˜checkpoint
                    checkpoint_data = {
                        "job_id": job.job_id,
                        "phase": "transcribe",
                        "processing_mode": processing_mode.value,
                        "total_segments": len(current_segments),
                        "processed_indices": list(processed_indices),
                        "segments": current_segments,
                        "unaligned_results": unaligned_results
                    }
                    self._save_checkpoint(job_dir, checkpoint_data, job)

                self._update_progress(job, 'transcribe', 1, 'è½¬å½•å®Œæˆï¼ˆç†”æ–­åï¼‰')

            if job.canceled:
                raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

            # ==========================================
            # 7. é˜¶æ®µ4: æ‰¹æ¬¡å¯¹é½ï¼ˆä½¿ç”¨æ‰¹æ¬¡å¯¹é½+SSEè¿›åº¦æ¨é€ï¼‰
            # ==========================================
            self._update_progress(job, 'align', 0, 'å‡†å¤‡å¯¹é½...')

            # æ ¹æ®å¤„ç†æ¨¡å¼é€‰æ‹©éŸ³é¢‘æº
            if processing_mode == ProcessingMode.MEMORY and audio_array is not None:
                # å†…å­˜æ¨¡å¼ï¼šå¤ç”¨å†…å­˜æ•°ç»„ï¼ˆé¿å…é‡æ–°åŠ è½½ï¼‰
                audio_source = audio_array
                self.logger.info("å¯¹é½é˜¶æ®µï¼šå¤ç”¨å†…å­˜éŸ³é¢‘æ•°ç»„")
            else:
                # ç¡¬ç›˜æ¨¡å¼ï¼šä¼ é€’éŸ³é¢‘æ–‡ä»¶è·¯å¾„
                audio_source = str(audio_path)
                self.logger.info("å¯¹é½é˜¶æ®µï¼šä»ç£ç›˜åŠ è½½éŸ³é¢‘")

            # ä½¿ç”¨æ‰¹æ¬¡å¯¹é½æ–¹æ³•ï¼ˆæ”¯æŒSSEè¿›åº¦æ¨é€ï¼‰
            aligned_results = self._align_all_results_batched(
                unaligned_results,
                job,
                audio_source,
                processing_mode
            )

            # ã€æµå¼è¾“å‡ºã€‘æ¨é€å¯¹é½å®Œæˆäº‹ä»¶
            self._push_sse_aligned(job, aligned_results)

            if job.canceled:
                raise RuntimeError('ä»»åŠ¡å·²å–æ¶ˆ')

            # ==========================================
            # 6. é˜¶æ®µ5: ç”ŸæˆSRT
            # ==========================================
            base_name = os.path.splitext(job.filename)[0]
            srt_path = job_dir / f'{base_name}.srt'
            self._update_progress(job, 'srt', 0, 'å†™å…¥ SRT...')
            self._generate_srt(
                aligned_results,
                str(srt_path),
                job.settings.word_timestamps
            )
            self._update_progress(job, 'srt', 1, 'å¤„ç†å®Œæˆ')

            job.srt_path = str(srt_path)

            # ã€æ¸…ç†ã€‘ä»»åŠ¡æˆåŠŸå®Œæˆåï¼Œåˆ é™¤ checkpoint
            try:
                checkpoint_file = job_dir / "checkpoint.json"
                checkpoint_file.unlink(missing_ok=True)
                self.logger.info("æ£€æŸ¥ç‚¹å·²æ¸…ç†")
            except Exception as e:
                self.logger.warning(f"æ¸…ç†æ£€æŸ¥ç‚¹å¤±è´¥: {e}")

            if job.canceled:
                job.status = 'canceled'
                job.message = 'å·²å–æ¶ˆ'
                # æ¨é€å–æ¶ˆä¿¡å·
                self._push_sse_signal(job, "job_canceled", "ä»»åŠ¡å·²å–æ¶ˆ")
            else:
                job.status = 'finished'
                job.message = 'å®Œæˆ'
                self.logger.info(f"ä»»åŠ¡å®Œæˆ: {job.job_id}")
                # æ¨é€å®Œæˆä¿¡å·
                self._push_sse_signal(job, "job_complete", "è½¬å½•å®Œæˆ")

                # å¼‚æ­¥è§¦å‘åª’ä½“é¢„å¤„ç†ï¼ˆç”Ÿæˆæ³¢å½¢ã€ç¼©ç•¥å›¾ç­‰ï¼Œä¸ºç¼–è¾‘å™¨åšå‡†å¤‡ï¼‰
                self._trigger_media_post_process(job.job_id)

        except Exception as e:
            if job.canceled and 'å–æ¶ˆ' in str(e):
                job.status = 'canceled'
                job.message = 'å·²å–æ¶ˆ'
                self.logger.info(f"ğŸ›‘ ä»»åŠ¡å·²å–æ¶ˆ: {job.job_id}")
                # æ¨é€å–æ¶ˆä¿¡å·
                self._push_sse_signal(job, "job_canceled", "ä»»åŠ¡å·²å–æ¶ˆ")
            elif job.paused and 'æš‚åœ' in str(e):
                job.status = 'paused'
                job.message = 'å·²æš‚åœ'
                self.logger.info(f"â¸ï¸ ä»»åŠ¡å·²æš‚åœ: {job.job_id}")
                # æ¨é€æš‚åœä¿¡å·
                self._push_sse_signal(job, "job_paused", "ä»»åŠ¡å·²æš‚åœ")
            else:
                job.status = 'failed'
                job.message = f'å¤±è´¥: {e}'
                job.error = str(e)
                self.logger.error(f"ä»»åŠ¡å¤±è´¥: {job.job_id} - {e}", exc_info=True)
                # æ¨é€å¤±è´¥ä¿¡å·
                self._push_sse_signal(job, "job_failed", f"ä»»åŠ¡å¤±è´¥: {e}")

        finally:
            # æ¢å¤CPUäº²å’Œæ€§è®¾ç½®
            if cpu_applied:
                restored = self.cpu_manager.restore_cpu_affinity()
                if restored:
                    self.logger.info(f"ä»»åŠ¡ {job.job_id} å·²æ¢å¤CPUäº²å’Œæ€§è®¾ç½®")

            # é‡Šæ”¾å†…å­˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    # ========== æ ¸å¿ƒå¤„ç†æ–¹æ³• ==========

    def _get_audio_duration(self, audio_path: str) -> float:
        """
        è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            float: éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        """
        try:
            # æ–¹æ³•1: ä½¿ç”¨pydubï¼ˆç²¾ç¡®ä½†è¾ƒæ…¢ï¼‰
            audio = AudioSegment.from_wav(audio_path)
            duration = len(audio) / 1000.0
            self.logger.debug(f"éŸ³é¢‘æ—¶é•¿ï¼ˆpydubï¼‰: {duration:.1f}ç§’")
            return duration
        except Exception as e:
            self.logger.warning(f"pydubè·å–æ—¶é•¿å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶å¤§å°ä¼°ç®—: {e}")
            # æ–¹æ³•2: æ ¹æ®æ–‡ä»¶å¤§å°ä¼°ç®—ï¼ˆ16kHz, 16bit, mono â‰ˆ 32KB/ç§’ï¼‰
            try:
                file_size = os.path.getsize(audio_path)
                duration = file_size / 32000
                self.logger.debug(f"éŸ³é¢‘æ—¶é•¿ï¼ˆä¼°ç®—ï¼‰: {duration:.1f}ç§’")
                return duration
            except Exception as e2:
                self.logger.error(f"è·å–éŸ³é¢‘æ—¶é•¿å¤±è´¥: {e2}")
                return 0.0

    def _decide_processing_mode(self, audio_path: str, job: JobState) -> ProcessingMode:
        """
        æ™ºèƒ½å†³ç­–å¤„ç†æ¨¡å¼ï¼ˆå†…å­˜æ¨¡å¼ vs ç¡¬ç›˜æ¨¡å¼ï¼‰

        å†³ç­–é€»è¾‘ï¼š
        1. ä¼°ç®—éŸ³é¢‘å†…å­˜éœ€æ±‚
        2. æ£€æµ‹ç³»ç»Ÿå¯ç”¨å†…å­˜
        3. é¢„ç•™å®‰å…¨ä½™é‡ï¼ˆæ¨¡å‹ã€è½¬å½•ä¸­é—´å˜é‡ç­‰ï¼‰
        4. å†³å®šä½¿ç”¨å“ªç§æ¨¡å¼

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡

        Returns:
            ProcessingMode: å¤„ç†æ¨¡å¼
        """
        # è·å–éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰
        audio_duration_sec = self._get_audio_duration(audio_path)

        # ä¼°ç®—éŸ³é¢‘å†…å­˜éœ€æ±‚ (16kHz, float32)
        # å…¬å¼: duration * 16000 * 4 bytes
        estimated_audio_mb = (audio_duration_sec * 16000 * 4) / (1024 * 1024)

        # é¢„ç•™é¢å¤–å†…å­˜ï¼ˆæ¨¡å‹åŠ è½½ã€VADå¤„ç†ã€è½¬å½•ä¸­é—´å˜é‡ç­‰ï¼‰
        # ä¿å®ˆä¼°è®¡ï¼šéŸ³é¢‘å†…å­˜çš„2å€ + 500MBåŸºç¡€å¼€é”€
        total_estimated_mb = estimated_audio_mb * 2 + 500

        # è·å–ç³»ç»Ÿå¯ç”¨å†…å­˜
        mem_info = psutil.virtual_memory()
        available_mb = mem_info.available / (1024 * 1024)
        total_mb = mem_info.total / (1024 * 1024)

        # å®‰å…¨é˜ˆå€¼ï¼šåŠ¨æ€è®¡ç®—ï¼Œç»¼åˆè€ƒè™‘å¤šç§å› ç´ 
        # 1. åŸºç¡€ä¿ç•™ï¼š2GBï¼ˆä¿è¯ç³»ç»ŸåŸºæœ¬è¿è¡Œï¼‰
        # 2. åŠ¨æ€ä¿ç•™ï¼šå¯ç”¨å†…å­˜çš„10%ï¼ˆè€Œéæ€»å†…å­˜çš„20%ï¼Œé¿å…è¿‡åº¦ä¿å®ˆï¼‰
        # 3. æœ€å¤§ä¿ç•™ä¸Šé™ï¼š4GBï¼ˆé¿å…åœ¨å¤§å†…å­˜ç³»ç»Ÿä¸Šè¿‡åº¦ä¿ç•™ï¼‰
        base_reserve_mb = 2048
        dynamic_reserve_mb = available_mb * 0.1
        safety_reserve_mb = min(base_reserve_mb + dynamic_reserve_mb, 4096)
        usable_mb = available_mb - safety_reserve_mb

        self.logger.info(f"å†…å­˜è¯„ä¼°:")
        self.logger.info(f"éŸ³é¢‘æ—¶é•¿: {audio_duration_sec/60:.1f}åˆ†é’Ÿ")
        self.logger.info(f"é¢„ä¼°éœ€æ±‚: {total_estimated_mb:.0f}MB")
        self.logger.info(f"å¯ç”¨å†…å­˜: {available_mb:.0f}MB")
        self.logger.info(f"å®‰å…¨ä½™é‡: {safety_reserve_mb:.0f}MB")
        self.logger.info(f"å¯ç”¨äºå¤„ç†: {usable_mb:.0f}MB")

        # å†³ç­–
        if usable_mb >= total_estimated_mb:
            self.logger.info("é€‰æ‹©ã€å†…å­˜æ¨¡å¼ã€‘- å†…å­˜å……è¶³ï¼Œä½¿ç”¨é«˜æ€§èƒ½æ¨¡å¼")
            job.message = "å†…å­˜å……è¶³ï¼Œä½¿ç”¨é«˜æ€§èƒ½æ¨¡å¼"
            return ProcessingMode.MEMORY
        else:
            self.logger.warning(f"é€‰æ‹©ã€ç¡¬ç›˜æ¨¡å¼ã€‘- å†…å­˜ä¸è¶³ï¼ˆéœ€è¦{total_estimated_mb:.0f}MBï¼Œå¯ç”¨{usable_mb:.0f}MBï¼‰")
            job.message = "å†…å­˜å—é™ï¼Œä½¿ç”¨ç¨³å®šæ¨¡å¼"
            return ProcessingMode.DISK

    def _safe_load_audio(self, audio_path: str, job: JobState) -> np.ndarray:
        """
        å®‰å…¨åŠ è½½éŸ³é¢‘åˆ°å†…å­˜ï¼ˆå¸¦å¼‚å¸¸å¤„ç†ï¼‰

        ç”¨äºå†…å­˜æ¨¡å¼ä¸‹å°†å®Œæ•´éŸ³é¢‘ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ä¸­ã€‚
        åŒ…å«åŠ è½½éªŒè¯å’Œè¯¦ç»†çš„å¼‚å¸¸å¤„ç†ï¼ŒåŠ è½½å¤±è´¥æ—¶æŠ›å‡ºRuntimeErrorè§¦å‘é™çº§ã€‚

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡ï¼ˆç”¨äºæ›´æ–°çŠ¶æ€æ¶ˆæ¯ï¼‰

        Returns:
            np.ndarray: éŸ³é¢‘æ•°ç»„ï¼ˆfloat32, 16kHzé‡‡æ ·ç‡ï¼‰

        Raises:
            RuntimeError: éŸ³é¢‘åŠ è½½å¤±è´¥æ—¶æŠ›å‡ºï¼Œè°ƒç”¨æ–¹å¯æ®æ­¤è§¦å‘ç¡¬ç›˜æ¨¡å¼é™çº§
        """
        try:
            self.logger.info(f"åŠ è½½éŸ³é¢‘åˆ°å†…å­˜: {audio_path}")
            # ä½¿ç”¨ whisper_service æä¾›çš„ load_audio å‡½æ•°åŠ è½½éŸ³é¢‘
            audio_array = whisper_load_audio(audio_path)

            # éªŒè¯åŠ è½½ç»“æœ
            if audio_array is None or len(audio_array) == 0:
                raise ValueError("éŸ³é¢‘æ•°ç»„ä¸ºç©º")

            # è®°å½•åŠ è½½ä¿¡æ¯
            duration_sec = len(audio_array) / 16000
            memory_mb = audio_array.nbytes / (1024 * 1024)
            # Audio loaded: {duration_sec/60:.1f}åˆ†é’Ÿ")
            self.logger.info(f"å†…å­˜å ç”¨: {memory_mb:.1f}MB")
            self.logger.info(f"é‡‡æ ·ç‚¹æ•°: {len(audio_array):,}")

            return audio_array

        except MemoryError as e:
            self.logger.error(f"å†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½éŸ³é¢‘: {e}")
            job.message = "å†…å­˜ä¸è¶³ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç¡¬ç›˜æ¨¡å¼"
            raise RuntimeError(f"å†…å­˜ä¸è¶³: {e}")

        except Exception as e:
            self.logger.error(f"éŸ³é¢‘åŠ è½½å¤±è´¥: {e}")
            job.message = f"éŸ³é¢‘åŠ è½½å¤±è´¥: {e}"
            raise RuntimeError(f"éŸ³é¢‘åŠ è½½å¤±è´¥ï¼ˆå¯èƒ½æ–‡ä»¶æŸåï¼‰: {e}")

    def _split_audio_in_memory(
        self,
        audio_array: np.ndarray,
        sr: int = 16000,
        vad_config: Optional[VADConfig] = None
    ) -> List[Dict]:
        """
        å†…å­˜VADåˆ†æ®µï¼ˆä¸äº§ç”Ÿç£ç›˜IOï¼‰

        é»˜è®¤ä½¿ç”¨Silero VADï¼ˆæ— éœ€è®¤è¯ï¼‰ï¼Œå¯é€šè¿‡é…ç½®åˆ‡æ¢åˆ°Pyannote VADã€‚
        å½“VADæ¨¡å‹åŠ è½½å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨é™çº§åˆ°åŸºäºèƒ½é‡çš„ç®€æ˜“åˆ†æ®µã€‚

        Args:
            audio_array: å®Œæ•´éŸ³é¢‘æ•°ç»„ (np.ndarray, float32, 16kHz)
            sr: é‡‡æ ·ç‡ï¼ˆé»˜è®¤16000Hzï¼‰
            vad_config: VADé…ç½®ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨Sileroï¼‰

        Returns:
            List[Dict]: åˆ†æ®µå…ƒæ•°æ®åˆ—è¡¨
            [
                {"index": 0, "start": 0.0, "end": 30.5, "mode": "memory"},
                {"index": 1, "start": 30.5, "end": 58.2, "mode": "memory"},
                ...
            ]
        """
        # ä½¿ç”¨é»˜è®¤é…ç½®
        if vad_config is None:
            vad_config = VADConfig()

        self.logger.info(f"å¼€å§‹å†…å­˜VADåˆ†æ®µ (æ¨¡å‹: {vad_config.method.value})...")

        try:
            # æ ¹æ®é…ç½®é€‰æ‹©VADæ¨¡å‹
            if vad_config.method == VADMethod.SILERO:
                segments = self._vad_silero(audio_array, sr, vad_config)
            else:
                segments = self._vad_pyannote(audio_array, sr, vad_config)

            self.logger.info(f"VADåˆ†æ®µå®Œæˆ: {len(segments)}æ®µ (æ¨¡å‹: {vad_config.method.value})")
            return segments

        except Exception as e:
            self.logger.error(f"VADåˆ†æ®µå¤±è´¥: {e}")
            # é™çº§åˆ°ç®€æ˜“èƒ½é‡æ£€æµ‹
            self.logger.warning("å°è¯•é™çº§åˆ°èƒ½é‡æ£€æµ‹åˆ†æ®µ...")
            return self._energy_based_split(audio_array, sr, vad_config.chunk_size)

    def _vad_silero(
        self,
        audio_array: np.ndarray,
        sr: int,
        vad_config: VADConfig
    ) -> List[Dict]:
        """
        Silero VADåˆ†æ®µï¼ˆä½¿ç”¨å†…ç½®ONNXæ¨¡å‹ï¼Œæ— éœ€ä¸‹è½½ï¼‰

        ä¼˜ç‚¹ï¼š
        - ä½¿ç”¨é¡¹ç›®å†…ç½®ONNXæ¨¡å‹ï¼Œæ— éœ€ç½‘ç»œä¸‹è½½
        - ä½¿ç”¨ onnxruntime æ¨ç†ï¼Œè·¨å¹³å°å…¼å®¹æ€§å¥½
        - é€Ÿåº¦å¿«ï¼Œå†…å­˜å ç”¨ä½ï¼ˆ~2MBï¼‰

        Args:
            audio_array: éŸ³é¢‘æ•°ç»„
            sr: é‡‡æ ·ç‡
            vad_config: VADé…ç½®

        Returns:
            List[Dict]: åˆ†æ®µå…ƒæ•°æ®åˆ—è¡¨
        """
        self.logger.info("åŠ è½½Silero VADæ¨¡å‹ï¼ˆå†…ç½®ONNXï¼‰...")

        # ä½¿ç”¨ silero-vad åº“ï¼ˆåŸºäº onnxruntimeï¼‰
        from silero_vad import get_speech_timestamps
        from silero_vad.utils_vad import OnnxWrapper
        from pathlib import Path as PathlibPath

        # ä½¿ç”¨é¡¹ç›®å†…ç½®çš„ ONNX æ¨¡å‹
        builtin_model_path = PathlibPath(__file__).parent.parent / "assets" / "silero" / "silero_vad.onnx"

        if not builtin_model_path.exists():
            raise FileNotFoundError(
                f"å†…ç½®Silero VADæ¨¡å‹ä¸å­˜åœ¨: {builtin_model_path}\n"
                "è¯·ç¡®ä¿é¡¹ç›®å®Œæ•´ï¼Œæˆ–é‡æ–°ä»æºç ä»“åº“è·å–"
            )

        self.logger.info(f"ä½¿ç”¨å†…ç½®æ¨¡å‹: {builtin_model_path}")

        # åŠ è½½ONNXæ¨¡å‹ï¼ˆç›´æ¥ä»æœ¬åœ°è·¯å¾„ï¼‰
        model = OnnxWrapper(str(builtin_model_path), force_onnx_cpu=False)

        # è½¬æ¢ä¸ºtorch tensorï¼ˆsilero-vad éœ€è¦ï¼‰
        audio_tensor = torch.from_numpy(audio_array)

        # è·å–è¯­éŸ³æ—¶é—´æˆ³
        speech_timestamps = get_speech_timestamps(
            audio_tensor,
            model,
            sampling_rate=sr,
            threshold=vad_config.onset,                    # æ£€æµ‹é˜ˆå€¼ï¼ˆä»configè¯»å–ï¼Œé»˜è®¤0.65ï¼‰
            min_speech_duration_ms=vad_config.min_speech_duration_ms,   # æœ€å°è¯­éŸ³æ®µé•¿åº¦ï¼ˆé»˜è®¤400msï¼‰
            min_silence_duration_ms=vad_config.min_silence_duration_ms, # æœ€å°é™éŸ³é•¿åº¦ï¼ˆé»˜è®¤400msï¼‰
            return_seconds=False  # è¿”å›é‡‡æ ·ç‚¹è€Œéç§’æ•°
        )

        # VAD detection complete

        # åˆå¹¶åˆ†æ®µï¼ˆç¡®ä¿æ¯æ®µä¸è¶…è¿‡chunk_sizeç§’ï¼‰
        segments_metadata = []
        current_start = None
        current_end = None

        for ts in speech_timestamps:
            start_sec = ts['start'] / sr
            end_sec = ts['end'] / sr

            if current_start is None:
                current_start = start_sec
                current_end = end_sec
            elif (end_sec - current_start) <= vad_config.chunk_size:
                # å¯ä»¥åˆå¹¶
                current_end = end_sec
            else:
                # ä¿å­˜å½“å‰æ®µï¼Œå¼€å§‹æ–°æ®µ
                segments_metadata.append({
                    "index": len(segments_metadata),
                    "start": current_start,
                    "end": current_end,
                    "mode": "memory"
                })
                current_start = start_sec
                current_end = end_sec

        # ä¿å­˜æœ€åä¸€æ®µ
        if current_start is not None:
            segments_metadata.append({
                "index": len(segments_metadata),
                "start": current_start,
                "end": current_end,
                "mode": "memory"
            })

        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•è¯­éŸ³æ®µï¼ŒæŒ‰å›ºå®šæ—¶é•¿åˆ†æ®µ
        if len(segments_metadata) == 0:
            self.logger.warning("VADæœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œä½¿ç”¨å›ºå®šæ—¶é•¿åˆ†æ®µ")
            return self._energy_based_split(audio_array, sr, vad_config.chunk_size)

        return segments_metadata

    def _vad_pyannote(
        self,
        audio_array: np.ndarray,
        sr: int,
        vad_config: VADConfig
    ) -> List[Dict]:
        """
        Pyannote VADåˆ†æ®µï¼ˆé«˜ç²¾åº¦æ–¹æ¡ˆï¼Œéœ€è¦HF Tokenï¼‰

        ä¼˜ç‚¹ï¼š
        - ç²¾åº¦æ›´é«˜
        - æ”¯æŒæ›´å¤æ‚çš„è¯­éŸ³æ´»åŠ¨æ£€æµ‹

        æ³¨æ„ï¼š
        - éœ€è¦HuggingFace Token
        - é¦–æ¬¡ä½¿ç”¨éœ€è¦æ¥å—æ¨¡å‹ä½¿ç”¨åè®®

        Args:
            audio_array: éŸ³é¢‘æ•°ç»„
            sr: é‡‡æ ·ç‡
            vad_config: VADé…ç½®

        Returns:
            List[Dict]: åˆ†æ®µå…ƒæ•°æ®åˆ—è¡¨

        Raises:
            ValueError: æœªé…ç½®HF Tokenæ—¶æŠ›å‡º
        """
        if not vad_config.hf_token:
            raise ValueError("Pyannote VADéœ€è¦HuggingFace Tokenï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®")

        self.logger.info("åŠ è½½Pyannote VADæ¨¡å‹ï¼ˆéœ€è¦HF Tokenï¼‰...")

        try:
            from pyannote.audio import Pipeline
        except ImportError:
            raise RuntimeError("Pyannoteæœªå®‰è£…ï¼Œè¯·ä½¿ç”¨Silero VADæˆ–å®‰è£…pyannote-audio")

        # åˆå§‹åŒ–Pyannote VAD Pipeline
        pipeline = Pipeline.from_pretrained(
            "pyannote/voice-activity-detection",
            use_auth_token=vad_config.hf_token
        )

        # å‡†å¤‡è¾“å…¥ï¼ˆPyannoteéœ€è¦ç‰¹å®šæ ¼å¼ï¼‰
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºPyannoteå¤„ç†
        import tempfile
        import soundfile as sf

        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as f:
            temp_path = f.name
            sf.write(temp_path, audio_array, sr)

        try:
            # æ‰§è¡ŒVAD
            vad_result = pipeline(temp_path)

            # åˆå¹¶åˆ†æ®µ
            segments_metadata = []
            current_start = None
            current_end = None

            for speech in vad_result.get_timeline().support():
                start_sec = speech.start
                end_sec = speech.end

                if current_start is None:
                    current_start = start_sec
                    current_end = end_sec
                elif (end_sec - current_start) <= vad_config.chunk_size:
                    current_end = end_sec
                else:
                    segments_metadata.append({
                        "index": len(segments_metadata),
                        "start": current_start,
                        "end": current_end,
                        "mode": "memory"
                    })
                    current_start = start_sec
                    current_end = end_sec

            # ä¿å­˜æœ€åä¸€æ®µ
            if current_start is not None:
                segments_metadata.append({
                    "index": len(segments_metadata),
                    "start": current_start,
                    "end": current_end,
                    "mode": "memory"
                })

            return segments_metadata

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_path):
                os.unlink(temp_path)

    def _energy_based_split(
        self,
        audio_array: np.ndarray,
        sr: int,
        chunk_size: int = 30
    ) -> List[Dict]:
        """
        åŸºäºèƒ½é‡çš„ç®€æ˜“åˆ†æ®µï¼ˆé™çº§æ–¹æ¡ˆï¼‰

        å½“VADæ¨¡å‹åŠ è½½å¤±è´¥æ—¶ä½¿ç”¨ï¼ŒæŒ‰å›ºå®šæ—¶é•¿åˆ†æ®µã€‚
        ä¼šå°è¯•åœ¨é™éŸ³å¤„åˆ†å‰²ä»¥é¿å…åˆ‡æ–­è¯­éŸ³ã€‚

        Args:
            audio_array: éŸ³é¢‘æ•°ç»„
            sr: é‡‡æ ·ç‡
            chunk_size: æ¯æ®µæœ€å¤§é•¿åº¦ï¼ˆç§’ï¼‰

        Returns:
            List[Dict]: åˆ†æ®µå…ƒæ•°æ®åˆ—è¡¨
        """
        self.logger.warning("ä½¿ç”¨èƒ½é‡æ£€æµ‹é™çº§åˆ†æ®µï¼ˆå›ºå®šæ—¶é•¿ï¼‰")

        total_duration = len(audio_array) / sr
        segments_metadata = []
        pos = 0.0

        while pos < total_duration:
            # è®¡ç®—ç†æƒ³ç»“æŸä½ç½®
            ideal_end = min(pos + chunk_size, total_duration)

            # å°è¯•åœ¨é™éŸ³å¤„åˆ†å‰²ï¼ˆåœ¨ç†æƒ³ç»“æŸç‚¹å‰å1ç§’èŒƒå›´å†…å¯»æ‰¾ï¼‰
            if ideal_end < total_duration:
                search_start = max(pos, ideal_end - 1.0)
                search_end = min(total_duration, ideal_end + 1.0)

                # è®¡ç®—æœç´¢èŒƒå›´å†…çš„èƒ½é‡
                start_sample = int(search_start * sr)
                end_sample = int(search_end * sr)
                search_audio = audio_array[start_sample:end_sample]

                if len(search_audio) > 0:
                    # è®¡ç®—çŸ­æ—¶èƒ½é‡ï¼ˆæ¯100msä¸€ä¸ªçª—å£ï¼‰
                    window_size = int(0.1 * sr)
                    energies = []
                    for i in range(0, len(search_audio) - window_size, window_size):
                        window = search_audio[i:i + window_size]
                        energy = np.sum(window ** 2)
                        energies.append((i, energy))

                    if energies:
                        # æ‰¾åˆ°èƒ½é‡æœ€ä½çš„ç‚¹
                        min_energy_idx = min(energies, key=lambda x: x[1])[0]
                        actual_end = search_start + (min_energy_idx / sr)
                        # ç¡®ä¿åˆ†æ®µè‡³å°‘æœ‰1ç§’
                        if actual_end - pos >= 1.0:
                            ideal_end = actual_end

            segments_metadata.append({
                "index": len(segments_metadata),
                "start": pos,
                "end": ideal_end,
                "mode": "memory"
            })
            pos = ideal_end

        self.logger.info(f"èƒ½é‡æ£€æµ‹åˆ†æ®µå®Œæˆ: {len(segments_metadata)}æ®µ")
        return segments_metadata

    # ==========================================
    # Demucs äººå£°åˆ†ç¦»ç›¸å…³æ–¹æ³•
    # ==========================================

    def _detect_bgm(self, audio_path: str, job: JobState):
        """
        æ‰§è¡ŒBGMæ£€æµ‹ï¼Œæ›´æ–°è¿›åº¦

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡

        Returns:
            Tuple[BGMLevel, List[float]]: (BGMå¼ºåº¦çº§åˆ«, å„é‡‡æ ·ç‚¹çš„BGMæ¯”ä¾‹åˆ—è¡¨)
        """
        from services.demucs_service import get_demucs_service, BGMLevel

        self._update_progress(job, 'bgm_detect', 0, 'BGMæ£€æµ‹ä¸­...')

        try:
            demucs = get_demucs_service()

            # æ‰§è¡ŒBGMæ£€æµ‹
            level, ratios = demucs.detect_background_music_level(audio_path)

            self._update_progress(job, 'bgm_detect', 1, f'BGMæ£€æµ‹å®Œæˆ: {level.value}')

            # æ¨é€SSEäº‹ä»¶
            self._push_sse_bgm_detected(job, level, ratios)

            self.logger.info(
                f"BGMæ£€æµ‹ç»“æœ: {level.value}, "
                f"æ¯”ä¾‹={ratios}, æœ€å¤§={max(ratios) if ratios else 0:.2f}"
            )

            return level, ratios

        except Exception as e:
            self.logger.warning(f"BGMæ£€æµ‹å¤±è´¥ï¼Œå°†è·³è¿‡Demucs: {e}")
            # å¤±è´¥æ—¶è¿”å› NONE çº§åˆ«ï¼Œä¸å½±å“ä¸»æµç¨‹
            from services.demucs_service import BGMLevel
            return BGMLevel.NONE, []

    def _separate_vocals_global(self, audio_path: str, job: JobState) -> str:
        """
        æ‰§è¡Œå…¨å±€äººå£°åˆ†ç¦»ï¼Œæ›´æ–°è¿›åº¦

        Args:
            audio_path: åŸå§‹éŸ³é¢‘è·¯å¾„
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡

        Returns:
            str: åˆ†ç¦»åçš„äººå£°æ–‡ä»¶è·¯å¾„
        """
        from services.demucs_service import get_demucs_service

        self._update_progress(job, 'demucs_global', 0, 'äººå£°åˆ†ç¦»ä¸­...')

        try:
            demucs = get_demucs_service()

            def progress_callback(progress: float, message: str):
                """è¿›åº¦å›è°ƒ"""
                self._update_progress(job, 'demucs_global', progress, message)

            # æ‰§è¡Œäººå£°åˆ†ç¦»
            vocals_path = demucs.separate_vocals(
                audio_path,
                progress_callback=progress_callback
            )

            self._update_progress(job, 'demucs_global', 1, 'äººå£°åˆ†ç¦»å®Œæˆ')
            self.logger.info(f"å…¨å±€äººå£°åˆ†ç¦»å®Œæˆ: {vocals_path}")

            return vocals_path

        except Exception as e:
            self.logger.error(f"äººå£°åˆ†ç¦»å¤±è´¥: {e}")
            # åˆ†ç¦»å¤±è´¥æ—¶è¿”å›åŸå§‹éŸ³é¢‘è·¯å¾„ï¼ˆé™çº§å¤„ç†ï¼‰
            self.logger.warning("äººå£°åˆ†ç¦»å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸå§‹éŸ³é¢‘ç»§ç»­å¤„ç†")
            return audio_path

    def _push_sse_bgm_detected(self, job: JobState, level, ratios):
        """
        æ¨é€BGMæ£€æµ‹ç»“æœäº‹ä»¶

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            level: BGMå¼ºåº¦çº§åˆ«
            ratios: å„é‡‡æ ·ç‚¹çš„BGMæ¯”ä¾‹åˆ—è¡¨
        """
        try:
            from services.sse_service import get_sse_manager

            sse_manager = get_sse_manager()
            channel_id = f"job:{job.job_id}"

            # å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œé¿å…JSONåºåˆ—åŒ–é”™è¯¯
            native_ratios = [float(r) for r in ratios] if ratios else []
            max_ratio = float(max(ratios)) if ratios else 0.0

            # æ„é€ äº‹ä»¶æ•°æ®
            event_data = {
                "level": level.value,
                "ratios": native_ratios,
                "max_ratio": max_ratio,
                "recommendation": self._get_demucs_recommendation(level)
            }

            # å¹¿æ’­äº‹ä»¶
            sse_manager.broadcast_sync(channel_id, "bgm_detected", event_data)

        except Exception as e:
            # SSEæ¨é€å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
            self.logger.debug(f"SSEæ¨é€å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

    def _push_sse_separation_strategy(self, job: JobState, strategy):
        """
        æ¨é€åˆ†ç¦»ç­–ç•¥å†³ç­–äº‹ä»¶

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            strategy: SeparationStrategy å¯¹è±¡
        """
        try:
            from services.sse_service import get_sse_manager

            sse_manager = get_sse_manager()
            channel_id = f"job:{job.job_id}"

            # ä½¿ç”¨ strategy.to_dict() è·å–äº‹ä»¶æ•°æ®
            event_data = strategy.to_dict()

            # å¹¿æ’­äº‹ä»¶
            sse_manager.broadcast_sync(channel_id, "separation_strategy", event_data)

            self.logger.debug(f"åˆ†ç¦»ç­–ç•¥äº‹ä»¶å·²æ¨é€: {strategy.reason}")

        except Exception as e:
            # SSEæ¨é€å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
            self.logger.debug(f"SSEæ¨é€å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

    def _push_sse_model_escalated(
        self,
        job: JobState,
        from_model: str,
        to_model: str,
        reason: str,
        breaker_state: CircuitBreakerState
    ):
        """
        æ¨é€æ¨¡å‹å‡çº§äº‹ä»¶

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            from_model: åŸæ¨¡å‹åç§°
            to_model: æ–°æ¨¡å‹åç§°
            reason: å‡çº§åŸå› 
            breaker_state: ç†”æ–­å™¨çŠ¶æ€
        """
        try:
            from services.sse_service import get_sse_manager

            sse_manager = get_sse_manager()
            channel_id = f"job:{job.job_id}"

            # æ„é€ äº‹ä»¶æ•°æ®
            event_data = {
                "from_model": from_model,
                "to_model": to_model,
                "reason": reason,
                "escalation_count": breaker_state.escalation_count,
                "max_escalations": job.settings.demucs.max_escalations,
                "stats": breaker_state.get_stats()
            }

            # å¹¿æ’­äº‹ä»¶
            sse_manager.broadcast_sync(channel_id, "model_escalated", event_data)

            self.logger.info(f"æ¨¡å‹å‡çº§äº‹ä»¶å·²æ¨é€: {from_model} -> {to_model}")

        except Exception as e:
            # SSEæ¨é€å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
            self.logger.debug(f"SSEæ¨é€å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

    def _push_sse_circuit_breaker_triggered(
        self,
        job: JobState,
        circuit_breaker: Optional[CircuitBreakerState]
    ):
        """
        æ¨é€ç†”æ–­è§¦å‘äº‹ä»¶ï¼ˆPhase 4: æ‰©å±•åŒ…å«å‡çº§ä¿¡æ¯ï¼‰

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            circuit_breaker: ç†”æ–­å™¨çŠ¶æ€å¯¹è±¡
        """
        try:
            from services.sse_service import get_sse_manager

            sse_manager = get_sse_manager()
            channel_id = f"job:{job.job_id}"

            # æ„é€ äº‹ä»¶æ•°æ®ï¼ˆæ‰©å±•åŒ…å«å‡çº§å†å²ï¼‰
            stats = circuit_breaker.get_stats() if circuit_breaker else {}
            event_data = {
                "triggered": True,
                "reason": self._get_circuit_break_reason(circuit_breaker),
                "stats": stats,
                "action": "å‡çº§ä¸ºå…¨å±€äººå£°åˆ†ç¦»æ¨¡å¼"
            }

            # å¹¿æ’­äº‹ä»¶
            sse_manager.broadcast_sync(channel_id, "circuit_breaker_triggered", event_data)

            self.logger.info("ç†”æ–­äº‹ä»¶å·²æ¨é€åˆ°å‰ç«¯")

        except Exception as e:
            # SSEæ¨é€å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
            self.logger.debug(f"SSEæ¨é€å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")

    def _get_circuit_break_reason(self, circuit_breaker: Optional[CircuitBreakerState]) -> str:
        """
        ç”Ÿæˆç†”æ–­åŸå› æè¿°

        Args:
            circuit_breaker: ç†”æ–­å™¨çŠ¶æ€

        Returns:
            ç†”æ–­åŸå› æè¿°å­—ç¬¦ä¸²
        """
        if not circuit_breaker:
            return "è½¬å½•è´¨é‡ä½ï¼Œè§¦å‘ç†”æ–­å‡çº§"

        stats = circuit_breaker.get_stats()

        # å¦‚æœæœ‰å‡çº§å†å²ï¼Œè¯´æ˜å·²ç»å°è¯•è¿‡å‡çº§
        if circuit_breaker.escalation_count > 0:
            return (
                f"å·²å‡çº§ {circuit_breaker.escalation_count} æ¬¡æ¨¡å‹ä»æœªæ”¹å–„ï¼Œè§¦å‘ç†”æ–­ã€‚"
                f"å‡çº§å†å²: {', '.join(circuit_breaker.escalation_history)}"
            )
        else:
            return (
                f"è¿ç»­ {stats['consecutive_retries']} ä¸ªæ®µè½é‡è¯•å¤±è´¥ï¼Œ"
                f"æ€»é‡è¯•ç‡ {stats['retry_ratio']:.1%}ï¼Œè§¦å‘ç†”æ–­"
            )

    def _get_demucs_recommendation(self, level) -> str:
        """
        æ ¹æ®BGMçº§åˆ«è¿”å›å»ºè®®çš„å¤„ç†æ¨¡å¼

        Args:
            level: BGMå¼ºåº¦çº§åˆ«

        Returns:
            str: å»ºè®®çš„å¤„ç†æ¨¡å¼æè¿°
        """
        from services.demucs_service import BGMLevel

        if level == BGMLevel.HEAVY:
            return "å…¨å±€åˆ†ç¦»"
        elif level == BGMLevel.LIGHT:
            return "æŒ‰éœ€åˆ†ç¦»"
        else:
            return "æ— éœ€åˆ†ç¦»"

    # ==========================================
    # æŒ‰éœ€åˆ†ç¦»ä¸ç†”æ–­æœºåˆ¶ç›¸å…³æ–¹æ³•
    # ==========================================

    def _check_transcription_confidence(
        self,
        result: Dict,
        logprob_threshold: float,
        no_speech_threshold: float
    ) -> bool:
        """
        æ£€æŸ¥è½¬å½•ç»“æœçš„ç½®ä¿¡åº¦

        Args:
            result: è½¬å½•ç»“æœå­—å…¸
            logprob_threshold: logprobé˜ˆå€¼ï¼ˆä½äºæ­¤å€¼éœ€è¦é‡è¯•ï¼‰
            no_speech_threshold: no_speech_probé˜ˆå€¼ï¼ˆé«˜äºæ­¤å€¼éœ€è¦é‡è¯•ï¼‰

        Returns:
            bool: Trueè¡¨ç¤ºç½®ä¿¡åº¦ä½ï¼Œéœ€è¦é‡è¯•
        """
        segments = result.get('segments', [])

        if not segments:
            return True  # æ²¡æœ‰è¯†åˆ«å‡ºå†…å®¹ï¼Œéœ€è¦é‡è¯•

        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        total_logprob = 0
        total_no_speech = 0
        count = 0

        for seg in segments:
            if 'avg_logprob' in seg:
                total_logprob += seg['avg_logprob']
                count += 1
            if 'no_speech_prob' in seg:
                total_no_speech += seg['no_speech_prob']

        if count == 0:
            return False  # æ²¡æœ‰ç½®ä¿¡åº¦ä¿¡æ¯ï¼Œä¸é‡è¯•

        avg_logprob = total_logprob / count
        avg_no_speech = total_no_speech / count if count > 0 else 0

        # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
        if avg_logprob < logprob_threshold:
            self.logger.debug(f"avg_logprob={avg_logprob:.2f} < {logprob_threshold}, éœ€è¦é‡è¯•")
            return True

        if avg_no_speech > no_speech_threshold:
            self.logger.debug(f"no_speech_prob={avg_no_speech:.2f} > {no_speech_threshold}, éœ€è¦é‡è¯•")
            return True

        return False

    def _is_better_result(self, new_result: Dict, old_result: Dict) -> bool:
        """
        æ¯”è¾ƒä¸¤ä¸ªè½¬å½•ç»“æœï¼Œåˆ¤æ–­æ–°ç»“æœæ˜¯å¦æ›´å¥½

        Args:
            new_result: æ–°çš„è½¬å½•ç»“æœ
            old_result: æ—§çš„è½¬å½•ç»“æœ

        Returns:
            bool: Trueè¡¨ç¤ºæ–°ç»“æœæ›´å¥½
        """
        new_segments = new_result.get('segments', [])
        old_segments = old_result.get('segments', [])

        # å¦‚æœæ–°ç»“æœæ²¡æœ‰å†…å®¹ï¼Œæ—§çš„æ›´å¥½
        if not new_segments:
            return False

        # å¦‚æœæ—§ç»“æœæ²¡æœ‰å†…å®¹ï¼Œæ–°çš„æ›´å¥½
        if not old_segments:
            return True

        # æ¯”è¾ƒå¹³å‡logprob
        def get_avg_logprob(segments):
            logprobs = [s.get('avg_logprob', -1) for s in segments if 'avg_logprob' in s]
            return np.mean(logprobs) if logprobs else -1

        new_logprob = get_avg_logprob(new_segments)
        old_logprob = get_avg_logprob(old_segments)

        # æ–°ç»“æœçš„logprobæ›´é«˜ï¼ˆæ›´æ¥è¿‘0ï¼‰åˆ™æ›´å¥½
        return new_logprob > old_logprob

    def _transcribe_segment_with_retry(
        self,
        seg_meta: Dict,
        model,
        job: JobState,
        audio_array: Optional[np.ndarray] = None,
        circuit_breaker: Optional[CircuitBreakerState] = None
    ) -> Optional[Dict]:
        """
        å¸¦é‡è¯•çš„è½¬å½•æ–¹æ³•ï¼ˆæ”¯æŒDemucsäººå£°åˆ†ç¦»é‡è¯• + åŠ¨æ€ç†”æ–­ï¼‰

        æµç¨‹ï¼š
        1. é¦–æ¬¡è½¬å½•ï¼ˆä½¿ç”¨åŸå§‹éŸ³é¢‘ï¼‰
        2. æ£€æŸ¥ç½®ä¿¡åº¦
        3. å¦‚æœç½®ä¿¡åº¦ä½ï¼Œä½¿ç”¨Demucsåˆ†ç¦»äººå£°åé‡è¯•
        4. æ›´æ–°ç†”æ–­å™¨çŠ¶æ€
        5. æ£€æŸ¥æ˜¯å¦è§¦å‘ç†”æ–­
        6. è¿”å›ç½®ä¿¡åº¦æ›´é«˜çš„ç»“æœ

        Args:
            seg_meta: æ®µè½å…ƒæ•°æ®
            model: Whisperæ¨¡å‹
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            audio_array: å®Œæ•´éŸ³é¢‘æ•°ç»„ï¼ˆå†…å­˜æ¨¡å¼ï¼‰
            circuit_breaker: ç†”æ–­å™¨çŠ¶æ€å¯¹è±¡

        Returns:
            Optional[Dict]: è½¬å½•ç»“æœ

        Raises:
            BreakToGlobalSeparation: å½“è§¦å‘ç†”æ–­æ¡ä»¶æ—¶æŠ›å‡º
        """
        demucs_settings = job.settings.demucs

        # é¦–æ¬¡è½¬å½•
        result = self._transcribe_segment(seg_meta, model, job, audio_array)

        if not result or not demucs_settings.enabled:
            if circuit_breaker:
                circuit_breaker.record_success()
            return result

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
        needs_retry = self._check_transcription_confidence(
            result,
            demucs_settings.retry_threshold_logprob,
            demucs_settings.retry_threshold_no_speech
        )

        if not needs_retry:
            # ä¸éœ€è¦é‡è¯•ï¼Œè®°å½•æˆåŠŸ
            if circuit_breaker:
                circuit_breaker.record_success()
            return result

        # ========== éœ€è¦é‡è¯•çš„é€»è¾‘ ==========
        self.logger.info(f"æ®µè½ {seg_meta['index']} ç½®ä¿¡åº¦ä½ï¼Œå°è¯•äººå£°åˆ†ç¦»é‡è¯•")

        # æ›´æ–°ç†”æ–­å™¨çŠ¶æ€
        if circuit_breaker:
            circuit_breaker.record_retry()

            # æ£€æŸ¥æ˜¯å¦è§¦å‘ç†”æ–­
            if circuit_breaker.should_break(demucs_settings):
                stats = circuit_breaker.get_stats()
                self.logger.warning(
                    f"è§¦å‘ç†”æ–­ï¼è¿ç»­é‡è¯•={stats['consecutive_retries']}, "
                    f"æ€»é‡è¯•æ¯”ä¾‹={stats['retry_ratio']:.1%}"
                )
                raise BreakToGlobalSeparation(
                    f"è¿ç»­{stats['consecutive_retries']}æ®µéœ€è¦Demucsé‡è¯•ï¼Œ"
                    f"å»ºè®®å‡çº§ä¸ºå…¨å±€äººå£°åˆ†ç¦»æ¨¡å¼"
                )

        # å°è¯•æŒ‰éœ€åˆ†ç¦»
        try:
            from services.demucs_service import get_demucs_service
            demucs = get_demucs_service()

            start_sec = seg_meta['start']
            end_sec = seg_meta['end']

            if audio_array is not None:
                # å†…å­˜æ¨¡å¼ï¼šåˆ†ç¦»äººå£°
                vocals = demucs.separate_vocals_segment(
                    audio_array, sr=16000,
                    start_sec=start_sec, end_sec=end_sec
                )

                # æ„é€ ä¸´æ—¶seg_meta
                retry_seg = seg_meta.copy()
                retry_seg['start'] = 0  # å› ä¸ºvocalså·²ç»æ˜¯åˆ‡ç‰‡
                retry_seg['end'] = len(vocals) / 16000

                # é‡æ–°è½¬å½•
                retry_result = self._transcribe_segment_in_memory(
                    vocals,
                    retry_seg,
                    model,
                    job,
                    is_vocals=True  # æ ‡è®°æ˜¯äººå£°
                )
            else:
                # ç¡¬ç›˜æ¨¡å¼ï¼šæš‚ä¸æ”¯æŒ
                self.logger.warning("ç¡¬ç›˜æ¨¡å¼æš‚ä¸æ”¯æŒDemucsé‡è¯•")
                return result

            if retry_result:
                # æ ¡æ­£æ—¶é—´åç§»ï¼ˆæ¢å¤åˆ°åŸå§‹æ—¶é—´è½´ï¼‰
                original_start = seg_meta['start']
                for seg in retry_result.get('segments', []):
                    seg['start'] += original_start
                    seg['end'] += original_start

                # æ¯”è¾ƒä¸¤æ¬¡ç»“æœï¼Œè¿”å›æ›´å¥½çš„
                if self._is_better_result(retry_result, result):
                    self.logger.info(f"æ®µè½ {seg_meta['index']} é‡è¯•æˆåŠŸï¼Œä½¿ç”¨åˆ†ç¦»åçš„ç»“æœ")
                    retry_result['used_demucs'] = True
                    return retry_result

        except Exception as e:
            self.logger.warning(f"Demucsé‡è¯•å¤±è´¥: {e}")

        return result

    def _extract_audio(self, input_file: str, audio_out: str) -> bool:
        """
        ä½¿ç”¨FFmpegæå–éŸ³é¢‘

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            audio_out: è¾“å‡ºéŸ³é¢‘è·¯å¾„

        Returns:
            bool: æ˜¯å¦æå–æˆåŠŸ
        """
        if os.path.exists(audio_out):
            self.logger.debug(f"éŸ³é¢‘æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡æå–: {audio_out}")
            return True

        # ä½¿ç”¨é…ç½®ä¸­çš„FFmpegå‘½ä»¤ï¼ˆæ”¯æŒç‹¬ç«‹æ‰“åŒ…ï¼‰
        ffmpeg_cmd = config.get_ffmpeg_command()
        self.logger.debug(f"ä½¿ç”¨FFmpeg: {ffmpeg_cmd}")

        cmd = [
            ffmpeg_cmd, '-y', '-i', input_file,
            '-vn',                    # ä»…éŸ³é¢‘
            '-ac', '1',               # å•å£°é“
            '-ar', '16000',           # 16kHz é‡‡æ ·ç‡
            '-acodec', 'pcm_s16le',   # PCM ç¼–ç 
            audio_out
        ]

        try:
            proc = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )

            if proc.returncode == 0 and os.path.exists(audio_out):
                self.logger.debug(f"éŸ³é¢‘æå–æˆåŠŸ: {audio_out}")
                return True
            else:
                error_msg = proc.stderr.decode('utf-8', errors='ignore')
                self.logger.error(f"FFmpegæ‰§è¡Œå¤±è´¥: {error_msg}")
                return False

        except subprocess.TimeoutExpired:
            self.logger.error("FFmpegè¶…æ—¶")
            return False
        except Exception as e:
            self.logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
            return False

    def _split_audio_to_disk(self, audio_path: str) -> List[Dict]:
        """
        ç¡¬ç›˜åˆ†æ®µæ¨¡å¼ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰

        ä½¿ç”¨pydubè¿›è¡Œé™éŸ³æ£€æµ‹ï¼Œç”Ÿæˆsegment_N.wavæ–‡ä»¶ã€‚
        é€‚ç”¨äºå†…å­˜ä¸è¶³çš„åœºæ™¯ã€‚

        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        Returns:
            List[Dict]: åˆ†æ®µä¿¡æ¯åˆ—è¡¨ï¼Œä¸å†…å­˜æ¨¡å¼æ ¼å¼ç»Ÿä¸€
            [
                {"index": 0, "file": "segment_0.wav", "start": 0.0, "end": 30.0, "start_ms": 0, "duration_ms": 30000, "mode": "disk"},
                ...
            ]
        """
        self.logger.info("å¼€å§‹ç¡¬ç›˜åˆ†æ®µï¼ˆpydubé™éŸ³æ£€æµ‹ï¼‰...")

        # ä½¿ç”¨é…ç½®ä¸­çš„éŸ³é¢‘å¤„ç†å‚æ•°
        audio_config = config.get_audio_config()
        SEGMENT_LEN_MS = audio_config['segment_length_ms']
        SILENCE_SEARCH_MS = audio_config['silence_search_ms']
        MIN_SILENCE_LEN_MS = audio_config['min_silence_len_ms']
        SILENCE_THRESH_DBFS = audio_config['silence_threshold_dbfs']

        audio = AudioSegment.from_wav(audio_path)
        length = len(audio)
        segments = []
        pos = 0
        idx = 0

        while pos < length:
            end = min(pos + SEGMENT_LEN_MS, length)

            # æ™ºèƒ½å¯»æ‰¾é™éŸ³ç‚¹ï¼ˆé¿å…åœ¨å¥å­ä¸­é—´åˆ†å‰²ï¼‰
            if end < length and (end - pos) > SILENCE_SEARCH_MS:
                search_start = max(pos, end - SILENCE_SEARCH_MS)
                search_chunk = audio[search_start:end]

                try:
                    silences = silence.detect_silence(
                        search_chunk,
                        min_silence_len=MIN_SILENCE_LEN_MS,
                        silence_thresh=SILENCE_THRESH_DBFS
                    )

                    if silences:
                        # ä½¿ç”¨ç¬¬ä¸€ä¸ªé™éŸ³ç‚¹
                        silence_start = silences[0][0]
                        new_end = search_start + silence_start
                        if new_end - pos > MIN_SILENCE_LEN_MS:
                            end = new_end
                except Exception as e:
                    self.logger.warning(f"silence detection failed: {e}")

            # å¯¼å‡ºåˆ†æ®µæ–‡ä»¶
            chunk = audio[pos:end]
            seg_file = os.path.join(os.path.dirname(audio_path), f'segment_{idx}.wav')
            chunk.export(seg_file, format='wav')

            # ç»Ÿä¸€è¿”å›æ ¼å¼ï¼ˆä¸å†…å­˜æ¨¡å¼ä¸€è‡´ï¼‰
            segments.append({
                'index': idx,                    # æ–°å¢ï¼šåˆ†æ®µç´¢å¼•
                'file': seg_file,
                'start': pos / 1000.0,           # æ–°å¢ï¼šèµ·å§‹æ—¶é—´ï¼ˆç§’ï¼‰
                'end': end / 1000.0,             # æ–°å¢ï¼šç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
                'start_ms': pos,                 # ä¿ç•™ï¼šå…¼å®¹æ—§ä»£ç 
                'duration_ms': end - pos,        # ä¿ç•™ï¼šå…¼å®¹æ—§ä»£ç 
                'mode': 'disk'                   # æ–°å¢ï¼šæ¨¡å¼æ ‡è®°
            })

            pos = end
            idx += 1

        self.logger.info(f"disk segmentation complete: {len(segments)} segments (with segment files)")
        return segments

    # å…¼å®¹æ€§åˆ«åï¼šä¿ç•™æ—§æ–¹æ³•åï¼ˆæŒ‡å‘æ–°æ–¹æ³•ï¼‰
    def _split_audio(self, audio_path: str) -> List[Dict]:
        """å…¼å®¹æ€§åˆ«åï¼ŒæŒ‡å‘ _split_audio_to_disk()"""
        return self._split_audio_to_disk(audio_path)

    def _get_model(self, settings: JobSettings, job: Optional[JobState] = None):
        """
        è·å– Faster-Whisper æ¨¡å‹ï¼ˆå¸¦ç¼“å­˜ï¼‰

        ä¼˜å…ˆä½¿ç”¨æ¨¡å‹ç®¡ç†æœåŠ¡æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨ç®€å•ç¼“å­˜

        Args:
            settings: ä»»åŠ¡è®¾ç½®
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡(å¯é€‰,ç”¨äºæ›´æ–°ä¸‹è½½è¿›åº¦)

        Returns:
            æ¨¡å‹å¯¹è±¡
        """
        # å°è¯•ä½¿ç”¨æ¨¡å‹ç®¡ç†æœåŠ¡æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹
        try:
            from services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()
            whisper_model_info = model_mgr.whisper_models.get(settings.model)

            if whisper_model_info:
                # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
                if whisper_model_info.status == "not_downloaded" or whisper_model_info.status == "incomplete":
                    self.logger.warning(f"Whisperæ¨¡å‹æœªä¸‹è½½æˆ–ä¸å®Œæ•´: {settings.model}")

                    # è·å–æ¨¡å‹å¤§å°ä¿¡æ¯
                    model_size_mb = whisper_model_info.size_mb

                    # å¦‚æœæ¨¡å‹å¤§å°>=1GB,ç»™å‡ºç‰¹æ®Šæç¤º
                    download_msg = ""
                    if model_size_mb >= 1024:
                        size_gb = model_size_mb / 1024
                        download_msg = f"å½“å‰ä¸‹è½½æ¨¡å‹å¤§äº1GB ({size_gb:.1f}GB),è¯·è€å¿ƒç­‰å¾…"
                        self.logger.info(f"{download_msg}")
                    else:
                        download_msg = f"å¼€å§‹ä¸‹è½½æ¨¡å‹ {settings.model} ({model_size_mb}MB)"

                    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                    if job:
                        job.message = download_msg

                    self.logger.info(f"è‡ªåŠ¨è§¦å‘ä¸‹è½½Whisperæ¨¡å‹: {settings.model} ({model_size_mb}MB)")

                    # è§¦å‘ä¸‹è½½
                    success = model_mgr.download_whisper_model(settings.model)
                    if not success:
                        self.logger.warning(f"æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥æˆ–å·²åœ¨ä¸‹è½½ä¸­,ä½¿ç”¨å¤‡ç”¨æ–¹å¼")
                        raise RuntimeError("æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥")

                    # ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆæœ€å¤šç­‰å¾…10åˆ†é’Ÿï¼‰
                    import time
                    max_wait_time = 600  # 10åˆ†é’Ÿ
                    wait_interval = 5  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                    elapsed = 0

                    while elapsed < max_wait_time:
                        time.sleep(wait_interval)
                        elapsed += wait_interval

                        current_status = model_mgr.whisper_models[settings.model].status
                        progress = model_mgr.whisper_models[settings.model].download_progress

                        if current_status == "ready":
                            self.logger.info(f"Whisperæ¨¡å‹ä¸‹è½½å®Œæˆ: {settings.model}")
                            if job:
                                job.message = f"æ¨¡å‹ä¸‹è½½å®Œæˆ,å‡†å¤‡åŠ è½½"
                            break
                        elif current_status == "error":
                            self.logger.error(f"æ¨¡å‹ç®¡ç†å™¨ä¸‹è½½å¤±è´¥,ä½¿ç”¨å¤‡ç”¨æ–¹å¼")
                            raise RuntimeError(f"Whisperæ¨¡å‹ä¸‹è½½å¤±è´¥: {settings.model}")
                        else:
                            # å¦‚æœæ¨¡å‹å¤§å°>=1GB,å®šæœŸæé†’ç”¨æˆ·è€å¿ƒç­‰å¾…
                            if model_size_mb >= 1024 and elapsed % 30 == 0:  # æ¯30ç§’æé†’ä¸€æ¬¡
                                wait_msg = f"å½“å‰ä¸‹è½½æ¨¡å‹å¤§äº1GB,è¯·è€å¿ƒç­‰å¾…... {progress:.1f}% ({elapsed}s/{max_wait_time}s)"
                                self.logger.info(f"{wait_msg}")
                                if job:
                                    job.message = wait_msg
                            else:
                                wait_msg = f"ç­‰å¾…æ¨¡å‹ä¸‹è½½... {progress:.1f}%"
                                self.logger.info(f"{wait_msg} ({elapsed}s/{max_wait_time}s)")
                                # æ›´æ–°ä»»åŠ¡çŠ¶æ€(æ¯æ¬¡éƒ½æ›´æ–°,è¿™æ ·ç”¨æˆ·å¯ä»¥çœ‹åˆ°è¿›åº¦å˜åŒ–)
                                if job:
                                    job.message = wait_msg

                    if elapsed >= max_wait_time:
                        self.logger.error(f"æ¨¡å‹ä¸‹è½½è¶…æ—¶,ä½¿ç”¨å¤‡ç”¨æ–¹å¼")
                        raise TimeoutError(f"Whisperæ¨¡å‹ä¸‹è½½è¶…æ—¶: {settings.model}")

        except Exception as e:
            self.logger.warning(f"æ¨¡å‹ç®¡ç†æœåŠ¡æ£€æŸ¥å¤±è´¥,ä½¿ç”¨å¤‡ç”¨æ–¹å¼: {e}")

        # å°è¯•ä½¿ç”¨æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨
        try:
            from services.model_preload_manager import get_model_manager as get_preload_manager
            model_manager = get_preload_manager()
            if model_manager:
                self.logger.debug("ä½¿ç”¨æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨è·å–æ¨¡å‹")
                if job:
                    job.message = "åŠ è½½æ¨¡å‹ä¸­"
                return model_manager.get_model(settings)
        except Exception as e:
            self.logger.debug(f"æ— æ³•ä½¿ç”¨æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨ï¼Œå›é€€åˆ°æœ¬åœ°ç¼“å­˜: {e}")
            pass

        # å›é€€åˆ°ç®€å•ç¼“å­˜æœºåˆ¶
        key = (settings.model, settings.compute_type, settings.device)
        with _model_lock:
            if key in _model_cache:
                self.logger.debug(f"å‘½ä¸­æ¨¡å‹ç¼“å­˜: {key}")
                if job:
                    job.message = "ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹"
                return _model_cache[key]

            self.logger.info(f"åŠ è½½æ¨¡å‹: {key}")
            if job:
                job.message = f"åŠ è½½æ¨¡å‹ {settings.model}"

            # é¦–å…ˆå°è¯•ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ (ä½¿ç”¨ Faster-Whisper)
            try:
                from core.config import config
                from faster_whisper import WhisperModel
                m = WhisperModel(
                    settings.model,
                    device=settings.device,
                    compute_type=settings.compute_type,
                    download_root=str(config.HF_CACHE_DIR),
                    local_files_only=True  # ç¦æ­¢è‡ªåŠ¨ä¸‹è½½ï¼Œåªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                )
                _model_cache[key] = m
                if job:
                    job.message = "æ¨¡å‹åŠ è½½å®Œæˆ"
                return m
            except Exception as e:
                self.logger.warning(f"æœ¬åœ°åŠ è½½å¤±è´¥,å…è®¸ä¸‹è½½: {e}")
                if job:
                    job.message = "æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨,æ­£åœ¨ä¸‹è½½"
                # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥,å…è®¸ä¸‹è½½
                m = WhisperModel(
                    settings.model,
                    device=settings.device,
                    compute_type=settings.compute_type,
                    download_root=str(config.HF_CACHE_DIR),
                    local_files_only=False  # å…è®¸ä¸‹è½½
                )
                _model_cache[key] = m
                if job:
                    job.message = "æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ"
                return m

  
    def _transcribe_segment_unaligned(
        self,
        seg: Dict,
        model,
        job: JobState
    ) -> Optional[Dict]:
        """
        è½¬å½•å•ä¸ªéŸ³é¢‘æ®µï¼ˆä»…è½¬å½•ï¼Œä¸å¯¹é½ï¼‰

        Args:
            seg: æ®µä¿¡æ¯ {file, start_ms, duration_ms, index}
            model: Faster-Whisper æ¨¡å‹
            job: ä»»åŠ¡çŠ¶æ€

        Returns:
            Dict: æœªå¯¹é½çš„è½¬å½•ç»“æœ
            {
                "segment_index": 0,
                "language": "zh",
                "segments": [{"id": 0, "start": 10.5, "end": 15.2, "text": "..."}]
            }
        """
        # ä½¿ç”¨ whisper_service æä¾›çš„ load_audio å‡½æ•°
        audio = whisper_load_audio(seg['file'])

        try:
            # ä½¿ç”¨ Faster-Whisper è½¬å½•
            segments_gen, info = model.transcribe(
                audio,
                language=job.language,
                beam_size=5,
                vad_filter=True
            )

            # è½¬æ¢ç”Ÿæˆå™¨ä¸ºåˆ—è¡¨
            segments_list = list(segments_gen)

            if not segments_list:
                return None

            # æ£€æµ‹è¯­è¨€ï¼ˆé¦–æ¬¡ï¼‰
            if not job.language and info.language:
                job.language = info.language
                self.logger.info(f"æ£€æµ‹åˆ°è¯­è¨€: {job.language}")

            # æ—¶é—´åç§»æ ¡æ­£ï¼ˆé’ˆå¯¹ç²—ç•¥æ—¶é—´æˆ³ï¼‰
            start_offset = seg['start_ms'] / 1000.0
            adjusted_segments = []

            for idx, s in enumerate(segments_list):
                adjusted_segments.append({
                    'id': idx,
                    'start': s.start + start_offset,
                    'end': s.end + start_offset,
                    'text': s.text.strip()
                })

            return {
                'segment_index': seg.get('index', 0),
                'language': info.language or job.language,
                'segments': adjusted_segments
            }

        finally:
            del audio
            gc.collect()

    def _transcribe_segment_in_memory(
        self,
        audio_array: np.ndarray,
        seg_meta: Dict,
        model,
        job: JobState,
        is_vocals: bool = False
    ) -> Optional[Dict]:
        """
        ä»å†…å­˜åˆ‡ç‰‡è½¬å½•ï¼ˆZero-copyï¼Œé«˜æ€§èƒ½ï¼‰

        å†…å­˜æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œç›´æ¥ä»å®Œæ•´éŸ³é¢‘æ•°ç»„ä¸­åˆ‡ç‰‡ï¼Œæ— éœ€ç£ç›˜IOã€‚

        Args:
            audio_array: å®Œæ•´éŸ³é¢‘æ•°ç»„
            seg_meta: åˆ†æ®µå…ƒæ•°æ® {"index": 0, "start": 0.0, "end": 30.5, "mode": "memory"}
            model: Faster-Whisper æ¨¡å‹
            job: ä»»åŠ¡çŠ¶æ€
            is_vocals: æ˜¯å¦æ˜¯Demucsåˆ†ç¦»åçš„äººå£°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            Dict: æœªå¯¹é½çš„è½¬å½•ç»“æœ
        """
        sr = 16000
        start_sample = int(seg_meta['start'] * sr)
        end_sample = int(seg_meta['end'] * sr)

        # Zero-copyåˆ‡ç‰‡ï¼ˆnumpy viewï¼Œä¸å¤åˆ¶æ•°æ®ï¼‰
        audio_slice = audio_array[start_sample:end_sample]

        try:
            # ä½¿ç”¨ Faster-Whisper è½¬å½•
            segments_gen, info = model.transcribe(
                audio_slice,
                language=job.language,
                beam_size=5,
                vad_filter=False  # å·²ç»æ˜¯åˆ‡ç‰‡ï¼Œä¸éœ€è¦å†åš VAD
            )

            # è½¬æ¢ç”Ÿæˆå™¨ä¸ºåˆ—è¡¨
            segments_list = list(segments_gen)

            if not segments_list:
                return None

            # æ£€æµ‹è¯­è¨€ï¼ˆé¦–æ¬¡ï¼‰
            if not job.language and info.language:
                job.language = info.language
                self.logger.info(f"detected language: {job.language}")

            # æ—¶é—´åç§»æ ¡æ­£
            start_offset = seg_meta['start']
            adjusted_segments = []

            for idx, s in enumerate(segments_list):
                adjusted_segments.append({
                    'id': idx,
                    'start': s.start + start_offset,
                    'end': s.end + start_offset,
                    'text': s.text.strip()
                })

            return {
                'segment_index': seg_meta['index'],
                'language': info.language or job.language,
                'segments': adjusted_segments
            }

        finally:
            # æ³¨æ„ï¼šaudio_sliceæ˜¯viewï¼Œä¸éœ€è¦å•ç‹¬é‡Šæ”¾
            gc.collect()

    def _transcribe_segment_from_disk(
        self,
        seg: Dict,
        model,
        job: JobState
    ) -> Optional[Dict]:
        """
        ä»æ–‡ä»¶åŠ è½½è½¬å½•ï¼ˆç¡¬ç›˜æ¨¡å¼ï¼‰

        ç¡¬ç›˜æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œä»segmentæ–‡ä»¶åŠ è½½éŸ³é¢‘è¿›è¡Œè½¬å½•ã€‚

        Args:
            seg: åˆ†æ®µä¿¡æ¯ {"index": 0, "file": "segment_0.wav", "start": 0.0, "end": 30.0, "mode": "disk"}
            model: Faster-Whisper æ¨¡å‹
            job: ä»»åŠ¡çŠ¶æ€

        Returns:
            Dict: æœªå¯¹é½çš„è½¬å½•ç»“æœ
        """
        # ä½¿ç”¨ whisper_service æä¾›çš„ load_audio å‡½æ•°
        audio = whisper_load_audio(seg['file'])

        try:
            # ä½¿ç”¨ Faster-Whisper è½¬å½•
            segments_gen, info = model.transcribe(
                audio,
                language=job.language,
                beam_size=5,
                vad_filter=True
            )

            # è½¬æ¢ç”Ÿæˆå™¨ä¸ºåˆ—è¡¨
            segments_list = list(segments_gen)

            if not segments_list:
                return None

            # æ£€æµ‹è¯­è¨€ï¼ˆé¦–æ¬¡ï¼‰
            if not job.language and info.language:
                job.language = info.language
                self.logger.info(f"detected language: {job.language}")

            # æ—¶é—´åç§»æ ¡æ­£ï¼ˆä½¿ç”¨startå­—æ®µï¼Œç§’ä¸ºå•ä½ï¼‰
            start_offset = seg.get('start', seg.get('start_ms', 0) / 1000.0)
            adjusted_segments = []

            for idx, s in enumerate(segments_list):
                adjusted_segments.append({
                    'id': idx,
                    'start': s.start + start_offset,
                    'end': s.end + start_offset,
                    'text': s.text.strip()
                })

            return {
                'segment_index': seg['index'],
                'language': info.language or job.language,
                'segments': adjusted_segments
            }

        finally:
            del audio
            gc.collect()

    def _transcribe_segment(
        self,
        seg_meta: Dict,
        model,
        job: JobState,
        audio_array: Optional[np.ndarray] = None
    ) -> Optional[Dict]:
        """
        ç»Ÿä¸€è½¬å½•å…¥å£ï¼ˆæ ¹æ®æ¨¡å¼è‡ªåŠ¨é€‰æ‹©ï¼‰

        Args:
            seg_meta: åˆ†æ®µå…ƒæ•°æ®
            model: Whisperæ¨¡å‹
            job: ä»»åŠ¡çŠ¶æ€
            audio_array: éŸ³é¢‘æ•°ç»„ï¼ˆå†…å­˜æ¨¡å¼æ—¶å¿…é¡»æä¾›ï¼‰

        Returns:
            Dict: æœªå¯¹é½çš„è½¬å½•ç»“æœ
        """
        mode = seg_meta.get('mode', 'disk')

        if mode == 'memory':
            if audio_array is None:
                raise ValueError("memory mode requires audio_array parameter")
            return self._transcribe_segment_in_memory(audio_array, seg_meta, model, job)
        else:
            return self._transcribe_segment_from_disk(seg_meta, model, job)

    def _check_memory_during_transcription(self, job: JobState) -> bool:
        """
        è½¬å½•è¿‡ç¨‹ä¸­æ£€æŸ¥å†…å­˜çŠ¶æ€

        å¦‚æœå†…å­˜ä¸¥é‡ä¸è¶³ï¼Œæš‚åœä»»åŠ¡å¹¶è­¦å‘Šç”¨æˆ·ã€‚

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡

        Returns:
            bool: True=ç»§ç»­å¤„ç†ï¼ŒFalse=éœ€è¦æš‚åœ
        """
        mem_info = psutil.virtual_memory()
        available_mb = mem_info.available / (1024 * 1024)
        percent_used = mem_info.percent

        # å±é™©é˜ˆå€¼ï¼šå¯ç”¨å†…å­˜<500MB æˆ– ä½¿ç”¨ç‡>95%
        if available_mb < 500 or percent_used > 95:
            self.logger.error(f"memory critically low! available: {available_mb:.0f}MB, usage: {percent_used}%")
            job.status = 'paused'
            job.message = f"memory insufficient (available {available_mb:.0f}MB), please close other programs"
            job.paused = True

            # æ¨é€è­¦å‘ŠSSE
            self._push_sse_signal(job, "memory_warning",
                f"memory critically low (available {available_mb:.0f}MB), task paused")

            return False

        # è­¦å‘Šé˜ˆå€¼ï¼šå¯ç”¨å†…å­˜<1GB æˆ– ä½¿ç”¨ç‡>90%
        if available_mb < 1024 or percent_used > 90:
            self.logger.warning(f"memory tight: available {available_mb:.0f}MB, usage {percent_used}%")
            # ä¸æš‚åœï¼Œä½†è®°å½•è­¦å‘Š

        return True

    def _align_all_results(
        self,
        unaligned_results: List[Dict],
        job: JobState,
        audio_path: str
    ) -> List[Dict]:
        """
        åˆå¹¶è½¬å½•ç»“æœçš„åˆ†æ®µ

        æ­¤æ–¹æ³•åˆå¹¶æ‰€æœ‰ segments å¹¶è¿”å›ï¼Œä¸æ‰§è¡Œå¯¹é½æ“ä½œã€‚
        """
        self.logger.info(f"åˆå¹¶ {len(unaligned_results)} ä¸ªåˆ†æ®µçš„è½¬å½•ç»“æœï¼ˆè·³è¿‡å¼ºåˆ¶å¯¹é½ï¼‰")

        # åˆå¹¶æ‰€æœ‰segments
        all_segments = []
        for result in unaligned_results:
            all_segments.extend(result['segments'])

        if not all_segments:
            self.logger.warning("æ²¡æœ‰å¯å¤„ç†çš„å†…å®¹")
            return []

        # ç›´æ¥è¿”å›åˆå¹¶åçš„ç»“æœï¼ˆFaster-Whisper å·²æä¾›æ—¶é—´æˆ³ï¼‰
        return [{
            'segments': all_segments,
            'word_segments': []  # æ–°æ¶æ„ä½¿ç”¨ä¼ªå¯¹é½ç”Ÿæˆå­—çº§æ—¶é—´æˆ³
        }]

    def _push_sse_align_progress(
        self,
        job: JobState,
        current_batch: int,
        total_batches: int,
        aligned_count: int,
        total_count: int
    ):
        """
        æ¨é€å¯¹é½è¿›åº¦SSEäº‹ä»¶ï¼ˆå‰ç«¯è¿›åº¦æ¡å®æ—¶æ›´æ–°ï¼‰

        äº‹ä»¶ç±»å‹: "align_progress"

        Args:
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            current_batch: å½“å‰æ‰¹æ¬¡å·ï¼ˆ1-basedï¼‰
            total_batches: æ€»æ‰¹æ¬¡æ•°
            aligned_count: å·²å¯¹é½çš„segmentæ•°é‡
            total_count: æ€»segmentæ•°é‡
        """
        try:
            from services.sse_service import get_sse_manager
            sse_manager = get_sse_manager()

            channel_id = f"job:{job.job_id}"

            # è®¡ç®—ç™¾åˆ†æ¯”
            batch_progress = (current_batch / total_batches) * 100 if total_batches > 0 else 0
            segment_progress = (aligned_count / total_count) * 100 if total_count > 0 else 0

            sse_manager.broadcast_sync(
                channel_id,
                "align_progress",  # ä¸“ç”¨äº‹ä»¶ç±»å‹
                {
                    "job_id": job.job_id,
                    "phase": "align",
                    "batch": {
                        "current": current_batch,
                        "total": total_batches,
                        "progress": round(batch_progress, 2)
                    },
                    "segments": {
                        "aligned": aligned_count,
                        "total": total_count,
                        "progress": round(segment_progress, 2)
                    },
                    "message": f"aligning batch {current_batch}/{total_batches} ({aligned_count}/{total_count} segments)"
                }
            )

        except Exception as e:
            self.logger.debug(f"SSE align progress push failed (non-fatal): {e}")

    def _align_all_results_batched(
        self,
        unaligned_results: List[Dict],
        job: JobState,
        audio_source,  # Union[np.ndarray, str]
        processing_mode: ProcessingMode
    ) -> List[Dict]:
        """
        æ‰¹é‡å¤„ç†è½¬å½•ç»“æœçš„åˆ†æ®µ

        æ­¤æ–¹æ³•åˆå¹¶æ‰€æœ‰ segmentsï¼Œåº”ç”¨æ—¶é—´æ ¡éªŒå’Œå¾®è°ƒï¼Œç„¶åè¿”å›ã€‚
        """
        self.logger.info(f"å¤„ç† {len(unaligned_results)} ä¸ªåˆ†æ®µçš„è½¬å½•ç»“æœï¼ˆè·³è¿‡å¼ºåˆ¶å¯¹é½ï¼‰")

        # 1. åˆå¹¶æ‰€æœ‰segments
        all_segments = []
        for result in unaligned_results:
            all_segments.extend(result['segments'])

        if not all_segments:
            self.logger.warning("æ²¡æœ‰å¯å¤„ç†çš„å†…å®¹")
            return []

        # 2. å¯¹ç»“æœè¿›è¡Œè¾¹ç•Œæ ¡éªŒï¼Œè¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_segments = []
        for seg in all_segments:
            start = seg.get('start', 0)
            end = seg.get('end', 0)
            text = seg.get('text', '').strip()

            # æ ¡éªŒ1ï¼šæ—¶é—´æˆ³å¿…é¡»æœ‰æ•ˆ
            if start is None or end is None or start < 0 or end <= start:
                self.logger.warning(f"è¿‡æ»¤æ— æ•ˆæ—¶é—´æˆ³: start={start}, end={end}, text={text[:20] if text else ''}...")
                continue

            # æ ¡éªŒ2ï¼šå­—å¹•æ—¶é•¿ä¸èƒ½è¿‡é•¿ï¼ˆè¶…è¿‡30ç§’å¯èƒ½æ˜¯å¼‚å¸¸ï¼‰
            duration = end - start
            if duration > 30:
                self.logger.warning(f"è¿‡æ»¤è¿‡é•¿å­—å¹•({duration:.1f}s): {text[:30] if text else ''}...")
                continue

            # æ ¡éªŒ3ï¼šå­—å¹•æ—¶é•¿ä¸èƒ½è¿‡çŸ­ï¼ˆå°äº0.1ç§’å¯èƒ½æ˜¯å™ªéŸ³ï¼‰
            if duration < 0.1 and len(text) > 0:
                self.logger.warning(f"è¿‡æ»¤è¿‡çŸ­å­—å¹•({duration:.2f}s): {text}")
                continue

            valid_segments.append(seg)

        # 3. å­—å¹•æ—¶é—´å¾®è°ƒ - ä¿®æ­£"æŠ¢å…ˆå‡ºç°"é—®é¢˜
        valid_segments = self._adjust_subtitle_timing(valid_segments)

        self.logger.info(f"å¤„ç†å®Œæˆ: {len(valid_segments)}/{len(all_segments)} ä¸ªæœ‰æ•ˆå­—å¹•æ®µ")

        return [{
            'segments': valid_segments,
            'word_segments': []  # æ–°æ¶æ„ä½¿ç”¨ä¼ªå¯¹é½ç”Ÿæˆå­—çº§æ—¶é—´æˆ³
        }]

    def _adjust_subtitle_timing(
        self,
        segments: List[Dict],
        start_delay_ms: int = 25,
        end_padding_ms: int = 25
    ) -> List[Dict]:
        """
        å­—å¹•æ—¶é—´å¾®è°ƒ - ä¿®æ­£å¯¹é½åå·®

        é—®é¢˜èƒŒæ™¯ï¼š
        è½¬å½•åçš„å­—å¹•å¯èƒ½å‡ºç°æ—¶é—´åå·®ï¼Œéœ€è¦é€‚å½“è°ƒæ•´ã€‚

        è§£å†³æ–¹æ¡ˆï¼š
        1. å°†å­—å¹•å¼€å§‹æ—¶é—´å»¶å start_delay_msï¼ˆé»˜è®¤25msï¼‰
        2. å°†å­—å¹•ç»“æŸæ—¶é—´å»¶å end_padding_msï¼ˆé»˜è®¤25msï¼Œç»™ä¸€ç‚¹ä½™é‡ï¼‰
        3. ç¡®ä¿ç›¸é‚»å­—å¹•ä¸é‡å 

        Args:
            segments: å¯¹é½åçš„å­—å¹•åˆ—è¡¨
            start_delay_ms: å¼€å§‹æ—¶é—´å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰ï¼Œæ¨è20-50ms
            end_padding_ms: ç»“æŸæ—¶é—´å»¶é•¿ï¼ˆæ¯«ç§’ï¼‰ï¼Œæ¨è20-50ms

        Returns:
            è°ƒæ•´åçš„å­—å¹•åˆ—è¡¨
        """
        if not segments:
            return segments

        start_delay = start_delay_ms / 1000.0
        end_padding = end_padding_ms / 1000.0

        adjusted = []
        for i, seg in enumerate(segments):
            new_seg = seg.copy()
            old_start = seg.get('start', 0)
            old_end = seg.get('end', 0)

            # å»¶è¿Ÿå¼€å§‹æ—¶é—´
            new_start = old_start + start_delay

            # å»¶é•¿ç»“æŸæ—¶é—´
            new_end = old_end + end_padding

            # ç¡®ä¿å¼€å§‹æ—¶é—´ä¸è¶…è¿‡ç»“æŸæ—¶é—´
            if new_start >= new_end:
                new_start = old_start  # å›é€€åˆ°åŸå§‹å¼€å§‹æ—¶é—´

            # ç¡®ä¿ä¸ä¸ä¸‹ä¸€æ¡å­—å¹•é‡å 
            if i < len(segments) - 1:
                next_start = segments[i + 1].get('start', float('inf'))
                # ä¸‹ä¸€æ¡ä¹Ÿä¼šè¢«å»¶è¿Ÿï¼Œæ‰€ä»¥æ¯”è¾ƒæ—¶è¦è€ƒè™‘
                next_adjusted_start = next_start + start_delay
                if new_end > next_adjusted_start:
                    new_end = next_adjusted_start - 0.05  # ç•™50msé—´éš”

            # ç¡®ä¿ç»“æŸæ—¶é—´ä»ç„¶æœ‰æ•ˆ
            if new_end <= new_start:
                new_end = old_end  # å›é€€åˆ°åŸå§‹ç»“æŸæ—¶é—´

            new_seg['start'] = round(new_start, 3)
            new_seg['end'] = round(new_end, 3)
            adjusted.append(new_seg)

        self.logger.info(f"å­—å¹•æ—¶é—´å¾®è°ƒ: å»¶è¿Ÿå¼€å§‹25ms, å»¶é•¿ç»“æŸ25ms")
        return adjusted

    def _format_ts(self, sec: float) -> str:
        """
        æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸ºSRTæ ¼å¼

        Args:
            sec: ç§’æ•°

        Returns:
            str: SRTæ—¶é—´æˆ³ (HH:MM:SS,mmm)
        """
        if sec < 0:
            sec = 0

        ms = int(round(sec * 1000))
        h = ms // 3600000
        ms %= 3600000
        m = ms // 60000
        ms %= 60000
        s = ms // 1000
        ms %= 1000

        return f"{h:02}:{m:02}:{s:02},{ms:03}"

    def _generate_srt(self, results: List[Dict], path: str, word_level: bool):
        """
        ç”ŸæˆSRTå­—å¹•æ–‡ä»¶

        Args:
            results: è½¬å½•ç»“æœåˆ—è¡¨
            path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            word_level: æ˜¯å¦ä½¿ç”¨è¯çº§æ—¶é—´æˆ³
        """
        lines = []
        n = 1  # å­—å¹•åºå·

        for r in results:
            if not r:
                continue

            entries = []

            # è¯çº§æ—¶é—´æˆ³æ¨¡å¼
            if word_level and r.get('word_segments'):
                for w in r['word_segments']:
                    if w.get('start') is not None and w.get('end') is not None:
                        txt = (w.get('word') or '').strip()
                        if txt:
                            entries.append({
                                'start': w['start'],
                                'end': w['end'],
                                'text': txt
                            })

            # å¥å­çº§æ—¶é—´æˆ³æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            elif r.get('segments'):
                for s in r['segments']:
                    if s.get('start') is not None and s.get('end') is not None:
                        txt = (s.get('text') or '').strip()
                        if txt:
                            entries.append({
                                'start': s['start'],
                                'end': s['end'],
                                'text': txt
                            })

            # å†™å…¥SRTæ ¼å¼
            for e in entries:
                if e['end'] <= e['start']:
                    continue  # è·³è¿‡æ— æ•ˆæ—¶é—´æˆ³

                lines.append(str(n))  # åºå·
                lines.append(
                    f"{self._format_ts(e['start'])} --> {self._format_ts(e['end'])}"
                )  # æ—¶é—´æˆ³
                lines.append(e['text'])  # å­—å¹•æ–‡æœ¬
                lines.append("")  # ç©ºè¡Œ
                n += 1

        # å†™å…¥æ–‡ä»¶
        with open(path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        self.logger.info(f"SRTæ–‡ä»¶å·²ç”Ÿæˆ: {path}, å…±{n-1}æ¡å­—å¹•")

    def clear_model_cache(self):
        """
        æ¸…ç©ºæ¨¡å‹ç¼“å­˜ï¼ˆä¾›é˜Ÿåˆ—æœåŠ¡è°ƒç”¨ï¼‰

        æ³¨æ„: æ–°æ¶æ„å·²ç§»é™¤å¯¹é½æ¨¡å‹ï¼Œä»…æ¸…ç† Whisper æ¨¡å‹
        """
        global _model_cache

        with _model_lock:
            for key in list(_model_cache.keys()):
                try:
                    del _model_cache[key]
                except:
                    pass
            _model_cache.clear()
            self.logger.info("Whisperæ¨¡å‹ç¼“å­˜å·²æ¸…ç©º")

    # ==========================================
    # SenseVoice é›†æˆæ–¹æ³•ï¼ˆPhase 3ï¼‰
    # ==========================================

    def _extract_audio_with_array(
        self,
        input_file: str,
        job: 'JobState',
        target_sr: int = 16000
    ) -> Tuple[np.ndarray, int]:
        """
        æå–éŸ³é¢‘å¹¶è¿”å› numpy æ•°ç»„

        Args:
            input_file: è¾“å…¥è§†é¢‘/éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            target_sr: ç›®æ ‡é‡‡æ ·ç‡ï¼ˆé»˜è®¤16000Hzï¼ŒSenseVoiceæ ‡å‡†ï¼‰

        Returns:
            Tuple[np.ndarray, int]: (éŸ³é¢‘æ•°ç»„, é‡‡æ ·ç‡)

        Raises:
            RuntimeError: æå–å¤±è´¥æ—¶æŠ›å‡º
        """
        import tempfile
        import librosa

        self.logger.info(f"å¼€å§‹æå–éŸ³é¢‘åˆ°å†…å­˜: {input_file}")

        # åˆ›å»ºä¸´æ—¶WAVæ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            tmp_path = tmp_file.name

        try:
            # ä½¿ç”¨FFmpegæå–éŸ³é¢‘
            cmd = [
                'ffmpeg', '-y', '-i', input_file,
                '-vn',  # æ— è§†é¢‘
                '-acodec', 'pcm_s16le',  # 16ä½PCM
                '-ar', str(target_sr),  # é‡‡æ ·ç‡
                '-ac', '1',  # å•å£°é“
                tmp_path
            ]

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300
            )

            if result.returncode != 0:
                raise RuntimeError(f"FFmpegæå–å¤±è´¥: {result.stderr}")

            # åŠ è½½éŸ³é¢‘åˆ°å†…å­˜
            audio_array, sr = librosa.load(tmp_path, sr=target_sr, mono=True)

            self.logger.info(f"éŸ³é¢‘æå–å®Œæˆ: {len(audio_array)} samples, {sr}Hz")
            return audio_array, sr

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(tmp_path):
                os.remove(tmp_path)

    def _sensevoice_transcribe(
        self,
        audio_array: np.ndarray,
        job: 'JobState',
        sample_rate: int = 16000
    ) -> 'SenseVoiceResult':
        """
        è°ƒç”¨ SenseVoice æœåŠ¡è¿›è¡Œè½¬å½•ï¼ˆè¿”å›å¸¦çœŸå®å­—çº§æ—¶é—´æˆ³çš„ç»“æœï¼‰

        Args:
            audio_array: éŸ³é¢‘æ•°ç»„
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            sample_rate: é‡‡æ ·ç‡

        Returns:
            SenseVoiceResult: è½¬å½•ç»“æœï¼ŒåŒ…å«:
                - text: åŸå§‹æ–‡æœ¬ï¼ˆå¸¦æ ‡ç­¾ï¼‰
                - text_clean: æ¸…æ´—åæ–‡æœ¬
                - words: å­—çº§æ—¶é—´æˆ³åˆ—è¡¨ï¼ˆçœŸå®æ—¶é—´æˆ³ï¼Œéä¼ªå¯¹é½ï¼‰
                - confidence: å¹³å‡ç½®ä¿¡åº¦
                - language: æ£€æµ‹åˆ°çš„è¯­è¨€
                - emotion: æƒ…æ„Ÿæ ‡ç­¾
                - event: äº‹ä»¶æ ‡ç­¾
        """
        from .sensevoice_onnx_service import get_sensevoice_service
        from ..models.sensevoice_models import SenseVoiceResult, WordTimestamp

        self.logger.info("è°ƒç”¨ SenseVoice è½¬å½•æœåŠ¡")

        try:
            service = get_sensevoice_service()

            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if not service.is_loaded:
                self.logger.info("åŠ è½½ SenseVoice æ¨¡å‹...")
                service.load_model()

            # è°ƒç”¨è½¬å½•ï¼ˆè¿”å›å­—å…¸ï¼‰
            result_dict = service.transcribe_audio_array(
                audio_array=audio_array,
                sample_rate=sample_rate,
                language=job.settings.sensevoice.preset_id if hasattr(job.settings, 'sensevoice') else "auto"
            )

            # è½¬æ¢ä¸º SenseVoiceResult å¯¹è±¡
            words = [
                WordTimestamp(**w) if isinstance(w, dict) else w
                for w in result_dict.get('words', [])
            ]

            result = SenseVoiceResult(
                text=result_dict.get('text', ''),
                text_clean=result_dict.get('text_clean', ''),
                confidence=result_dict.get('confidence', 1.0),
                words=words,
                start=0.0,  # Chunk çº§åˆ«çš„èµ·å§‹æ—¶é—´ï¼Œç”±è°ƒç”¨è€…è®¾ç½®
                end=len(audio_array) / sample_rate,
                language=result_dict.get('language'),
                emotion=result_dict.get('emotion'),
                event=result_dict.get('event'),
                raw_result=result_dict
            )

            self.logger.info(f"SenseVoice è½¬å½•å®Œæˆ: {len(result.text_clean)} å­—ç¬¦, {len(words)} ä¸ªå­—")
            return result

        except Exception as e:
            self.logger.error(f"SenseVoice è½¬å½•å¤±è´¥: {e}")
            raise

    def _split_sentences(
        self,
        sv_result: 'SenseVoiceResult',
        chunk_start_time: float = 0.0
    ) -> List['SentenceSegment']:
        """
        å°† SenseVoice ç»“æœåˆ‡åˆ†ä¸ºå¥å­ï¼ˆåŸºäºçœŸå®å­—çº§æ—¶é—´æˆ³ï¼‰

        Args:
            sv_result: SenseVoice è½¬å½•ç»“æœï¼ˆåŒ…å«çœŸå®å­—çº§æ—¶é—´æˆ³ï¼‰
            chunk_start_time: Chunk åœ¨å®Œæ•´éŸ³é¢‘ä¸­çš„èµ·å§‹æ—¶é—´ï¼ˆç”¨äºæ—¶é—´åç§»ï¼‰

        Returns:
            List[SentenceSegment]: å¥å­åˆ—è¡¨
        """
        from ..models.sensevoice_models import SentenceSegment, TextSource
        from .sentence_splitter import SentenceSplitter, SplitConfig

        self.logger.info(f"å¼€å§‹å¥å­åˆ‡åˆ†: {len(sv_result.words)} ä¸ªå­—")

        if not sv_result.words:
            return []

        # ä½¿ç”¨åˆ†å¥å™¨è¿›è¡Œåˆ‡åˆ†ï¼ˆåŸºäºçœŸå®æ—¶é—´æˆ³ï¼‰
        splitter = SentenceSplitter(SplitConfig())
        sentences = splitter.split(sv_result.words, sv_result.text_clean)

        # è°ƒæ•´æ—¶é—´åç§»ï¼ˆå°† Chunk å†…çš„ç›¸å¯¹æ—¶é—´è½¬æ¢ä¸ºç»å¯¹æ—¶é—´ï¼‰
        for sentence in sentences:
            sentence.start += chunk_start_time
            sentence.end += chunk_start_time
            sentence.source = TextSource.SENSEVOICE
            sentence.confidence = sv_result.confidence

            # è°ƒæ•´å­—çº§æ—¶é—´æˆ³çš„åç§»
            for word in sentence.words:
                word.start += chunk_start_time
                word.end += chunk_start_time

        self.logger.info(f"å¥å­åˆ‡åˆ†å®Œæˆ: {len(sentences)} å¥")
        return sentences

    def _split_text_by_punctuation(self, text: str) -> List[str]:
        """
        åŸºäºæ ‡ç‚¹ç¬¦å·åˆ‡åˆ†æ–‡æœ¬

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            List[str]: å¥å­åˆ—è¡¨
        """
        import re

        # å¥æœ«æ ‡ç‚¹
        sentence_end_pattern = r'([ã€‚ï¼Ÿï¼.?!])'

        # ä½¿ç”¨æ­£åˆ™åˆ‡åˆ†ï¼Œä¿ç•™æ ‡ç‚¹
        parts = re.split(sentence_end_pattern, text)

        # åˆå¹¶æ ‡ç‚¹åˆ°å‰ä¸€ä¸ªå¥å­
        sentences = []
        i = 0
        while i < len(parts):
            if i + 1 < len(parts) and re.match(sentence_end_pattern, parts[i + 1]):
                sentences.append(parts[i] + parts[i + 1])
                i += 2
            else:
                if parts[i].strip():
                    sentences.append(parts[i])
                i += 1

        return [s.strip() for s in sentences if s.strip()]

    def _generate_pseudo_word_timestamps(
        self,
        text: str,
        start: float,
        end: float,
        confidence: float = 1.0
    ) -> List['WordTimestamp']:
        """
        ç”Ÿæˆä¼ªå­—çº§æ—¶é—´æˆ³ï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰

        Args:
            text: å¥å­æ–‡æœ¬
            start: å¥å­å¼€å§‹æ—¶é—´
            end: å¥å­ç»“æŸæ—¶é—´
            confidence: ç½®ä¿¡åº¦

        Returns:
            List[WordTimestamp]: å­—çº§æ—¶é—´æˆ³åˆ—è¡¨
        """
        from ..models.sensevoice_models import WordTimestamp

        if not text:
            return []

        duration = end - start
        char_duration = duration / len(text)

        words = []
        for i, char in enumerate(text):
            word_start = start + i * char_duration
            word_end = start + (i + 1) * char_duration

            words.append(WordTimestamp(
                word=char,
                start=word_start,
                end=word_end,
                confidence=confidence,
                is_pseudo=True  # æ ‡è®°ä¸ºä¼ªå¯¹é½
            ))

        return words

    def _generate_subtitle_from_sentences(
        self,
        sentences: List['SentenceSegment'],
        output_path: str,
        include_translation: bool = False
    ) -> str:
        """
        ä»å¥å­åˆ—è¡¨ç”Ÿæˆ SRT å­—å¹•æ–‡ä»¶

        Args:
            sentences: å¥å­åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            include_translation: æ˜¯å¦åŒ…å«ç¿»è¯‘ï¼ˆåŒè¯­å­—å¹•ï¼‰

        Returns:
            str: ç”Ÿæˆçš„SRTæ–‡ä»¶è·¯å¾„
        """
        self.logger.info(f"ç”ŸæˆSRTå­—å¹•: {len(sentences)} å¥ -> {output_path}")

        lines = []
        for idx, sentence in enumerate(sentences, 1):
            # åºå·
            lines.append(str(idx))

            # æ—¶é—´æˆ³
            start_ts = self._format_ts(sentence.start)
            end_ts = self._format_ts(sentence.end)
            lines.append(f"{start_ts} --> {end_ts}")

            # å­—å¹•æ–‡æœ¬
            if include_translation and sentence.translation:
                # åŒè¯­å­—å¹•ï¼šåŸæ–‡ + ç¿»è¯‘
                lines.append(sentence.text)
                lines.append(sentence.translation)
            else:
                lines.append(sentence.text)

            # ç©ºè¡Œåˆ†éš”
            lines.append("")

        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        self.logger.info(f"SRTå­—å¹•ç”Ÿæˆå®Œæˆ: {output_path}")
        return output_path

    def _memory_vad_split(
        self,
        audio_array: np.ndarray,
        sr: int = 16000,
        job: 'JobState' = None
    ) -> List[Dict]:
        """
        VAD å†…å­˜åˆ‡åˆ†ï¼ˆåŸºäº Silero VADï¼‰

        Args:
            audio_array: éŸ³é¢‘æ•°ç»„
            sr: é‡‡æ ·ç‡
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡

        Returns:
            List[Dict]: VAD åˆ‡åˆ†ç»“æœï¼Œæ ¼å¼ï¼š
                [{"index": 0, "start": 0.0, "end": 15.5, "mode": "memory"}, ...]
        """
        self.logger.info("å¼€å§‹ VAD å†…å­˜åˆ‡åˆ†...")

        # è°ƒç”¨ç°æœ‰çš„ _vad_silero æ–¹æ³•
        vad_segments = self._vad_silero(audio_array, sr)

        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        result = []
        for i, seg in enumerate(vad_segments):
            result.append({
                "index": i,
                "start": seg["start"],
                "end": seg["end"],
                "mode": "memory"
            })

        self.logger.info(f"VAD åˆ‡åˆ†å®Œæˆ: {len(result)} ä¸ªç‰‡æ®µ")
        return result

    def _init_chunk_states(
        self,
        vad_segments: List[dict],
        audio_array: np.ndarray,
        sr: int = 16000
    ) -> List['ChunkProcessState']:
        """
        åˆå§‹åŒ–æ‰€æœ‰ Chunk çš„å¤„ç†çŠ¶æ€

        å…³é”®ï¼šä¿å­˜åŸå§‹éŸ³é¢‘å¼•ç”¨ï¼Œç”¨äºç†”æ–­å›æº¯

        Args:
            vad_segments: VAD åˆ‡åˆ†ç»“æœ
            audio_array: å®Œæ•´éŸ³é¢‘æ•°ç»„
            sr: é‡‡æ ·ç‡

        Returns:
            List[ChunkProcessState]: Chunk çŠ¶æ€åˆ—è¡¨
        """
        from ..models.circuit_breaker_models import ChunkProcessState, SeparationLevel

        self.logger.info(f"åˆå§‹åŒ– {len(vad_segments)} ä¸ª Chunk çŠ¶æ€...")

        states = []
        for seg in vad_segments:
            start_sample = int(seg['start'] * sr)
            end_sample = int(seg['end'] * sr)
            chunk_audio = audio_array[start_sample:end_sample]

            state = ChunkProcessState(
                chunk_index=seg['index'],
                start_time=seg['start'],
                end_time=seg['end'],
                original_audio=chunk_audio.copy(),  # å…³é”®ï¼šä¿å­˜åŸå§‹éŸ³é¢‘å‰¯æœ¬
                current_audio=chunk_audio,           # å½“å‰ä½¿ç”¨çš„éŸ³é¢‘ï¼ˆå¯èƒ½è¢«åˆ†ç¦»åæ›¿æ¢ï¼‰
                sample_rate=sr,
                separation_level=SeparationLevel.NONE
            )
            states.append(state)

        self.logger.info(f"Chunk çŠ¶æ€åˆå§‹åŒ–å®Œæˆ")
        return states

    async def _transcribe_chunk_with_fusing(
        self,
        chunk_state: 'ChunkProcessState',
        job: 'JobState',
        subtitle_manager: 'StreamingSubtitleManager',
        demucs_service
    ) -> List['SentenceSegment']:
        """
        å•ä¸ª Chunk çš„è½¬å½•æµç¨‹ï¼ˆå«ç†”æ–­å›æº¯ï¼‰

        æµç¨‹ï¼š
        1. ä½¿ç”¨ current_audio è¿›è¡Œ SenseVoice è½¬å½•
        2. è¯„ä¼°ç½®ä¿¡åº¦å’Œäº‹ä»¶æ ‡ç­¾
        3. ç†”æ–­å†³ç­–
        4. å¦‚éœ€ç†”æ–­ï¼šå›æº¯åˆ° original_audioï¼Œå‡çº§åˆ†ç¦»ï¼Œé‡æ–°è½¬å½•
        5. æ­¢æŸç‚¹ï¼šmax_retry=1

        Args:
            chunk_state: Chunk å¤„ç†çŠ¶æ€
            job: ä»»åŠ¡çŠ¶æ€å¯¹è±¡
            subtitle_manager: æµå¼å­—å¹•ç®¡ç†å™¨
            demucs_service: Demucs æœåŠ¡å®ä¾‹

        Returns:
            List[SentenceSegment]: å¥å­åˆ—è¡¨
        """
        from .fuse_breaker import get_fuse_breaker, execute_fuse_upgrade, FuseAction

        fuse_breaker = get_fuse_breaker()

        while True:
            # 1. SenseVoice è½¬å½•
            sv_result = self._sensevoice_transcribe(chunk_state.current_audio, job, chunk_state.sample_rate)

            # 2. åˆ†å¥
            sentences = self._split_sentences(sv_result, chunk_state.start_time)

            # 3. è®¡ç®—ç½®ä¿¡åº¦å’Œäº‹ä»¶æ ‡ç­¾
            avg_confidence = sum(s.confidence for s in sentences) / len(sentences) if sentences else 0.0
            event_tag = sv_result.event  # SenseVoice æ£€æµ‹åˆ°çš„äº‹ä»¶ï¼ˆBGM/Noiseç­‰ï¼‰

            # 4. ç†”æ–­å†³ç­–
            decision = fuse_breaker.should_fuse(
                chunk_state=chunk_state,
                confidence=avg_confidence,
                event_tag=event_tag
            )

            self.logger.debug(
                f"Chunk {chunk_state.chunk_index} ç†”æ–­å†³ç­–: {decision.action.value}, "
                f"ç½®ä¿¡åº¦={avg_confidence:.2f}, äº‹ä»¶={event_tag}"
            )

            # 5. å¤„ç†å†³ç­–
            if decision.action == FuseAction.ACCEPT:
                # æ¥å—ç»“æœï¼Œæ¨é€ SSEï¼Œè¿”å›
                for sent in sentences:
                    subtitle_manager.add_sentence(sent)
                return sentences

            elif decision.action == FuseAction.UPGRADE_SEPARATION:
                # ç†”æ–­å›æº¯ï¼šä½¿ç”¨åŸå§‹éŸ³é¢‘é‡æ–°åˆ†ç¦»
                self.logger.info(
                    f"Chunk {chunk_state.chunk_index} è§¦å‘ç†”æ–­ï¼Œ"
                    f"å‡çº§åˆ†ç¦»: {chunk_state.separation_level.value} â†’ {decision.next_separation_level.value}"
                )

                chunk_state = execute_fuse_upgrade(
                    chunk_state=chunk_state,
                    next_level=decision.next_separation_level,
                    demucs_service=demucs_service
                )

                # ç»§ç»­å¾ªç¯ï¼Œä½¿ç”¨å‡çº§åçš„éŸ³é¢‘é‡æ–°è½¬å½•
                continue

            else:
                # æœªçŸ¥åŠ¨ä½œï¼Œæ¥å—å½“å‰ç»“æœ
                for sent in sentences:
                    subtitle_manager.add_sentence(sent)
                return sentences

    async def _whisper_text_patch(
        self,
        sentence: 'SentenceSegment',
        sentence_index: int,
        audio_array: np.ndarray,
        job: 'JobState',
        subtitle_manager: 'StreamingSubtitleManager'
    ) -> 'SentenceSegment':
        """
        Whisper è¡¥åˆ€ï¼ˆæ—¶ç©ºè§£è€¦ç‰ˆï¼šä»…å–æ–‡æœ¬ï¼‰

        æ ¸å¿ƒåŸåˆ™ï¼š
        - SenseVoice ç¡®å®šçš„æ—¶é—´è½´ï¼ˆstart/endï¼‰ä¸å¯å˜
        - ä»…ä½¿ç”¨ Whisper çš„æ–‡æœ¬ç»“æœ
        - æ–°æ–‡æœ¬ä½¿ç”¨ä¼ªå¯¹é½ç”Ÿæˆå­—çº§æ—¶é—´æˆ³

        Args:
            sentence: åŸå§‹å¥å­ï¼ˆç”± SenseVoice ç”Ÿæˆï¼‰
            sentence_index: å¥å­ç´¢å¼•
            audio_array: å®Œæ•´éŸ³é¢‘æ•°ç»„
            job: ä»»åŠ¡çŠ¶æ€
            subtitle_manager: æµå¼å­—å¹•ç®¡ç†å™¨

        Returns:
            SentenceSegment: æ›´æ–°åçš„å¥å­
        """
        from .whisper_service import get_whisper_service
        from ..models.sensevoice_models import TextSource

        whisper_service = get_whisper_service()

        # æå–å¯¹åº”æ—¶é—´æ®µçš„éŸ³é¢‘ï¼ˆä½¿ç”¨ SenseVoice çš„æ—¶é—´çª—å£ï¼‰
        sr = 16000
        start_sample = int(sentence.start * sr)
        end_sample = int(sentence.end * sr)
        audio_segment = audio_array[start_sample:end_sample]

        # è·å–ä¸Šä¸‹æ–‡æç¤º
        context = subtitle_manager.get_context_window(sentence_index)

        # Whisper è½¬å½•ï¼ˆä»…å–æ–‡æœ¬ï¼Œå¼ƒç”¨æ—¶é—´æˆ³ï¼‰
        result = whisper_service.transcribe(
            audio=audio_segment,
            initial_prompt=context,
            language=job.settings.get('language', 'auto'),
            word_timestamps=False  # ä¸éœ€è¦å­—çº§æ—¶é—´æˆ³ï¼Œä½¿ç”¨ä¼ªå¯¹é½
        )

        whisper_text = result.get('text', '').strip()

        if not whisper_text:
            self.logger.warning(f"Whisper è¡¥åˆ€è¿”å›ç©ºæ–‡æœ¬ï¼Œä¿ç•™åŸç»“æœ")
            return sentence

        # ä¿å­˜ Whisper å¤‡é€‰æ–‡æœ¬
        sentence.whisper_alternative = whisper_text

        # ä½¿ç”¨ä¼ªå¯¹é½æ›´æ–°å¥å­
        subtitle_manager.update_sentence(
            index=sentence_index,
            new_text=whisper_text,
            source=TextSource.WHISPER_PATCH,
            confidence=self._estimate_whisper_confidence(result)
        )

        return subtitle_manager.sentences[sentence_index]

    def _estimate_whisper_confidence(self, result: dict) -> float:
        """ä¼°ç®— Whisper ç»“æœç½®ä¿¡åº¦"""
        segments = result.get('segments', [])
        if not segments:
            return 0.7

        # åŸºäº avg_logprob å’Œ no_speech_prob è®¡ç®—
        total_logprob = sum(s.get('avg_logprob', -0.5) for s in segments)
        avg_logprob = total_logprob / len(segments)

        avg_no_speech = sum(s.get('no_speech_prob', 0.1) for s in segments) / len(segments)

        # è½¬æ¢ä¸º 0-1 ç½®ä¿¡åº¦
        confidence = min(1.0, max(0.0, 1.0 + avg_logprob))  # logprob è¶Šæ¥è¿‘ 0 è¶Šå¥½
        confidence *= (1.0 - avg_no_speech)  # no_speech è¶Šä½è¶Šå¥½

        return round(confidence, 3)

    async def _post_process_enhancement(
        self,
        sentences: List['SentenceSegment'],
        audio_array: np.ndarray,
        job: 'JobState',
        subtitle_manager: 'StreamingSubtitleManager',
        solution_config: 'SolutionConfig'
    ) -> List['SentenceSegment']:
        """
        åå¤„ç†å¢å¼ºå±‚ï¼ˆæ‰€æœ‰ Chunk è½¬å½•å®Œæˆåæ‰§è¡Œï¼‰

        æ ¹æ®ç”¨æˆ·é…ç½®æ‰§è¡Œï¼š
        1. ä½ç½®ä¿¡åº¦å¥å­ â†’ Whisper è¡¥åˆ€ï¼ˆä»…æ–‡æœ¬ + ä¼ªå¯¹é½ï¼‰
        2. [å¯é€‰] LLM æ ¡å¯¹
        3. [å¯é€‰] LLM ç¿»è¯‘

        æ³¨æ„ï¼šè¿™ä¸æ˜¯ç†”æ–­ï¼Œç†”æ–­åœ¨è½¬å½•é˜¶æ®µå·²ç»å¤„ç†å®Œæˆ

        Args:
            sentences: æ‰€æœ‰å¥å­åˆ—è¡¨
            audio_array: å®Œæ•´éŸ³é¢‘æ•°ç»„
            job: ä»»åŠ¡çŠ¶æ€
            subtitle_manager: æµå¼å­—å¹•ç®¡ç†å™¨
            solution_config: æ–¹æ¡ˆé…ç½®

        Returns:
            List[SentenceSegment]: å¢å¼ºåçš„å¥å­åˆ—è¡¨
        """
        from .progress_tracker import get_progress_tracker, ProcessPhase
        from .solution_matrix import EnhancementMode, ProofreadMode, TranslateMode
        from ..core.thresholds import needs_whisper_patch

        progress_tracker = get_progress_tracker(job.job_id, solution_config.preset_id)

        # 1. æ”¶é›†éœ€è¦ Whisper è¡¥åˆ€çš„å¥å­ï¼ˆæ ¹æ®ç”¨æˆ·é…ç½®ï¼‰
        patch_queue = []
        if solution_config.enhancement != EnhancementMode.OFF:
            for i, sentence in enumerate(sentences):
                if needs_whisper_patch(sentence.confidence):
                    patch_queue.append((i, sentence))

        # 2. Whisper è¡¥åˆ€é˜¶æ®µ
        if patch_queue:
            progress_tracker.start_phase(ProcessPhase.WHISPER_PATCH, len(patch_queue), "Whisper è¡¥åˆ€ä¸­...")

            for idx, (sent_idx, sentence) in enumerate(patch_queue):
                await self._whisper_text_patch(
                    sentence, sent_idx, audio_array, job, subtitle_manager
                )
                progress_tracker.update_phase(ProcessPhase.WHISPER_PATCH, increment=1)

            progress_tracker.complete_phase(ProcessPhase.WHISPER_PATCH)

        # 3. [å¯é€‰] LLM æ ¡å¯¹
        if solution_config.proofread != ProofreadMode.OFF:
            # TODO: å®ç° LLM æ ¡å¯¹
            self.logger.info("LLM æ ¡å¯¹åŠŸèƒ½å¾…å®ç°")

        # 4. [å¯é€‰] LLM ç¿»è¯‘
        if solution_config.translate != TranslateMode.OFF:
            # TODO: å®ç° LLM ç¿»è¯‘
            self.logger.info("LLM ç¿»è¯‘åŠŸèƒ½å¾…å®ç°")

        return subtitle_manager.get_all_sentences()

    async def _process_video_sensevoice(self, job: 'JobState'):
        """
        SenseVoice ä¸»å¤„ç†æµç¨‹ï¼ˆv2.1 æ¦‚å¿µæ¾„æ¸…ç‰ˆï¼‰

        æµç¨‹è¯´æ˜ï¼š
        1-4: å‡†å¤‡é˜¶æ®µï¼ˆéŸ³é¢‘æå–ã€VADã€é¢‘è°±åˆ†è¯Šã€æŒ‰éœ€åˆ†ç¦»ï¼‰
        5: è½¬å½•é˜¶æ®µï¼ˆé€Chunkè½¬å½• + ç†”æ–­å›æº¯ï¼‰
        6-8: åå¤„ç†å¢å¼ºé˜¶æ®µï¼ˆWhisperè¡¥åˆ€ã€LLMæ ¡å¯¹/ç¿»è¯‘ï¼‰
        9: è¾“å‡ºé˜¶æ®µï¼ˆç”Ÿæˆå­—å¹•ï¼‰
        """
        from .streaming_subtitle import get_streaming_subtitle_manager, remove_streaming_subtitle_manager
        from .progress_tracker import get_progress_tracker, remove_progress_tracker, ProcessPhase
        from .solution_matrix import SolutionConfig, TranslateMode
        from .audio_spectrum_classifier import get_spectrum_classifier
        from .demucs_service import get_demucs_service
        from .sse_service import get_sse_manager
        from ..models.circuit_breaker_models import SeparationLevel
        from pathlib import Path

        def push_signal_event(sse_manager, job_id: str, signal_code: str, message: str = ""):
            """æ¨é€ä¿¡å·äº‹ä»¶"""
            sse_manager.broadcast_sync(
                channel=f"job_{job_id}",
                event_type="signal",
                data={"code": signal_code, "message": message}
            )

        # è·å–æ–¹æ¡ˆé…ç½®
        preset_id = getattr(job.settings.sensevoice, 'preset_id', 'default')
        solution_config = SolutionConfig.from_preset(preset_id)

        # åˆå§‹åŒ–ç®¡ç†å™¨
        subtitle_manager = get_streaming_subtitle_manager(job.job_id)
        progress_tracker = get_progress_tracker(job.job_id, preset_id)

        try:
            # 1. éŸ³é¢‘æå–
            progress_tracker.start_phase(ProcessPhase.EXTRACT, 1, "æå–éŸ³é¢‘...")
            audio_array, sr = self._extract_audio_with_array(job.input_file, job, target_sr=16000)
            progress_tracker.complete_phase(ProcessPhase.EXTRACT)

            # 2. VAD ç‰©ç†åˆ‡åˆ†
            progress_tracker.start_phase(ProcessPhase.VAD, 1, "VAD åˆ‡åˆ†...")
            vad_segments = self._memory_vad_split(audio_array, sr, job)
            progress_tracker.complete_phase(ProcessPhase.VAD)

            # 3. é¢‘è°±åˆ†è¯Šï¼ˆChunkçº§åˆ«ï¼‰
            progress_tracker.start_phase(ProcessPhase.BGM_DETECT, 1, "é¢‘è°±åˆ†è¯Š...")
            spectrum_classifier = get_spectrum_classifier()
            diagnoses = spectrum_classifier.diagnose_chunks(
                [(audio_array[int(s['start']*sr):int(s['end']*sr)], s['start'], s['end'])
                 for s in vad_segments],
                sr=sr
            )
            progress_tracker.complete_phase(ProcessPhase.BGM_DETECT)

            # 4. åˆå§‹åŒ– Chunk çŠ¶æ€ + æŒ‰éœ€äººå£°åˆ†ç¦»
            chunk_states = self._init_chunk_states(vad_segments, audio_array, sr)
            demucs_service = get_demucs_service()

            chunks_to_separate = [(i, chunk_states[i], diagnoses[i])
                                  for i in range(len(diagnoses))
                                  if diagnoses[i].need_separation]

            if chunks_to_separate:
                progress_tracker.start_phase(ProcessPhase.DEMUCS, len(chunks_to_separate), "äººå£°åˆ†ç¦»...")
                for sep_idx, (chunk_idx, chunk_state, diag) in enumerate(chunks_to_separate):
                    # æ‰§è¡Œåˆ†ç¦»ï¼Œæ›´æ–° current_audio
                    separated_audio = demucs_service.separate_chunk(
                        audio=chunk_state.original_audio,
                        model=diag.recommended_model,
                        sr=sr
                    )
                    chunk_state.current_audio = separated_audio
                    chunk_state.separation_level = (
                        SeparationLevel.HTDEMUCS if diag.recommended_model == "htdemucs"
                        else SeparationLevel.MDX_EXTRA
                    )
                    chunk_state.separation_model_used = diag.recommended_model
                    progress_tracker.update_phase(ProcessPhase.DEMUCS, increment=1)
                progress_tracker.complete_phase(ProcessPhase.DEMUCS)

            # 5. é€Chunkè½¬å½• + ç†”æ–­å›æº¯ï¼ˆè½¬å½•å±‚æ ¸å¿ƒï¼‰
            progress_tracker.start_phase(ProcessPhase.SENSEVOICE, len(chunk_states), "SenseVoice è½¬å½•...")
            all_sentences = []

            for chunk_state in chunk_states:
                # å•ä¸ª Chunk è½¬å½•ï¼ˆå«ç†”æ–­å›æº¯å¾ªç¯ï¼‰
                sentences = await self._transcribe_chunk_with_fusing(
                    chunk_state=chunk_state,
                    job=job,
                    subtitle_manager=subtitle_manager,
                    demucs_service=demucs_service
                )
                all_sentences.extend(sentences)
                progress_tracker.update_phase(ProcessPhase.SENSEVOICE, increment=1)

            progress_tracker.complete_phase(ProcessPhase.SENSEVOICE)

            # 6. åå¤„ç†å¢å¼ºï¼ˆWhisperè¡¥åˆ€ã€LLMæ ¡å¯¹/ç¿»è¯‘ï¼‰
            final_results = await self._post_process_enhancement(
                all_sentences, audio_array, job, subtitle_manager, solution_config
            )

            # 7. ç”Ÿæˆå­—å¹•
            progress_tracker.start_phase(ProcessPhase.SRT, 1, "ç”Ÿæˆå­—å¹•...")
            output_path = str(Path(job.job_dir) / f"{job.job_id}.srt")
            self._generate_subtitle_from_sentences(
                final_results,
                output_path,
                include_translation=(solution_config.translate != TranslateMode.OFF)
            )
            progress_tracker.complete_phase(ProcessPhase.SRT)

            # 8. å®Œæˆ
            job.status = 'completed'
            push_signal_event(get_sse_manager(), job.job_id, "job_complete", "å¤„ç†å®Œæˆ")

        except Exception as e:
            self.logger.error(f"SenseVoice å¤„ç†å¤±è´¥: {e}", exc_info=True)
            job.status = 'failed'
            job.error = str(e)
            push_signal_event(get_sse_manager(), job.job_id, "job_failed", str(e))
            raise

        finally:
            # æ¸…ç†èµ„æº
            remove_streaming_subtitle_manager(job.job_id)
            remove_progress_tracker(job.job_id)


# å•ä¾‹å¤„ç†å™¨
_service_instance: Optional[TranscriptionService] = None


def get_transcription_service(root: str) -> TranscriptionService:
    """è·å–è½¬å½•æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _service_instance
    if _service_instance is None:
        _service_instance = TranscriptionService(root)
    return _service_instance