"""
æ¨¡å‹é¢„åŠ è½½å’Œç¼“å­˜ç®¡ç†å™¨
å®ç°æ¨¡å‹é¢„åŠ è½½ã€LRUç¼“å­˜ã€å†…å­˜ç›‘æ§ç­‰åŠŸèƒ½
"""

import os
import gc
import logging
import threading
import time
import asyncio
from collections import OrderedDict
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any
import psutil
import torch
import whisperx

# ä¿®å¤å¯¼å…¥è·¯å¾„
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.job_models import JobSettings


@dataclass
class ModelCacheInfo:
    """æ¨¡å‹ç¼“å­˜ä¿¡æ¯"""
    model: Any
    key: Tuple[str, str, str]
    load_time: float
    last_used: float
    memory_size: int  # ä¼°ç®—çš„å†…å­˜å ç”¨(MB)


@dataclass
class PreloadConfig:
    """é¢„åŠ è½½é…ç½®"""
    enabled: bool = True
    default_models: List[str] = None
    max_cache_size: int = 3  # æœ€å¤§ç¼“å­˜æ¨¡å‹æ•°é‡
    memory_threshold: float = 0.8  # å†…å­˜ä½¿ç”¨é˜ˆå€¼(80%)
    preload_timeout: int = 300  # é¢„åŠ è½½è¶…æ—¶æ—¶é—´(ç§’)
    warmup_enabled: bool = True  # æ˜¯å¦å¯ç”¨é¢„çƒ­

    def __post_init__(self):
        if self.default_models is None:
            self.default_models = ["medium"]


class ModelPreloadManager:
    """ç®€åŒ–ç‰ˆæ¨¡å‹é¢„åŠ è½½å’Œç¼“å­˜ç®¡ç†å™¨ - æ–¹æ¡ˆäºŒå®ç°
    
    æ ¸å¿ƒæ”¹è¿›:
    1. ç»Ÿä¸€é”æœºåˆ¶é¿å…æ­»é”
    2. å¹‚ç­‰æ€§é¢„åŠ è½½é¿å…é‡å¤æ‰§è¡Œ
    3. ç¼“å­˜ç‰ˆæœ¬å·ç¡®ä¿çŠ¶æ€åŒæ­¥
    4. æ ‡å‡†åŒ–æ—¥å¿—ä¾¿äºè°ƒè¯•
    """
    
    def __init__(self, config: PreloadConfig = None):
        self.config = config or PreloadConfig()
        self.logger = self._setup_logger()
        
        # æ¨¡å‹ç¼“å­˜ (LRU)
        self._whisper_cache: OrderedDict[Tuple[str, str, str], ModelCacheInfo] = OrderedDict()
        self._align_cache: OrderedDict[str, Tuple[Any, Any, float]] = OrderedDict()
        
        # ç»Ÿä¸€é” - ç®€åŒ–å¹¶å‘æ§åˆ¶ï¼Œé¿å…å¤šé”æ­»é”
        self._global_lock = threading.RLock()
        
        # ç®€åŒ–çš„é¢„åŠ è½½çŠ¶æ€ - å•ä¸€æ•°æ®æº
        self._preload_status = {
            "is_preloading": False,
            "progress": 0.0,
            "current_model": "",
            "total_models": 0,
            "loaded_models": 0,
            "errors": [],
            "failed_attempts": 0,
            "last_attempt_time": 0,
            "max_retry_attempts": 3,
            "retry_cooldown": 30,
            "cache_version": int(time.time())  # ç¼“å­˜ç‰ˆæœ¬å·ï¼Œç”¨äºçŠ¶æ€åŒæ­¥
        }
        
        # é¢„åŠ è½½ä»»åŠ¡ç®¡ç† - å®ç°å¹‚ç­‰æ€§
        self._preload_promise: Optional[asyncio.Task] = None
        
        # å†…å­˜ç›‘æ§
        self._memory_monitor = MemoryMonitor()
        
        self.logger.info("ğŸ—ï¸ ModelPreloadManageråˆå§‹åŒ–å®Œæˆ - ç®€åŒ–æ¶æ„")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ ‡å‡†åŒ–çš„æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(f"{__name__}.ModelPreloadManager")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - ğŸ¤–[æ¨¡å‹ç®¡ç†] - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
        
    def get_preload_status(self) -> Dict[str, Any]:
        """è·å–é¢„åŠ è½½çŠ¶æ€ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            status = self._preload_status.copy()
            self.logger.debug(f"ğŸ“Š çŠ¶æ€æŸ¥è¯¢: é¢„åŠ è½½={status['is_preloading']}, è¿›åº¦={status['progress']:.1f}%, å·²åŠ è½½={status['loaded_models']}")
            return status
    
    def get_cache_status(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çŠ¶æ€ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            whisper_models = [
                {
                    "key": info.key,
                    "memory_mb": info.memory_size,
                    "last_used": info.last_used,
                    "load_time": info.load_time
                }
                for info in self._whisper_cache.values()
            ]
            
            align_models = list(self._align_cache.keys())
            total_memory = sum(info.memory_size for info in self._whisper_cache.values())
            
            cache_status = {
                "whisper_models": whisper_models,
                "align_models": align_models,
                "total_memory_mb": total_memory,
                "max_cache_size": self.config.max_cache_size,
                "memory_info": self._memory_monitor.get_memory_info(),
                "cache_version": self._preload_status["cache_version"]
            }
            
            self.logger.debug(f"ğŸ’¾ ç¼“å­˜æŸ¥è¯¢: Whisperæ¨¡å‹={len(whisper_models)}ä¸ª, å¯¹é½æ¨¡å‹={len(align_models)}ä¸ª, å†…å­˜={total_memory}MB")
            return cache_status
    
    async def preload_models(self, progress_callback=None) -> Dict[str, Any]:
        """é¢„åŠ è½½é»˜è®¤æ¨¡å‹ - ç®€åŒ–ç‰ˆå®ç°ï¼Œå¸¦å¹‚ç­‰æ€§ä¿è¯

        é¢„åŠ è½½é€»è¾‘ï¼š
        1. å¦‚æœç”¨æˆ·é€‰æ‹©äº†é»˜è®¤é¢„åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹
        2. å¦‚æœç”¨æˆ·æœªé€‰æ‹©ï¼ˆæˆ–é€‰æ‹©çš„æ¨¡å‹ä¸å¯ç”¨ï¼‰ï¼Œè‡ªåŠ¨é€‰æ‹©ä½“ç§¯æœ€å¤§çš„readyæ¨¡å‹
        3. å¦‚æœæ²¡æœ‰readyçš„æ¨¡å‹ï¼Œè¿”å›å¤±è´¥
        """
        with self._global_lock:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨é¢„åŠ è½½ä¸­ï¼ˆå¹‚ç­‰æ€§æ£€æŸ¥ï¼‰
            if self._preload_status["is_preloading"]:
                self.logger.info("âš¡ é¢„åŠ è½½å·²åœ¨è¿›è¡Œä¸­ï¼Œè¿”å›å·²æœ‰ä»»åŠ¡")
                return {"success": True, "message": "é¢„åŠ è½½å·²åœ¨è¿›è¡Œä¸­"}

            if not self.config.enabled:
                self.logger.warning("âš ï¸ æ¨¡å‹é¢„åŠ è½½åŠŸèƒ½å·²ç¦ç”¨")
                return {"success": False, "message": "é¢„åŠ è½½åŠŸèƒ½å·²ç¦ç”¨"}

            # ğŸ”„ æ ¹æ®ç”¨æˆ·é…ç½®ç¡®å®šè¦åŠ è½½çš„æ¨¡å‹
            from services.user_config_service import get_user_config_service
            from services.model_manager_service import get_model_manager as get_model_mgr

            user_config = get_user_config_service()
            model_mgr = get_model_mgr()

            # è·å–ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹
            user_selected = user_config.get_default_preload_model()

            # è·å–æ‰€æœ‰readyçš„æ¨¡å‹
            ready_models = model_mgr.get_ready_whisper_models() if model_mgr else []

            # ç¡®å®šè¦åŠ è½½çš„æ¨¡å‹
            models_to_load = []
            if user_selected and user_selected in ready_models:
                # ç”¨æˆ·é€‰æ‹©äº†æœ‰æ•ˆçš„æ¨¡å‹
                models_to_load = [user_selected]
                self.logger.info(f"âœ… ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„é»˜è®¤é¢„åŠ è½½æ¨¡å‹: {user_selected}")
            else:
                # ç”¨æˆ·æœªé€‰æ‹©æˆ–é€‰æ‹©çš„æ¨¡å‹ä¸å¯ç”¨ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€å¤§çš„æ¨¡å‹
                largest_model = model_mgr.get_largest_ready_model() if model_mgr else None
                if largest_model:
                    models_to_load = [largest_model]
                    self.logger.info(f"âœ… è‡ªåŠ¨é€‰æ‹©ä½“ç§¯æœ€å¤§çš„readyæ¨¡å‹: {largest_model}")
                else:
                    self.logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„readyæ¨¡å‹")
                    return {"success": False, "message": "æ²¡æœ‰å¯ç”¨çš„readyæ¨¡å‹"}

            # è®¾ç½®é¢„åŠ è½½çŠ¶æ€
            self._preload_status.update({
                "is_preloading": True,
                "progress": 0.0,
                "current_model": "",
                "total_models": len(models_to_load),
                "loaded_models": 0,
                "errors": [],
                "last_attempt_time": time.time()
            })

            self.logger.info(f"ğŸš€ å¼€å§‹é¢„åŠ è½½ä»»åŠ¡: {models_to_load}")

        try:
            success_count = 0
            total_models = len(models_to_load)

            for i, model_name in enumerate(models_to_load):
                try:
                    # æ›´æ–°å½“å‰è¿›åº¦
                    with self._global_lock:
                        self._preload_status.update({
                            "current_model": model_name,
                            "progress": (i / total_models) * 100
                        })

                    self.logger.info(f"ğŸ”„ [{i+1}/{total_models}] å¤„ç†æ¨¡å‹: {model_name}")

                    # âœ… æ–°å¢ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£åœ¨ä¸‹è½½ï¼ˆé˜²å¾¡æœºåˆ¶ï¼‰
                    from services.model_manager_service import get_model_manager
                    model_mgr = get_model_manager()

                    if model_mgr.is_model_downloading("whisper", model_name):
                        self.logger.info(f"â³ æ¨¡å‹ {model_name} æ­£åœ¨ä¸‹è½½ä¸­ï¼Œç­‰å¾…å®Œæˆ...")

                        # æ›´æ–°çŠ¶æ€
                        with self._global_lock:
                            self._preload_status.update({
                                "current_model": f"{model_name} (ç­‰å¾…ä¸‹è½½å®Œæˆ)",
                                "progress": (i / total_models) * 100
                            })

                        # ç­‰å¾…ä¸‹è½½å®Œæˆï¼ˆæœ€å¤š10åˆ†é’Ÿï¼‰
                        if not model_mgr.wait_for_download_complete("whisper", model_name, timeout=600):
                            error_msg = f"ç­‰å¾…æ¨¡å‹ {model_name} ä¸‹è½½è¶…æ—¶æˆ–å¤±è´¥"
                            self.logger.warning(f"âš ï¸ {error_msg}")
                            with self._global_lock:
                                self._preload_status["errors"].append(error_msg)
                            continue

                        self.logger.info(f"âœ… æ¨¡å‹ {model_name} ä¸‹è½½å®Œæˆï¼Œç»§ç»­é¢„åŠ è½½")

                    # âœ… æ–°å¢ï¼šæ£€æŸ¥ç£ç›˜çŠ¶æ€ï¼ˆç¡®ä¿æ¨¡å‹å·²å°±ç»ªï¼‰
                    status, local_path, detail = model_mgr._check_whisper_model_exists(model_name)
                    if status != "ready":
                        error_msg = f"æ¨¡å‹ {model_name} æœªå°±ç»ªï¼ˆçŠ¶æ€: {status}ï¼‰ï¼Œè·³è¿‡é¢„åŠ è½½"
                        self.logger.warning(f"âš ï¸ {error_msg}")
                        with self._global_lock:
                            self._preload_status["errors"].append(error_msg)
                        continue

                    # æ£€æŸ¥å†…å­˜
                    if not self._memory_monitor.check_memory_available():
                        error_msg = f"å†…å­˜ä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹ {model_name}"
                        self.logger.warning(f"âš ï¸ {error_msg}")
                        with self._global_lock:
                            self._preload_status["errors"].append(error_msg)
                        continue

                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
                    device = "cuda" if torch.cuda.is_available() else "cpu"
                    key = (model_name, "float16", device)

                    with self._global_lock:
                        if key in self._whisper_cache:
                            self.logger.info(f"âœ… æ¨¡å‹ {model_name} å·²åœ¨ç¼“å­˜ä¸­")
                            self._preload_status["loaded_models"] += 1
                            success_count += 1
                            continue

                    # åŠ è½½æ–°æ¨¡å‹
                    settings = JobSettings(
                        model=model_name,
                        compute_type="float16",
                        device=device
                    )

                    self.logger.info(f"ğŸ” å¼€å§‹åŠ è½½æ¨¡å‹: {model_name} (device={device})")
                    start_time = time.time()

                    # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡ŒåŒæ­¥çš„æ¨¡å‹åŠ è½½ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
                    loop = asyncio.get_event_loop()
                    model = await loop.run_in_executor(None, self._load_whisper_model, settings)

                    load_time = time.time() - start_time

                    # é¢„çƒ­æ¨¡å‹
                    if self.config.warmup_enabled:
                        self.logger.info(f"ğŸ”¥ é¢„çƒ­æ¨¡å‹: {model_name}")
                        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œé¢„çƒ­ï¼Œé¿å…é˜»å¡
                        await loop.run_in_executor(None, self._warmup_model, model)

                    with self._global_lock:
                        self._preload_status["loaded_models"] += 1
                    success_count += 1

                    self.logger.info(f"âœ… æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ (è€—æ—¶: {load_time:.2f}s)")

                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        with self._global_lock:
                            progress_callback(self._preload_status.copy())

                except Exception as e:
                    error_msg = f"åŠ è½½æ¨¡å‹ {model_name} å¤±è´¥: {str(e)}"
                    self.logger.error(f"âŒ {error_msg}", exc_info=True)
                    with self._global_lock:
                        self._preload_status["errors"].append(error_msg)
            
            # å®Œæˆé¢„åŠ è½½
            success = success_count > 0
            
            with self._global_lock:
                # æ›´æ–°å¤±è´¥è®¡æ•°
                if not success:
                    self._preload_status["failed_attempts"] += 1
                else:
                    self._preload_status["failed_attempts"] = 0  # æˆåŠŸåé‡ç½®
                
                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())
                
                # ç»“æŸé¢„åŠ è½½çŠ¶æ€
                self._preload_status.update({
                    "is_preloading": False,
                    "progress": 100.0,
                    "current_model": ""
                })
                
                result = {
                    "success": success,
                    "loaded_models": self._preload_status["loaded_models"],
                    "total_models": self._preload_status["total_models"],
                    "errors": self._preload_status["errors"].copy(),
                    "failed_attempts": self._preload_status["failed_attempts"],
                    "cache_version": self._preload_status["cache_version"]
                }
            
            # æ—¥å¿—è¾“å‡º
            if success:
                self.logger.info(f"âœ… é¢„åŠ è½½ä»»åŠ¡æˆåŠŸå®Œæˆ: {success_count}/{total_models} ä¸ªæ¨¡å‹")
            else:
                self.logger.warning(f"âš ï¸ é¢„åŠ è½½ä»»åŠ¡å®Œæˆä½†æ— æˆåŠŸåŠ è½½çš„æ¨¡å‹")
                
            if result["errors"]:
                self.logger.warning(f"âš ï¸ é¢„åŠ è½½è¿‡ç¨‹ä¸­å‡ºç° {len(result['errors'])} ä¸ªé”™è¯¯")
            
            # è°ƒç”¨æœ€ç»ˆè¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(self._preload_status.copy())
            
            return result
            
        except Exception as e:
            with self._global_lock:
                self._preload_status["is_preloading"] = False
                self._preload_status["failed_attempts"] += 1
            
            self.logger.error(f"âŒ é¢„åŠ è½½è¿‡ç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
            return {
                "success": False, 
                "message": f"é¢„åŠ è½½å¤±è´¥: {str(e)}", 
                "failed_attempts": self._preload_status["failed_attempts"]
            }

    def reset_preload_attempts(self):
        """é‡ç½®é¢„åŠ è½½å¤±è´¥è®¡æ•° - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            old_attempts = self._preload_status["failed_attempts"]
            self._preload_status["failed_attempts"] = 0
            self._preload_status["last_attempt_time"] = 0
            self._preload_status["cache_version"] = int(time.time())
            
        self.logger.info(f"ğŸ”„ é¢„åŠ è½½å¤±è´¥è®¡æ•°å·²é‡ç½®: {old_attempts} -> 0")
    
    def get_model(self, settings: JobSettings):
        """è·å–Whisperæ¨¡å‹ (å¸¦LRUç¼“å­˜) - ç®€åŒ–ç‰ˆæœ¬"""
        key = (settings.model, settings.compute_type, settings.device)
        
        with self._global_lock:
            # å‘½ä¸­ç¼“å­˜
            if key in self._whisper_cache:
                info = self._whisper_cache[key]
                info.last_used = time.time()
                # ç§»åˆ°æœ€å (æœ€è¿‘ä½¿ç”¨)
                self._whisper_cache.move_to_end(key)
                self.logger.debug(f"âœ… å‘½ä¸­æ¨¡å‹ç¼“å­˜: {key}")
                return info.model
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½æ–°æ¨¡å‹
            self.logger.info(f"ğŸ”„ éœ€è¦åŠ è½½æ–°æ¨¡å‹: {key}")
            return self._load_whisper_model(settings)
    
    def _load_whisper_model(self, settings: JobSettings):
        """åŠ è½½Whisperæ¨¡å‹ - ç®€åŒ–ç‰ˆæœ¬å¸¦å¹¶å‘ä¿æŠ¤"""
        key = (settings.model, settings.compute_type, settings.device)

        # å†æ¬¡æ£€æŸ¥ç¼“å­˜ï¼ˆé¿å…å¹¶å‘åŠ è½½åŒä¸€æ¨¡å‹ï¼‰
        with self._global_lock:
            if key in self._whisper_cache:
                info = self._whisper_cache[key]
                info.last_used = time.time()
                self._whisper_cache.move_to_end(key)
                self.logger.debug(f"âš¡ å¹¶å‘æ£€æŸ¥å‘½ä¸­ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½: {key}")
                return info.model

        self.logger.info(f"ğŸ” å¼€å§‹åŠ è½½æ–°Whisperæ¨¡å‹: {key}")

        # æ£€æŸ¥å†…å­˜
        if not self._memory_monitor.check_memory_available():
            self.logger.warning("âš ï¸ å†…å­˜ä¸è¶³ï¼Œå°è¯•æ¸…ç†ç¼“å­˜")
            self._cleanup_old_models()

        # æ£€æŸ¥ç¼“å­˜å¤§å°
        with self._global_lock:
            if len(self._whisper_cache) >= self.config.max_cache_size:
                self._evict_lru_model()

        try:
            start_time = time.time()
            self.logger.info(f"ğŸš€ æ­£åœ¨ä»ç£ç›˜åŠ è½½æ¨¡å‹ {settings.model} (device={settings.device}, compute_type={settings.compute_type})")

            # å¯¼å…¥é…ç½®ä»¥è·å–ç¼“å­˜è·¯å¾„
            from core.config import config

            # âœ… ä¿®å¤ï¼šæ·»åŠ  download_root å’Œ local_files_only å‚æ•°ï¼Œé¿å…é‡å¤ä¸‹è½½
            model = whisperx.load_model(
                settings.model,
                settings.device,
                compute_type=settings.compute_type,
                download_root=str(config.HF_CACHE_DIR),  # æŒ‡å®šç¼“å­˜è·¯å¾„
                local_files_only=True  # ç¦æ­¢è‡ªåŠ¨ä¸‹è½½ï¼Œåªä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            load_time = time.time() - start_time

            # ä¼°ç®—å†…å­˜ä½¿ç”¨
            memory_size = self._estimate_model_memory(model)

            # æ·»åŠ åˆ°ç¼“å­˜
            info = ModelCacheInfo(
                model=model,
                key=key,
                load_time=load_time,
                last_used=time.time(),
                memory_size=memory_size
            )

            with self._global_lock:
                self._whisper_cache[key] = info
                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())

            self.logger.info(f"âœ… æˆåŠŸåŠ è½½å¹¶ç¼“å­˜Whisperæ¨¡å‹ {key} (å†…å­˜: {memory_size}MB, è€—æ—¶: {load_time:.2f}s)")
            return model

        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½Whisperæ¨¡å‹å¤±è´¥ {key}: {str(e)}", exc_info=True)
            raise
    
    def get_align_model(self, lang: str, device: str):
        """è·å–å¯¹é½æ¨¡å‹ (å¸¦LRUç¼“å­˜) - ç®€åŒ–ç‰ˆæœ¬"""
        with self._global_lock:
            # å‘½ä¸­ç¼“å­˜
            if lang in self._align_cache:
                model, meta, last_used = self._align_cache[lang]
                # æ›´æ–°ä½¿ç”¨æ—¶é—´å¹¶ç§»åˆ°æœ€å
                self._align_cache[lang] = (model, meta, time.time())
                self._align_cache.move_to_end(lang)
                self.logger.debug(f"âœ… å‘½ä¸­å¯¹é½æ¨¡å‹ç¼“å­˜: {lang}")
                return model, meta
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½æ–°æ¨¡å‹
            self.logger.info(f"ğŸ”„ åŠ è½½æ–°å¯¹é½æ¨¡å‹: {lang}")
            try:
                # âœ… ä¿®å¤ï¼šæ·»åŠ  model_dir å‚æ•°ï¼ŒæŒ‡å®šç¼“å­˜è·¯å¾„
                from core.config import config
                model, meta = whisperx.load_align_model(
                    language_code=lang,
                    device=device,
                    model_dir=str(config.HF_CACHE_DIR)  # æŒ‡å®šç¼“å­˜è·¯å¾„
                )
                
                # æ·»åŠ åˆ°ç¼“å­˜ (é™åˆ¶å¤§å°)
                if len(self._align_cache) >= 5:  # å¯¹é½æ¨¡å‹ç¼“å­˜ä¸Šé™
                    # ç§»é™¤æœ€æ—§çš„
                    oldest = next(iter(self._align_cache))
                    del self._align_cache[oldest]
                    self.logger.debug(f"ğŸ—‘ï¸ ç§»é™¤æœ€æ—§å¯¹é½æ¨¡å‹: {oldest}")
                
                self._align_cache[lang] = (model, meta, time.time())
                
                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())
                
                self.logger.info(f"âœ… æˆåŠŸåŠ è½½å¯¹é½æ¨¡å‹: {lang}")
                return model, meta
                
            except Exception as e:
                self.logger.error(f"âŒ åŠ è½½å¯¹é½æ¨¡å‹å¤±è´¥ {lang}: {str(e)}", exc_info=True)
                raise
    
    def _warmup_model(self, model):
        """é¢„çƒ­æ¨¡å‹ - ç©ºè·‘ä¸€æ¬¡ç¡®ä¿å®Œå…¨åŠ è½½"""
        try:
            self.logger.debug("å¼€å§‹æ¨¡å‹é¢„çƒ­")
            
            # åˆ›å»ºè™šæ‹ŸéŸ³é¢‘æ•°æ® (1ç§’é™éŸ³)
            import numpy as np
            dummy_audio = np.zeros(16000, dtype=np.float32)  # 16kHz 1ç§’
            
            # ç©ºè·‘ä¸€æ¬¡
            _ = model.transcribe(dummy_audio, batch_size=1, verbose=False)
            
            self.logger.debug("æ¨¡å‹é¢„çƒ­å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {str(e)}")
    
    def _evict_lru_model(self):
        """é©±é€æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹ - éœ€è¦åœ¨é”å†…è°ƒç”¨"""
        if not self._whisper_cache:
            return
        
        # æœ€ä¹…æœªä½¿ç”¨çš„åœ¨å¼€å¤´
        oldest_key = next(iter(self._whisper_cache))
        info = self._whisper_cache.pop(oldest_key)
        
        self.logger.info(f"ğŸ—‘ï¸ é©±é€LRUæ¨¡å‹: {oldest_key}, é‡Šæ”¾å†…å­˜: {info.memory_size}MB")
        
        # é‡Šæ”¾å†…å­˜
        del info.model
        del info
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def _cleanup_old_models(self):
        """æ¸…ç†æ—§æ¨¡å‹é‡Šæ”¾å†…å­˜ - éœ€è¦åœ¨é”å¤–è°ƒç”¨"""
        current_time = time.time()
        to_remove = []
        
        with self._global_lock:
            for key, info in self._whisper_cache.items():
                # è¶…è¿‡10åˆ†é’Ÿæœªä½¿ç”¨çš„æ¨¡å‹
                if current_time - info.last_used > 600:
                    to_remove.append(key)
            
            for key in to_remove:
                info = self._whisper_cache.pop(key)
                self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ—§æ¨¡å‹: {key}")
                del info.model
                del info
        
        if to_remove:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            self.logger.info(f"ğŸ’« æ¸…ç†äº† {len(to_remove)} ä¸ªæ—§æ¨¡å‹")
    
    def _estimate_model_memory(self, model) -> int:
        """ä¼°ç®—æ¨¡å‹å†…å­˜ä½¿ç”¨ (MB)"""
        try:
            # ç®€å•ä¼°ç®—ï¼ŒåŸºäºæ¨¡å‹å‚æ•°
            if hasattr(model, 'model') and hasattr(model.model, 'parameters'):
                total_params = sum(p.numel() for p in model.model.parameters())
                # å‡è®¾æ¯ä¸ªå‚æ•°4å­—èŠ‚ (float32) æˆ– 2å­—èŠ‚ (float16)
                bytes_per_param = 2  # float16
                total_bytes = total_params * bytes_per_param
                return int(total_bytes / (1024 * 1024))  # è½¬æ¢ä¸ºMB
        except:
            pass
        
        # é»˜è®¤ä¼°ç®—å€¼
        return 500  # é»˜è®¤500MB
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç«‹å³åŒæ­¥çŠ¶æ€"""
        with self._global_lock:
            # è®°å½•æ¸…ç†å‰çš„ç¼“å­˜çŠ¶æ€
            whisper_count = len(self._whisper_cache)
            align_count = len(self._align_cache)
            total_memory = sum(info.memory_size for info in self._whisper_cache.values())
            
            # æ¸…ç†Whisperæ¨¡å‹ç¼“å­˜
            for info in self._whisper_cache.values():
                del info.model
            self._whisper_cache.clear()
            
            # æ¸…ç†å¯¹é½æ¨¡å‹ç¼“å­˜
            self._align_cache.clear()
            
            # ç«‹å³æ›´æ–°é¢„åŠ è½½çŠ¶æ€ - è§£å†³çŠ¶æ€åŒæ­¥é—®é¢˜
            self._preload_status.update({
                "loaded_models": 0,
                "is_preloading": False,
                "progress": 0.0,
                "current_model": "",
                "errors": [],
                "cache_version": int(time.time())  # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
            })
            
        # åƒåœ¾å›æ”¶å’ŒGPUå†…å­˜æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰æ¨¡å‹ç¼“å­˜: Whisper={whisper_count}ä¸ª, å¯¹é½={align_count}ä¸ª, é‡Šæ”¾å†…å­˜={total_memory}MB")

    def evict_model(self, model_id: str, device: str = "cuda", compute_type: str = "float16"):
        """
        æ¸…ç†æŒ‡å®šWhisperæ¨¡å‹çš„ç¼“å­˜

        Args:
            model_id: æ¨¡å‹ID
            device: è®¾å¤‡ç±»å‹
            compute_type: è®¡ç®—ç±»å‹
        """
        key = (model_id, compute_type, device)

        with self._global_lock:
            if key in self._whisper_cache:
                info = self._whisper_cache.pop(key)
                self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ¨¡å‹ç¼“å­˜: {key}, é‡Šæ”¾å†…å­˜: {info.memory_size}MB")

                # é‡Šæ”¾å†…å­˜
                del info.model
                del info

                # æ›´æ–°é¢„åŠ è½½çŠ¶æ€ä¸­çš„loaded_modelsè®¡æ•°
                self._preload_status["loaded_models"] = len(self._whisper_cache)
                self._preload_status["cache_version"] = int(time.time())

        # åƒåœ¾å›æ”¶å’ŒGPUå†…å­˜æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def evict_align_model(self, language: str):
        """
        æ¸…ç†æŒ‡å®šå¯¹é½æ¨¡å‹çš„ç¼“å­˜

        Args:
            language: è¯­è¨€ä»£ç 
        """
        with self._global_lock:
            if language in self._align_cache:
                self._align_cache.pop(language)
                self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†å¯¹é½æ¨¡å‹ç¼“å­˜: {language}")
                self._preload_status["cache_version"] = int(time.time())

        # åƒåœ¾å›æ”¶å’ŒGPUå†…å­˜æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ========== å•æ¨¡å‹ç®¡ç†æ¥å£ - å§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡ ==========

    def download_whisper_model(self, model_id: str) -> bool:
        """
        ä¸‹è½½å•ä¸ªWhisperæ¨¡å‹ï¼ˆå§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡ï¼‰

        Args:
            model_id: æ¨¡å‹ID (tiny, base, small, medium, large-v2, large-v3)

        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯åŠ¨ä¸‹è½½
        """
        try:
            from services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()
            success = model_mgr.download_whisper_model(model_id)

            if success:
                self.logger.info(f"âœ… å·²å§”æ‰˜æ¨¡å‹ç®¡ç†æœåŠ¡ä¸‹è½½Whisperæ¨¡å‹: {model_id}")
            return success

        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è½½Whisperæ¨¡å‹å¤±è´¥: {model_id} - {e}")
            return False

    def download_align_model(self, language: str) -> bool:
        """
        ä¸‹è½½å•ä¸ªå¯¹é½æ¨¡å‹ï¼ˆå§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡ï¼‰

        Args:
            language: è¯­è¨€ä»£ç 

        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯åŠ¨ä¸‹è½½
        """
        try:
            from services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()
            success = model_mgr.download_align_model(language)

            if success:
                self.logger.info(f"âœ… å·²å§”æ‰˜æ¨¡å‹ç®¡ç†æœåŠ¡ä¸‹è½½å¯¹é½æ¨¡å‹: {language}")
            return success

        except Exception as e:
            self.logger.error(f"âŒ ä¸‹è½½å¯¹é½æ¨¡å‹å¤±è´¥: {language} - {e}")
            return False

    def delete_whisper_model(self, model_id: str) -> bool:
        """
        åˆ é™¤Whisperæ¨¡å‹ï¼ˆå§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡ï¼Œå¹¶æ¸…ç†ç¼“å­˜ï¼‰

        Args:
            model_id: æ¨¡å‹ID

        Returns:
            bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            from services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()

            # å…ˆä»ç¼“å­˜ä¸­ç§»é™¤
            with self._global_lock:
                keys_to_remove = [k for k in self._whisper_cache.keys() if k[0] == model_id]
                for key in keys_to_remove:
                    info = self._whisper_cache.pop(key)
                    del info.model
                    self.logger.debug(f"ğŸ—‘ï¸ ä»ç¼“å­˜ä¸­ç§»é™¤æ¨¡å‹: {key}")

                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())

            # æ¸…ç†GPUå†…å­˜
            if keys_to_remove:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            # å§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡åˆ é™¤ç£ç›˜æ–‡ä»¶
            success = model_mgr.delete_whisper_model(model_id)

            if success:
                self.logger.info(f"âœ… å·²åˆ é™¤Whisperæ¨¡å‹: {model_id}")
            return success

        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤Whisperæ¨¡å‹å¤±è´¥: {model_id} - {e}")
            return False

    def delete_align_model(self, language: str) -> bool:
        """
        åˆ é™¤å¯¹é½æ¨¡å‹ï¼ˆå§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡ï¼Œå¹¶æ¸…ç†ç¼“å­˜ï¼‰

        Args:
            language: è¯­è¨€ä»£ç 

        Returns:
            bool: æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            from services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()

            # å…ˆä»ç¼“å­˜ä¸­ç§»é™¤
            with self._global_lock:
                if language in self._align_cache:
                    del self._align_cache[language]
                    self.logger.debug(f"ğŸ—‘ï¸ ä»ç¼“å­˜ä¸­ç§»é™¤å¯¹é½æ¨¡å‹: {language}")

                    # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                    self._preload_status["cache_version"] = int(time.time())

            # å§”æ‰˜ç»™æ¨¡å‹ç®¡ç†æœåŠ¡åˆ é™¤ç£ç›˜æ–‡ä»¶
            success = model_mgr.delete_align_model(language)

            if success:
                self.logger.info(f"âœ… å·²åˆ é™¤å¯¹é½æ¨¡å‹: {language}")
            return success

        except Exception as e:
            self.logger.error(f"âŒ åˆ é™¤å¯¹é½æ¨¡å‹å¤±è´¥: {language} - {e}")
            return False

    def list_all_models(self) -> Dict[str, Any]:
        """
        åˆ—å‡ºæ‰€æœ‰æ¨¡å‹çš„çŠ¶æ€ï¼ˆæ•´åˆç£ç›˜çŠ¶æ€å’Œç¼“å­˜çŠ¶æ€ï¼‰

        Returns:
            Dict: åŒ…å«whisperå’Œalignæ¨¡å‹çš„çŠ¶æ€ä¿¡æ¯
        """
        try:
            from services.model_manager_service import get_model_manager
            model_mgr = get_model_manager()

            # è·å–ç£ç›˜ä¸Šçš„æ¨¡å‹çŠ¶æ€
            whisper_models = [
                {
                    "model_id": m.model_id,
                    "size_mb": m.size_mb,
                    "status": m.status,
                    "download_progress": m.download_progress,
                    "local_path": m.local_path,
                    "description": m.description,
                    "cached": any(k[0] == m.model_id for k in self._whisper_cache.keys())
                }
                for m in model_mgr.list_whisper_models()
            ]

            align_models = [
                {
                    "language": m.language,
                    "language_name": m.language_name,
                    "status": m.status,
                    "download_progress": m.download_progress,
                    "local_path": m.local_path,
                    "cached": m.language in self._align_cache
                }
                for m in model_mgr.list_align_models()
            ]

            return {
                "whisper_models": whisper_models,
                "align_models": align_models,
                "cache_info": {
                    "whisper_cached": len(self._whisper_cache),
                    "align_cached": len(self._align_cache),
                    "total_memory_mb": sum(info.memory_size for info in self._whisper_cache.values())
                }
            }

        except Exception as e:
            self.logger.error(f"âŒ åˆ—å‡ºæ¨¡å‹å¤±è´¥: {e}")
            return {"error": str(e)}


class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def get_memory_info(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä¿¡æ¯"""
        try:
            # ç³»ç»Ÿå†…å­˜
            memory = psutil.virtual_memory()

            # GPUå†…å­˜ (å¦‚æœå¯ç”¨)
            gpu_info = {}
            if torch.cuda.is_available():
                gpu_info = {
                    "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / (1024**3),  # GB
                    "gpu_memory_allocated": torch.cuda.memory_allocated() / (1024**3),  # GB
                    "gpu_memory_cached": torch.cuda.memory_reserved() / (1024**3),  # GB
                }

            return {
                "system_memory_total": memory.total / (1024**3),  # GB
                "system_memory_used": memory.used / (1024**3),  # GB
                "system_memory_percent": memory.percent,
                **gpu_info
            }
        except Exception as e:
            self.logger.error(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def check_memory_available(self, threshold: float = 0.85) -> bool:
        """æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³"""
        try:
            memory = psutil.virtual_memory()
            return memory.percent < (threshold * 100)
        except:
            return True  # é»˜è®¤è®¤ä¸ºå†…å­˜å……è¶³


# ========== å…¨å±€å•ä¾‹æ¨¡å¼ - æä¾›ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨æ¥å£ ==========

_model_manager: Optional[ModelPreloadManager] = None


def initialize_model_manager(config: PreloadConfig = None) -> ModelPreloadManager:
    """
    åˆå§‹åŒ–å…¨å±€æ¨¡å‹ç®¡ç†å™¨

    Args:
        config: é¢„åŠ è½½é…ç½®

    Returns:
        ModelPreloadManager: æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
    """
    global _model_manager
    if _model_manager is None:
        _model_manager = ModelPreloadManager(config)
        logging.getLogger(__name__).info("ğŸ—ï¸ å…¨å±€æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨å·²åˆå§‹åŒ–")
    return _model_manager


def get_model_manager() -> Optional[ModelPreloadManager]:
    """
    è·å–å…¨å±€æ¨¡å‹ç®¡ç†å™¨

    Returns:
        Optional[ModelPreloadManager]: æ¨¡å‹ç®¡ç†å™¨å®ä¾‹ï¼Œæœªåˆå§‹åŒ–åˆ™è¿”å›None
    """
    return _model_manager


async def preload_default_models(progress_callback=None) -> Dict[str, Any]:
    """
    é¢„åŠ è½½é»˜è®¤æ¨¡å‹

    Args:
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        Dict: é¢„åŠ è½½ç»“æœ
    """
    if _model_manager is None:
        return {"success": False, "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return await _model_manager.preload_models(progress_callback)


def get_preload_status() -> Dict[str, Any]:
    """
    è·å–é¢„åŠ è½½çŠ¶æ€

    Returns:
        Dict: é¢„åŠ è½½çŠ¶æ€ä¿¡æ¯
    """
    if _model_manager is None:
        return {"is_preloading": False, "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return _model_manager.get_preload_status()


def get_cache_status() -> Dict[str, Any]:
    """
    è·å–ç¼“å­˜çŠ¶æ€

    Returns:
        Dict: ç¼“å­˜çŠ¶æ€ä¿¡æ¯
    """
    if _model_manager is None:
        return {"message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return _model_manager.get_cache_status()
