"""
æ¨¡å‹é¢„åŠ è½½å’Œç¼“å­˜ç®¡ç†å™¨
å®ç°æ¨¡å‹é¢„åŠ è½½ã€LRUç¼“å­˜ã€å†…å­˜ç›‘æ§ç­‰åŠŸèƒ½
"""

import os
import gc
import logging
import threading
import time
import asyncio
from collections import OrderedDict
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any
import psutil
import torch
import whisperx

# ä¿®å¤å¯¼å…¥è·¯å¾„
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from models.job_models import JobSettings


@dataclass
class ModelCacheInfo:
    """æ¨¡å‹ç¼“å­˜ä¿¡æ¯"""
    model: Any
    key: Tuple[str, str, str]
    load_time: float
    last_used: float
    memory_size: int  # ä¼°ç®—çš„å†…å­˜å ç”¨(MB)


@dataclass
class PreloadConfig:
    """é¢„åŠ è½½é…ç½®"""
    enabled: bool = True
    default_models: List[str] = None
    max_cache_size: int = 3  # æœ€å¤§ç¼“å­˜æ¨¡å‹æ•°é‡
    memory_threshold: float = 0.8  # å†…å­˜ä½¿ç”¨é˜ˆå€¼(80%)
    preload_timeout: int = 300  # é¢„åŠ è½½è¶…æ—¶æ—¶é—´(ç§’)
    warmup_enabled: bool = True  # æ˜¯å¦å¯ç”¨é¢„çƒ­

    def __post_init__(self):
        if self.default_models is None:
            self.default_models = ["medium"]


class ModelPreloadManager:
    """ç®€åŒ–ç‰ˆæ¨¡å‹é¢„åŠ è½½å’Œç¼“å­˜ç®¡ç†å™¨ - æ–¹æ¡ˆäºŒå®ç°
    
    æ ¸å¿ƒæ”¹è¿›:
    1. ç»Ÿä¸€é”æœºåˆ¶é¿å…æ­»é”
    2. å¹‚ç­‰æ€§é¢„åŠ è½½é¿å…é‡å¤æ‰§è¡Œ
    3. ç¼“å­˜ç‰ˆæœ¬å·ç¡®ä¿çŠ¶æ€åŒæ­¥
    4. æ ‡å‡†åŒ–æ—¥å¿—ä¾¿äºè°ƒè¯•
    """
    
    def __init__(self, config: PreloadConfig = None):
        self.config = config or PreloadConfig()
        self.logger = self._setup_logger()
        
        # æ¨¡å‹ç¼“å­˜ (LRU)
        self._whisper_cache: OrderedDict[Tuple[str, str, str], ModelCacheInfo] = OrderedDict()
        self._align_cache: OrderedDict[str, Tuple[Any, Any, float]] = OrderedDict()
        
        # ç»Ÿä¸€é” - ç®€åŒ–å¹¶å‘æ§åˆ¶ï¼Œé¿å…å¤šé”æ­»é”
        self._global_lock = threading.RLock()
        
        # ç®€åŒ–çš„é¢„åŠ è½½çŠ¶æ€ - å•ä¸€æ•°æ®æº
        self._preload_status = {
            "is_preloading": False,
            "progress": 0.0,
            "current_model": "",
            "total_models": 0,
            "loaded_models": 0,
            "errors": [],
            "failed_attempts": 0,
            "last_attempt_time": 0,
            "max_retry_attempts": 3,
            "retry_cooldown": 30,
            "cache_version": int(time.time())  # ç¼“å­˜ç‰ˆæœ¬å·ï¼Œç”¨äºçŠ¶æ€åŒæ­¥
        }
        
        # é¢„åŠ è½½ä»»åŠ¡ç®¡ç† - å®ç°å¹‚ç­‰æ€§
        self._preload_promise: Optional[asyncio.Task] = None
        
        # å†…å­˜ç›‘æ§
        self._memory_monitor = MemoryMonitor()
        
        self.logger.info("ğŸ—ï¸ ModelPreloadManageråˆå§‹åŒ–å®Œæˆ - ç®€åŒ–æ¶æ„")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ ‡å‡†åŒ–çš„æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(f"{__name__}.ModelPreloadManager")
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - ğŸ¤–[æ¨¡å‹ç®¡ç†] - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
        
    def get_preload_status(self) -> Dict[str, Any]:
        """è·å–é¢„åŠ è½½çŠ¶æ€ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            status = self._preload_status.copy()
            self.logger.debug(f"ğŸ“Š çŠ¶æ€æŸ¥è¯¢: é¢„åŠ è½½={status['is_preloading']}, è¿›åº¦={status['progress']:.1f}%, å·²åŠ è½½={status['loaded_models']}")
            return status
    
    def get_cache_status(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜çŠ¶æ€ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            whisper_models = [
                {
                    "key": info.key,
                    "memory_mb": info.memory_size,
                    "last_used": info.last_used,
                    "load_time": info.load_time
                }
                for info in self._whisper_cache.values()
            ]
            
            align_models = list(self._align_cache.keys())
            total_memory = sum(info.memory_size for info in self._whisper_cache.values())
            
            cache_status = {
                "whisper_models": whisper_models,
                "align_models": align_models,
                "total_memory_mb": total_memory,
                "max_cache_size": self.config.max_cache_size,
                "memory_info": self._memory_monitor.get_memory_info(),
                "cache_version": self._preload_status["cache_version"]
            }
            
            self.logger.debug(f"ğŸ’¾ ç¼“å­˜æŸ¥è¯¢: Whisperæ¨¡å‹={len(whisper_models)}ä¸ª, å¯¹é½æ¨¡å‹={len(align_models)}ä¸ª, å†…å­˜={total_memory}MB")
            return cache_status
    
    async def preload_models(self, progress_callback=None) -> Dict[str, Any]:
        """é¢„åŠ è½½é»˜è®¤æ¨¡å‹ - ç®€åŒ–ç‰ˆå®ç°ï¼Œå¸¦å¹‚ç­‰æ€§ä¿è¯"""
        with self._global_lock:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨é¢„åŠ è½½ä¸­ï¼ˆå¹‚ç­‰æ€§æ£€æŸ¥ï¼‰
            if self._preload_status["is_preloading"]:
                self.logger.info("âš¡ é¢„åŠ è½½å·²åœ¨è¿›è¡Œä¸­ï¼Œè¿”å›å·²æœ‰ä»»åŠ¡")
                return {"success": True, "message": "é¢„åŠ è½½å·²åœ¨è¿›è¡Œä¸­"}
            
            if not self.config.enabled:
                self.logger.warning("âš ï¸ æ¨¡å‹é¢„åŠ è½½åŠŸèƒ½å·²ç¦ç”¨")
                return {"success": False, "message": "é¢„åŠ è½½åŠŸèƒ½å·²ç¦ç”¨"}

            # è®¾ç½®é¢„åŠ è½½çŠ¶æ€
            self._preload_status.update({
                "is_preloading": True,
                "progress": 0.0,
                "current_model": "",
                "total_models": len(self.config.default_models),
                "loaded_models": 0,
                "errors": [],
                "last_attempt_time": time.time()
            })
            
            self.logger.info(f"ğŸš€ å¼€å§‹é¢„åŠ è½½ä»»åŠ¡: {self.config.default_models}")

        try:
            success_count = 0
            total_models = len(self.config.default_models)
            
            for i, model_name in enumerate(self.config.default_models):
                try:
                    # æ›´æ–°å½“å‰è¿›åº¦
                    with self._global_lock:
                        self._preload_status.update({
                            "current_model": model_name,
                            "progress": (i / total_models) * 100
                        })
                    
                    self.logger.info(f"ğŸ”„ [{i+1}/{total_models}] å¤„ç†æ¨¡å‹: {model_name}")
                    
                    # æ£€æŸ¥å†…å­˜
                    if not self._memory_monitor.check_memory_available():
                        error_msg = f"å†…å­˜ä¸è¶³ï¼Œè·³è¿‡æ¨¡å‹ {model_name}"
                        self.logger.warning(f"âš ï¸ {error_msg}")
                        with self._global_lock:
                            self._preload_status["errors"].append(error_msg)
                        continue
                    
                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç¼“å­˜
                    device = "cuda" if torch.cuda.is_available() else "cpu"
                    key = (model_name, "float16", device)
                    
                    with self._global_lock:
                        if key in self._whisper_cache:
                            self.logger.info(f"âœ… æ¨¡å‹ {model_name} å·²åœ¨ç¼“å­˜ä¸­")
                            self._preload_status["loaded_models"] += 1
                            success_count += 1
                            continue
                    
                    # åŠ è½½æ–°æ¨¡å‹
                    settings = JobSettings(
                        model=model_name,
                        compute_type="float16",
                        device=device
                    )
                    
                    self.logger.info(f"ğŸ” å¼€å§‹åŠ è½½æ¨¡å‹: {model_name} (device={device})")
                    start_time = time.time()
                    
                    model = self._load_whisper_model(settings)
                    
                    load_time = time.time() - start_time
                    
                    # é¢„çƒ­æ¨¡å‹
                    if self.config.warmup_enabled:
                        self.logger.info(f"ğŸ”¥ é¢„çƒ­æ¨¡å‹: {model_name}")
                        self._warmup_model(model)
                    
                    with self._global_lock:
                        self._preload_status["loaded_models"] += 1
                    success_count += 1
                    
                    self.logger.info(f"âœ… æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ (è€—æ—¶: {load_time:.2f}s)")
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        with self._global_lock:
                            progress_callback(self._preload_status.copy())
                    
                except Exception as e:
                    error_msg = f"åŠ è½½æ¨¡å‹ {model_name} å¤±è´¥: {str(e)}"
                    self.logger.error(f"âŒ {error_msg}", exc_info=True)
                    with self._global_lock:
                        self._preload_status["errors"].append(error_msg)
            
            # å®Œæˆé¢„åŠ è½½
            success = success_count > 0
            
            with self._global_lock:
                # æ›´æ–°å¤±è´¥è®¡æ•°
                if not success:
                    self._preload_status["failed_attempts"] += 1
                else:
                    self._preload_status["failed_attempts"] = 0  # æˆåŠŸåé‡ç½®
                
                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())
                
                # ç»“æŸé¢„åŠ è½½çŠ¶æ€
                self._preload_status.update({
                    "is_preloading": False,
                    "progress": 100.0,
                    "current_model": ""
                })
                
                result = {
                    "success": success,
                    "loaded_models": self._preload_status["loaded_models"],
                    "total_models": self._preload_status["total_models"],
                    "errors": self._preload_status["errors"].copy(),
                    "failed_attempts": self._preload_status["failed_attempts"],
                    "cache_version": self._preload_status["cache_version"]
                }
            
            # æ—¥å¿—è¾“å‡º
            if success:
                self.logger.info(f"âœ… é¢„åŠ è½½ä»»åŠ¡æˆåŠŸå®Œæˆ: {success_count}/{total_models} ä¸ªæ¨¡å‹")
            else:
                self.logger.warning(f"âš ï¸ é¢„åŠ è½½ä»»åŠ¡å®Œæˆä½†æ— æˆåŠŸåŠ è½½çš„æ¨¡å‹")
                
            if result["errors"]:
                self.logger.warning(f"âš ï¸ é¢„åŠ è½½è¿‡ç¨‹ä¸­å‡ºç° {len(result['errors'])} ä¸ªé”™è¯¯")
            
            # è°ƒç”¨æœ€ç»ˆè¿›åº¦å›è°ƒ
            if progress_callback:
                progress_callback(self._preload_status.copy())
            
            return result
            
        except Exception as e:
            with self._global_lock:
                self._preload_status["is_preloading"] = False
                self._preload_status["failed_attempts"] += 1
            
            self.logger.error(f"âŒ é¢„åŠ è½½è¿‡ç¨‹å¼‚å¸¸: {str(e)}", exc_info=True)
            return {
                "success": False, 
                "message": f"é¢„åŠ è½½å¤±è´¥: {str(e)}", 
                "failed_attempts": self._preload_status["failed_attempts"]
            }

    def reset_preload_attempts(self):
        """é‡ç½®é¢„åŠ è½½å¤±è´¥è®¡æ•° - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        with self._global_lock:
            old_attempts = self._preload_status["failed_attempts"]
            self._preload_status["failed_attempts"] = 0
            self._preload_status["last_attempt_time"] = 0
            self._preload_status["cache_version"] = int(time.time())
            
        self.logger.info(f"ğŸ”„ é¢„åŠ è½½å¤±è´¥è®¡æ•°å·²é‡ç½®: {old_attempts} -> 0")
    
    def get_model(self, settings: JobSettings):
        """è·å–Whisperæ¨¡å‹ (å¸¦LRUç¼“å­˜) - ç®€åŒ–ç‰ˆæœ¬"""
        key = (settings.model, settings.compute_type, settings.device)
        
        with self._global_lock:
            # å‘½ä¸­ç¼“å­˜
            if key in self._whisper_cache:
                info = self._whisper_cache[key]
                info.last_used = time.time()
                # ç§»åˆ°æœ€å (æœ€è¿‘ä½¿ç”¨)
                self._whisper_cache.move_to_end(key)
                self.logger.debug(f"âœ… å‘½ä¸­æ¨¡å‹ç¼“å­˜: {key}")
                return info.model
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½æ–°æ¨¡å‹
            self.logger.info(f"ğŸ”„ éœ€è¦åŠ è½½æ–°æ¨¡å‹: {key}")
            return self._load_whisper_model(settings)
    
    def _load_whisper_model(self, settings: JobSettings):
        """åŠ è½½Whisperæ¨¡å‹ - ç®€åŒ–ç‰ˆæœ¬å¸¦å¹¶å‘ä¿æŠ¤"""
        key = (settings.model, settings.compute_type, settings.device)
        
        # å†æ¬¡æ£€æŸ¥ç¼“å­˜ï¼ˆé¿å…å¹¶å‘åŠ è½½åŒä¸€æ¨¡å‹ï¼‰
        with self._global_lock:
            if key in self._whisper_cache:
                info = self._whisper_cache[key]
                info.last_used = time.time()
                self._whisper_cache.move_to_end(key)
                self.logger.debug(f"âš¡ å¹¶å‘æ£€æŸ¥å‘½ä¸­ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½: {key}")
                return info.model
        
        self.logger.info(f"ğŸ” å¼€å§‹åŠ è½½æ–°Whisperæ¨¡å‹: {key}")
        
        # æ£€æŸ¥å†…å­˜
        if not self._memory_monitor.check_memory_available():
            self.logger.warning("âš ï¸ å†…å­˜ä¸è¶³ï¼Œå°è¯•æ¸…ç†ç¼“å­˜")
            self._cleanup_old_models()
        
        # æ£€æŸ¥ç¼“å­˜å¤§å°
        with self._global_lock:
            if len(self._whisper_cache) >= self.config.max_cache_size:
                self._evict_lru_model()
        
        try:
            start_time = time.time()
            self.logger.info(f"ğŸš€ æ­£åœ¨ä»ç£ç›˜åŠ è½½æ¨¡å‹ {settings.model} (device={settings.device}, compute_type={settings.compute_type})")
            
            model = whisperx.load_model(
                settings.model, 
                settings.device, 
                compute_type=settings.compute_type
            )
            load_time = time.time() - start_time
            
            # ä¼°ç®—å†…å­˜ä½¿ç”¨
            memory_size = self._estimate_model_memory(model)
            
            # æ·»åŠ åˆ°ç¼“å­˜
            info = ModelCacheInfo(
                model=model,
                key=key,
                load_time=load_time,
                last_used=time.time(),
                memory_size=memory_size
            )
            
            with self._global_lock:
                self._whisper_cache[key] = info
                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())
            
            self.logger.info(f"âœ… æˆåŠŸåŠ è½½å¹¶ç¼“å­˜Whisperæ¨¡å‹ {key} (å†…å­˜: {memory_size}MB, è€—æ—¶: {load_time:.2f}s)")
            return model
            
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½Whisperæ¨¡å‹å¤±è´¥ {key}: {str(e)}", exc_info=True)
            raise
    
    def get_align_model(self, lang: str, device: str):
        """è·å–å¯¹é½æ¨¡å‹ (å¸¦LRUç¼“å­˜) - ç®€åŒ–ç‰ˆæœ¬"""
        with self._global_lock:
            # å‘½ä¸­ç¼“å­˜
            if lang in self._align_cache:
                model, meta, last_used = self._align_cache[lang]
                # æ›´æ–°ä½¿ç”¨æ—¶é—´å¹¶ç§»åˆ°æœ€å
                self._align_cache[lang] = (model, meta, time.time())
                self._align_cache.move_to_end(lang)
                self.logger.debug(f"âœ… å‘½ä¸­å¯¹é½æ¨¡å‹ç¼“å­˜: {lang}")
                return model, meta
            
            # ç¼“å­˜æœªå‘½ä¸­ï¼ŒåŠ è½½æ–°æ¨¡å‹
            self.logger.info(f"ğŸ”„ åŠ è½½æ–°å¯¹é½æ¨¡å‹: {lang}")
            try:
                model, meta = whisperx.load_align_model(language_code=lang, device=device)
                
                # æ·»åŠ åˆ°ç¼“å­˜ (é™åˆ¶å¤§å°)
                if len(self._align_cache) >= 5:  # å¯¹é½æ¨¡å‹ç¼“å­˜ä¸Šé™
                    # ç§»é™¤æœ€æ—§çš„
                    oldest = next(iter(self._align_cache))
                    del self._align_cache[oldest]
                    self.logger.debug(f"ğŸ—‘ï¸ ç§»é™¤æœ€æ—§å¯¹é½æ¨¡å‹: {oldest}")
                
                self._align_cache[lang] = (model, meta, time.time())
                
                # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
                self._preload_status["cache_version"] = int(time.time())
                
                self.logger.info(f"âœ… æˆåŠŸåŠ è½½å¯¹é½æ¨¡å‹: {lang}")
                return model, meta
                
            except Exception as e:
                self.logger.error(f"âŒ åŠ è½½å¯¹é½æ¨¡å‹å¤±è´¥ {lang}: {str(e)}", exc_info=True)
                raise
    
    def _warmup_model(self, model):
        """é¢„çƒ­æ¨¡å‹ - ç©ºè·‘ä¸€æ¬¡ç¡®ä¿å®Œå…¨åŠ è½½"""
        try:
            self.logger.debug("å¼€å§‹æ¨¡å‹é¢„çƒ­")
            
            # åˆ›å»ºè™šæ‹ŸéŸ³é¢‘æ•°æ® (1ç§’é™éŸ³)
            import numpy as np
            dummy_audio = np.zeros(16000, dtype=np.float32)  # 16kHz 1ç§’
            
            # ç©ºè·‘ä¸€æ¬¡
            _ = model.transcribe(dummy_audio, batch_size=1, verbose=False)
            
            self.logger.debug("æ¨¡å‹é¢„çƒ­å®Œæˆ")
            
        except Exception as e:
            self.logger.warning(f"æ¨¡å‹é¢„çƒ­å¤±è´¥: {str(e)}")
    
    def _evict_lru_model(self):
        """é©±é€æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹ - éœ€è¦åœ¨é”å†…è°ƒç”¨"""
        if not self._whisper_cache:
            return
        
        # æœ€ä¹…æœªä½¿ç”¨çš„åœ¨å¼€å¤´
        oldest_key = next(iter(self._whisper_cache))
        info = self._whisper_cache.pop(oldest_key)
        
        self.logger.info(f"ğŸ—‘ï¸ é©±é€LRUæ¨¡å‹: {oldest_key}, é‡Šæ”¾å†…å­˜: {info.memory_size}MB")
        
        # é‡Šæ”¾å†…å­˜
        del info.model
        del info
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def _cleanup_old_models(self):
        """æ¸…ç†æ—§æ¨¡å‹é‡Šæ”¾å†…å­˜ - éœ€è¦åœ¨é”å¤–è°ƒç”¨"""
        current_time = time.time()
        to_remove = []
        
        with self._global_lock:
            for key, info in self._whisper_cache.items():
                # è¶…è¿‡10åˆ†é’Ÿæœªä½¿ç”¨çš„æ¨¡å‹
                if current_time - info.last_used > 600:
                    to_remove.append(key)
            
            for key in to_remove:
                info = self._whisper_cache.pop(key)
                self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ—§æ¨¡å‹: {key}")
                del info.model
                del info
        
        if to_remove:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            self.logger.info(f"ğŸ’« æ¸…ç†äº† {len(to_remove)} ä¸ªæ—§æ¨¡å‹")
    
    def _estimate_model_memory(self, model) -> int:
        """ä¼°ç®—æ¨¡å‹å†…å­˜ä½¿ç”¨ (MB)"""
        try:
            # ç®€å•ä¼°ç®—ï¼ŒåŸºäºæ¨¡å‹å‚æ•°
            if hasattr(model, 'model') and hasattr(model.model, 'parameters'):
                total_params = sum(p.numel() for p in model.model.parameters())
                # å‡è®¾æ¯ä¸ªå‚æ•°4å­—èŠ‚ (float32) æˆ– 2å­—èŠ‚ (float16)
                bytes_per_param = 2  # float16
                total_bytes = total_params * bytes_per_param
                return int(total_bytes / (1024 * 1024))  # è½¬æ¢ä¸ºMB
        except:
            pass
        
        # é»˜è®¤ä¼°ç®—å€¼
        return 500  # é»˜è®¤500MB
    
    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç«‹å³åŒæ­¥çŠ¶æ€"""
        with self._global_lock:
            # è®°å½•æ¸…ç†å‰çš„ç¼“å­˜çŠ¶æ€
            whisper_count = len(self._whisper_cache)
            align_count = len(self._align_cache)
            total_memory = sum(info.memory_size for info in self._whisper_cache.values())
            
            # æ¸…ç†Whisperæ¨¡å‹ç¼“å­˜
            for info in self._whisper_cache.values():
                del info.model
            self._whisper_cache.clear()
            
            # æ¸…ç†å¯¹é½æ¨¡å‹ç¼“å­˜
            self._align_cache.clear()
            
            # ç«‹å³æ›´æ–°é¢„åŠ è½½çŠ¶æ€ - è§£å†³çŠ¶æ€åŒæ­¥é—®é¢˜
            self._preload_status.update({
                "loaded_models": 0,
                "is_preloading": False,
                "progress": 0.0,
                "current_model": "",
                "errors": [],
                "cache_version": int(time.time())  # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
            })
            
        # åƒåœ¾å›æ”¶å’ŒGPUå†…å­˜æ¸…ç†
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        self.logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰æ¨¡å‹ç¼“å­˜: Whisper={whisper_count}ä¸ª, å¯¹é½={align_count}ä¸ª, é‡Šæ”¾å†…å­˜={total_memory}MB")


class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def get_memory_info(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä¿¡æ¯"""
        try:
            # ç³»ç»Ÿå†…å­˜
            memory = psutil.virtual_memory()

            # GPUå†…å­˜ (å¦‚æœå¯ç”¨)
            gpu_info = {}
            if torch.cuda.is_available():
                gpu_info = {
                    "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / (1024**3),  # GB
                    "gpu_memory_allocated": torch.cuda.memory_allocated() / (1024**3),  # GB
                    "gpu_memory_cached": torch.cuda.memory_reserved() / (1024**3),  # GB
                }

            return {
                "system_memory_total": memory.total / (1024**3),  # GB
                "system_memory_used": memory.used / (1024**3),  # GB
                "system_memory_percent": memory.percent,
                **gpu_info
            }
        except Exception as e:
            self.logger.error(f"è·å–å†…å­˜ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}

    def check_memory_available(self, threshold: float = 0.85) -> bool:
        """æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³"""
        try:
            memory = psutil.virtual_memory()
            return memory.percent < (threshold * 100)
        except:
            return True  # é»˜è®¤è®¤ä¸ºå†…å­˜å……è¶³


# ========== å…¨å±€å•ä¾‹æ¨¡å¼ - æä¾›ç»Ÿä¸€çš„æ¨¡å‹ç®¡ç†å™¨æ¥å£ ==========

_model_manager: Optional[ModelPreloadManager] = None


def initialize_model_manager(config: PreloadConfig = None) -> ModelPreloadManager:
    """
    åˆå§‹åŒ–å…¨å±€æ¨¡å‹ç®¡ç†å™¨

    Args:
        config: é¢„åŠ è½½é…ç½®

    Returns:
        ModelPreloadManager: æ¨¡å‹ç®¡ç†å™¨å®ä¾‹
    """
    global _model_manager
    if _model_manager is None:
        _model_manager = ModelPreloadManager(config)
        logging.getLogger(__name__).info("ğŸ—ï¸ å…¨å±€æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨å·²åˆå§‹åŒ–")
    return _model_manager


def get_model_manager() -> Optional[ModelPreloadManager]:
    """
    è·å–å…¨å±€æ¨¡å‹ç®¡ç†å™¨

    Returns:
        Optional[ModelPreloadManager]: æ¨¡å‹ç®¡ç†å™¨å®ä¾‹ï¼Œæœªåˆå§‹åŒ–åˆ™è¿”å›None
    """
    return _model_manager


async def preload_default_models(progress_callback=None) -> Dict[str, Any]:
    """
    é¢„åŠ è½½é»˜è®¤æ¨¡å‹

    Args:
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

    Returns:
        Dict: é¢„åŠ è½½ç»“æœ
    """
    if _model_manager is None:
        return {"success": False, "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return await _model_manager.preload_models(progress_callback)


def get_preload_status() -> Dict[str, Any]:
    """
    è·å–é¢„åŠ è½½çŠ¶æ€

    Returns:
        Dict: é¢„åŠ è½½çŠ¶æ€ä¿¡æ¯
    """
    if _model_manager is None:
        return {"is_preloading": False, "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return _model_manager.get_preload_status()


def get_cache_status() -> Dict[str, Any]:
    """
    è·å–ç¼“å­˜çŠ¶æ€

    Returns:
        Dict: ç¼“å­˜çŠ¶æ€ä¿¡æ¯
    """
    if _model_manager is None:
        return {"message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

    return _model_manager.get_cache_status()
