import os
import sys
import uuid
import shutil
import logging
import asyncio
import time
from fastapi import FastAPI, Form, HTTPException, UploadFile, File
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import json
from typing import Optional, List
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥æ ¸å¿ƒé…ç½®å’Œæ—¥å¿—
from core.config import config
from core.logging import setup_logging

# å¯¼å…¥æ–°çš„è½¬å½•æœåŠ¡ï¼ˆæ›¿æ¢processorï¼‰
from services.transcription_service import get_transcription_service
from models.job_models import JobSettings
from services.cpu_affinity_service import CPUAffinityConfig
from services.model_preload_manager import (
    PreloadConfig,
    get_model_manager,
    initialize_model_manager,
    preload_default_models,
    get_preload_status,
    get_cache_status
)
from config.model_config import ModelPreloadConfig

# å¯¼å…¥APIè·¯ç”±
from api.routes import model_routes
from api.routes.transcription_routes import create_transcription_router
from services.file_service import FileManagementService

# å¯¼å…¥FFmpegç®¡ç†å™¨
from services.ffmpeg_manager import get_ffmpeg_manager

# é…ç½®æ—¥å¿—ï¼ˆåœ¨å…¶ä»–åˆå§‹åŒ–ä¹‹å‰ï¼‰
logger = setup_logging()

app = FastAPI(title="Video To SRT API", version="0.3.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†ŒAPIè·¯ç”±
app.include_router(model_routes.router)

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶ - åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨å’ŒFFmpegæ£€æµ‹"""
    try:
        logger.info("="  * 60)
        logger.info("æœåŠ¡å¯åŠ¨ä¸­...")
        logger.info("=" * 60)

        # 1. è®¾ç½®SSEäº‹ä»¶å¾ªç¯å¼•ç”¨ï¼ˆå¿…é¡»åœ¨æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–ä¹‹å‰ï¼ï¼‰
        logger.info("æ­¥éª¤ 1/3: è®¾ç½®SSEäº‹ä»¶å¾ªç¯...")
        try:
            from api.routes.model_routes import set_event_loop
            if set_event_loop():
                logger.info("âœ… SSEäº‹ä»¶å¾ªç¯å·²è®¾ç½®")
            else:
                logger.warning("âš ï¸ SSEäº‹ä»¶å¾ªç¯è®¾ç½®å¤±è´¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æœºåˆ¶")
        except Exception as e:
            logger.warning(f"è®¾ç½®SSEäº‹ä»¶å¾ªç¯å¼‚å¸¸: {e}")

        # 2. FFmpegæ£€æµ‹å’Œè‡ªåŠ¨ä¸‹è½½
        logger.info("æ­¥éª¤ 2/3: æ£€æµ‹FFmpeg...")
        ffmpeg_mgr = get_ffmpeg_manager()
        try:
            ffmpeg_path = ffmpeg_mgr.ensure_ffmpeg()
            logger.info(f"FFmpegæ£€æµ‹å®Œæˆ: {ffmpeg_path}")
        except RuntimeError as e:
            # FFmpegä¸å¯ç”¨ä½†ä¸é˜»æ­¢å¯åŠ¨ï¼Œåªæ˜¯è®°å½•è­¦å‘Š
            logger.warning(f"FFmpegæ£€æµ‹å¤±è´¥: {e}")
            logger.warning("è½¬å½•åŠŸèƒ½å¯èƒ½æ— æ³•ä½¿ç”¨ï¼Œè¯·æ‰‹åŠ¨å®‰è£…FFmpeg")

        # 3. åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨ï¼ˆæ­¤æ—¶äº‹ä»¶å¾ªç¯å·²è®¾ç½®ï¼Œåå°éªŒè¯å¯ä»¥æ­£å¸¸æ¨é€SSEï¼‰
        logger.info("æ­¥éª¤ 3/3: åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨...")
        model_manager = initialize_model_manager(preload_config)
        logger.info("æ¨¡å‹ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        # ä¸åœ¨å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹ï¼Œç­‰å¾…å‰ç«¯å°±ç»ªåé€šè¿‡APIè°ƒç”¨
        logger.info("åç«¯æœåŠ¡å·²å°±ç»ªï¼Œç­‰å¾…å‰ç«¯å¯åŠ¨åè¿›è¡Œæ¨¡å‹é¢„åŠ è½½")

        logger.info("=" * 60)
        logger.info("æœåŠ¡å¯åŠ¨å®Œæˆ")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"å¯åŠ¨åˆå§‹åŒ–å¤±è´¥: {str(e)}", exc_info=True)


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶ - æ¸…ç†èµ„æº"""
    try:
        model_manager = get_model_manager()
        if model_manager:
            model_manager.clear_cache()
            logger.info("å·²æ¸…ç†æ¨¡å‹ç¼“å­˜")
    except Exception as e:
        logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {str(e)}")

# ä½¿ç”¨ç»Ÿä¸€é…ç½®ä¸­çš„ç›®å½•
INPUT_DIR = str(config.INPUT_DIR)
OUTPUT_DIR = str(config.OUTPUT_DIR)
JOBS_DIR = str(config.JOBS_DIR)
TEMP_DIR = str(config.TEMP_DIR)

# åˆå§‹åŒ–è½¬å½•æœåŠ¡
transcription_service = get_transcription_service(JOBS_DIR)

# åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†æœåŠ¡
file_service = FileManagementService(INPUT_DIR, OUTPUT_DIR)

# æ³¨å†ŒAPIè·¯ç”±
app.include_router(model_routes.router)
# æ³¨å†Œè½¬å½•è·¯ç”±ï¼ˆåŒ…å«æš‚åœã€æ¢å¤ç­‰æ–°åŠŸèƒ½ï¼‰
transcription_router = create_transcription_router(transcription_service, file_service, OUTPUT_DIR)
app.include_router(transcription_router)

# åˆå§‹åŒ–æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨
preload_config = ModelPreloadConfig.get_preload_config()

# æ‰“å°é…ç½®ä¿¡æ¯
ModelPreloadConfig.print_config()

class TranscribeSettings(BaseModel):
    model: str = "medium"
    compute_type: str = "float16"
    device: str = "cuda"
    batch_size: int = 16
    word_timestamps: bool = False
    # CPUäº²å’Œæ€§é…ç½®
    cpu_affinity_enabled: bool = True
    cpu_affinity_strategy: str = "auto"  # "auto", "half", "custom"
    cpu_affinity_custom_cores: Optional[List[int]] = None
    cpu_affinity_exclude_cores: Optional[List[int]] = None

class UploadResponse(BaseModel):
    job_id: str
    filename: str
    original_name: str
    message: str

class FileInfo(BaseModel):
    name: str
    size: int
    modified: str
    path: str

def get_file_size_str(size_bytes):
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes == 0:
        return "0 B"
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    return f"{size_bytes:.1f} {size_names[i]}"

def is_video_or_audio_file(filename):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶"""
    video_extensions = {'.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.m4v'}
    audio_extensions = {'.mp3', '.wav', '.flac', '.aac', '.ogg', '.m4a', '.wma'}
    ext = os.path.splitext(filename.lower())[1]
    return ext in video_extensions or ext in audio_extensions

@app.get("/api/files")
async def list_files():
    """è·å–è¾“å…¥ç›®å½•ä¸­çš„æ‰€æœ‰åª’ä½“æ–‡ä»¶ï¼ˆå«æ–­ç‚¹ä¿¡æ¯ï¼‰"""
    try:
        files = []
        if os.path.exists(INPUT_DIR):
            for filename in os.listdir(INPUT_DIR):
                file_path = os.path.join(INPUT_DIR, filename)
                if os.path.isfile(file_path) and is_video_or_audio_file(filename):
                    stat = os.stat(file_path)

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹
                    checkpoint_info = transcription_service.check_file_checkpoint(file_path)

                    file_info = FileInfo(
                        name=filename,
                        size=stat.st_size,
                        modified=datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                        path=file_path
                    )

                    # æ·»åŠ æ–­ç‚¹ä¿¡æ¯
                    file_dict = file_info.dict()
                    if checkpoint_info:
                        file_dict['checkpoint'] = checkpoint_info
                    else:
                        file_dict['checkpoint'] = None

                    files.append(file_dict)

        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files.sort(key=lambda x: x['modified'], reverse=True)
        return {"files": files, "input_dir": INPUT_DIR}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.delete("/api/files/{filename}")
async def delete_file(filename: str):
    """åˆ é™¤inputç›®å½•ä¸­çš„æ–‡ä»¶"""
    try:
        file_path = os.path.join(INPUT_DIR, filename)
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        os.remove(file_path)
        return {"success": True, "message": f"æ–‡ä»¶ {filename} å·²åˆ é™¤"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡ä»¶å¹¶è‡ªåŠ¨åˆ›å»ºè½¬å½•ä»»åŠ¡"""
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not is_video_or_audio_file(file.filename):
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
        
        # ä¿å­˜ç”¨æˆ·åŸå§‹æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        original_filename = file.filename
        
        # å°†æ–‡ä»¶ä¿å­˜åˆ°inputç›®å½•
        input_path = os.path.join(INPUT_DIR, original_filename)
        
        # å¦‚æœåŒåæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
        counter = 1
        base_name, ext = os.path.splitext(original_filename)
        while os.path.exists(input_path):
            new_filename = f"{base_name}_{counter}{ext}"
            input_path = os.path.join(INPUT_DIR, new_filename)
            original_filename = new_filename
            counter += 1
        
        # ä¿å­˜æ–‡ä»¶
        with open(input_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        
        # åˆ›å»ºè½¬å½•ä»»åŠ¡
        job_id = uuid.uuid4().hex
        settings = JobSettings()
        transcription_service.create_job(original_filename, input_path, settings, job_id=job_id)
        
        return {
            "job_id": job_id, 
            "filename": original_filename,
            "original_name": file.filename,
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œè½¬å½•ä»»åŠ¡å·²åˆ›å»º"
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.post("/api/create-job")
async def create_job(filename: str = Form(...)):
    """ä¸ºæŒ‡å®šæ–‡ä»¶åˆ›å»ºè½¬å½•ä»»åŠ¡ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    try:
        input_path = os.path.join(INPUT_DIR, filename)
        if not os.path.exists(input_path):
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        if not is_video_or_audio_file(filename):
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")
        
        job_id = uuid.uuid4().hex
        settings = JobSettings()
        transcription_service.create_job(filename, input_path, settings, job_id=job_id)
        
        return {"job_id": job_id, "filename": filename}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}")

@app.post("/api/start")
async def start(job_id: str = Form(...), settings: str = Form(...)):
    try:
        settings_obj = TranscribeSettings(**json.loads(settings))
        job = transcription_service.get_job(job_id)
        if not job:
            return {"error": "æ— æ•ˆ job_id"}
        
        # åˆ›å»ºCPUäº²å’Œæ€§é…ç½®
        cpu_config = CPUAffinityConfig(
            enabled=settings_obj.cpu_affinity_enabled,
            strategy=settings_obj.cpu_affinity_strategy,
            custom_cores=settings_obj.cpu_affinity_custom_cores,
            exclude_cores=settings_obj.cpu_affinity_exclude_cores
        )
        
        # è¦†ç›–è®¾ç½®
        job.settings = JobSettings(
            model=settings_obj.model,
            compute_type=settings_obj.compute_type,
            device=settings_obj.device,
            batch_size=settings_obj.batch_size,
            word_timestamps=settings_obj.word_timestamps,
            cpu_affinity=cpu_config
        )
        
        transcription_service.start_job(job_id)
        return {"job_id": job_id, "started": True}
    except json.JSONDecodeError as e:
        logger.error(f"JSON è§£æå¤±è´¥: {str(e)}, åŸå§‹æ•°æ®: {settings}")
        raise HTTPException(status_code=400, detail=f"è®¾ç½®å‚æ•° JSON æ ¼å¼æ— æ•ˆ: {str(e)}")
    except Exception as e:
        logger.error(f"å¯åŠ¨ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨ä»»åŠ¡å¤±è´¥: {str(e)}")

# å–æ¶ˆä»»åŠ¡ç«¯ç‚¹å·²ç§»è‡³ transcription_routes.pyï¼Œé¿å…è·¯ç”±å†²çª

@app.get("/api/status/{job_id}")
async def status(job_id: str):
    job = transcription_service.get_job(job_id)
    if not job:
        return {"error": "æœªæ‰¾åˆ°"}
    return job.to_dict()

@app.get("/api/download/{job_id}")
async def download(job_id: str, copy_to_source: bool = False):
    job = transcription_service.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    
    if job.srt_path and os.path.exists(job.srt_path):
        filename = os.path.basename(job.srt_path)
        
        # å¦‚æœéœ€è¦å¤åˆ¶åˆ°æºæ–‡ä»¶ç›®å½•
        if copy_to_source:
            # è·å–åŸå§‹æ–‡ä»¶è·¯å¾„çš„ç›®å½•
            source_dir = os.path.dirname(job.input_path)
            source_srt_path = os.path.join(source_dir, filename)
            
            try:
                # å¤åˆ¶SRTæ–‡ä»¶åˆ°æºæ–‡ä»¶ç›®å½•
                shutil.copy2(job.srt_path, source_srt_path)
                print(f"SRTæ–‡ä»¶å·²å¤åˆ¶åˆ°æºç›®å½•: {source_srt_path}")
            except Exception as e:
                print(f"å¤åˆ¶åˆ°æºç›®å½•å¤±è´¥: {e}")
        
        # åŒæ—¶å¤åˆ¶åˆ°è¾“å‡ºç›®å½•
        output_path = os.path.join(OUTPUT_DIR, filename)
        try:
            if not os.path.exists(output_path):
                shutil.copy2(job.srt_path, output_path)
            
            return FileResponse(
                path=output_path, 
                filename=filename, 
                media_type='text/plain; charset=utf-8'
            )
        except Exception as e:
            # å¦‚æœå¤åˆ¶å¤±è´¥ï¼Œç›´æ¥è¿”å›åŸæ–‡ä»¶
            return FileResponse(
                path=job.srt_path, 
                filename=filename, 
                media_type='text/plain; charset=utf-8'
            )
    
    raise HTTPException(status_code=404, detail="å­—å¹•æ–‡ä»¶æœªç”Ÿæˆ")

@app.post("/api/copy-result/{job_id}")
async def copy_result_to_source(job_id: str):
    """å°†è½¬å½•ç»“æœå¤åˆ¶åˆ°æºæ–‡ä»¶ç›®å½•"""
    job = transcription_service.get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    
    if not job.srt_path or not os.path.exists(job.srt_path):
        raise HTTPException(status_code=404, detail="å­—å¹•æ–‡ä»¶æœªç”Ÿæˆ")
    
    try:
        # è·å–åŸå§‹æ–‡ä»¶ç›®å½•
        if hasattr(job, 'original_path') and job.original_path:
            source_dir = os.path.dirname(job.original_path)
        else:
            # å¦‚æœæ²¡æœ‰original_pathï¼Œä½¿ç”¨input_pathçš„åŒçº§ç›®å½•
            source_dir = os.path.dirname(job.input_path)
        
        # ç”Ÿæˆç›®æ ‡è·¯å¾„
        srt_filename = os.path.basename(job.srt_path)
        target_path = os.path.join(source_dir, srt_filename)
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(job.srt_path, target_path)
        
        return {
            "success": True,
            "message": f"å­—å¹•æ–‡ä»¶å·²å¤åˆ¶åˆ°: {target_path}",
            "target_path": target_path
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {str(e)}")

@app.get("/api/ping")
async def ping():
    return {"pong": True}

@app.get("/api/cpu-info")
async def get_cpu_info():
    """è·å–ç³»ç»ŸCPUä¿¡æ¯å’Œäº²å’Œæ€§æ”¯æŒçŠ¶æ€"""
    try:
        cpu_info = transcription_service.cpu_manager.get_system_info()
        return {
            "success": True,
            "cpu_info": cpu_info,
            "available_strategies": ["auto", "half", "custom"]
        }
    except Exception as e:
        return {
            "success": False, 
            "error": str(e),
            "cpu_info": {"supported": False}
        }

@app.get("/api/hardware/basic")
async def get_hardware_basic():
    """è·å–æ ¸å¿ƒç¡¬ä»¶ä¿¡æ¯"""
    try:
        # åˆ›å»ºä¸´æ—¶çš„ç¡¬ä»¶æ£€æµ‹æœåŠ¡ä»¥è·å–ä¿¡æ¯
        from services.hardware_service import get_hardware_detector
        detector = get_hardware_detector()
        hardware_info = detector.detect()
        
        return {
            "success": True,
            "hardware": hardware_info.to_dict(),
            "message": "ç¡¬ä»¶ä¿¡æ¯è·å–æˆåŠŸ"
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–ç¡¬ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}"
        }

@app.get("/api/hardware/optimize")
async def get_hardware_optimization():
    """è·å–åŸºäºç¡¬ä»¶çš„ä¼˜åŒ–é…ç½®"""
    try:
        from services.hardware_service import get_hardware_detector, get_hardware_optimizer
        detector = get_hardware_detector()
        optimizer = get_hardware_optimizer()
        
        hardware_info = detector.detect()
        optimization_config = optimizer.get_optimization_config(hardware_info)
        
        return {
            "success": True,
            "optimization": optimization_config.to_dict(),
            "message": "ä¼˜åŒ–é…ç½®è·å–æˆåŠŸ"
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–ä¼˜åŒ–é…ç½®å¤±è´¥: {str(e)}"
        }

@app.get("/api/hardware/status")
async def get_hardware_status():
    """è·å–å®Œæ•´çš„ç¡¬ä»¶çŠ¶æ€å’Œä¼˜åŒ–ä¿¡æ¯"""
    try:
        from services.hardware_service import get_hardware_detector, get_hardware_optimizer
        detector = get_hardware_detector()
        optimizer = get_hardware_optimizer()
        
        hardware_info = detector.detect()
        optimization_config = optimizer.get_optimization_config(hardware_info)
        
        return {
            "success": True,
            "hardware": hardware_info.to_dict(),
            "optimization": optimization_config.to_dict(),
            "message": "ç¡¬ä»¶çŠ¶æ€è·å–æˆåŠŸ"
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"è·å–ç¡¬ä»¶çŠ¶æ€å¤±è´¥: {str(e)}"
        }

# æ¨¡å‹ç®¡ç†APIç«¯ç‚¹
@app.get("/api/models/preload/status")
async def get_models_preload_status():
    """è·å–æ¨¡å‹é¢„åŠ è½½çŠ¶æ€"""
    try:
        status = get_preload_status()
        return {
            "success": True,
            "data": status,
            "message": "è·å–é¢„åŠ è½½çŠ¶æ€æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"è·å–é¢„åŠ è½½çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"è·å–é¢„åŠ è½½çŠ¶æ€å¤±è´¥: {str(e)}"
        }

@app.get("/api/models/cache/status")
async def get_models_cache_status():
    """è·å–æ¨¡å‹ç¼“å­˜çŠ¶æ€"""
    try:
        status = get_cache_status()
        return {
            "success": True,
            "data": status,
            "message": "è·å–ç¼“å­˜çŠ¶æ€æˆåŠŸ"
        }
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"è·å–ç¼“å­˜çŠ¶æ€å¤±è´¥: {str(e)}"
        }

@app.post("/api/models/preload/start")
async def start_models_preload():
    """æ‰‹åŠ¨å¯åŠ¨æ¨¡å‹é¢„åŠ è½½ - ç®€åŒ–ç‰ˆæœ¬ï¼Œå®ç°çœŸæ­£çš„å¹‚ç­‰æ€§"""
    try:
        logger.info("ğŸš€ æ”¶åˆ°æ¨¡å‹é¢„åŠ è½½è¯·æ±‚")

        # æ£€æŸ¥æ¨¡å‹ç®¡ç†å™¨
        model_manager = get_model_manager()
        if not model_manager:
            logger.error("âŒ æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            return {"success": False, "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"}

        # ç›´æ¥è°ƒç”¨æ¨¡å‹ç®¡ç†å™¨çš„é¢„åŠ è½½æ–¹æ³• - å®ƒå·²ç»å®ç°äº†å¹‚ç­‰æ€§
        result = await model_manager.preload_models()
        
        if result["success"]:
            logger.info(f"âœ… æ¨¡å‹é¢„åŠ è½½æˆåŠŸ: {result.get('loaded_models', 0)}/{result.get('total_models', 0)} ä¸ªæ¨¡å‹")
            return {
                "success": True,
                "message": "é¢„åŠ è½½å·²å¯åŠ¨",
                "loaded_models": result.get("loaded_models", 0),
                "total_models": result.get("total_models", 0)
            }
        else:
            logger.warning(f"âš ï¸ æ¨¡å‹é¢„åŠ è½½æœªæˆåŠŸ: {result.get('message', 'Unknown error')}")
            return {
                "success": False,
                "message": result.get("message", "é¢„åŠ è½½å¤±è´¥"),
                "failed_attempts": result.get("failed_attempts", 0)
            }

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹é¢„åŠ è½½å¼‚å¸¸: {str(e)}", exc_info=True)
        return {"success": False, "message": f"å¯åŠ¨é¢„åŠ è½½å¤±è´¥: {str(e)}"}

@app.post("/api/models/cache/clear")
async def clear_models_cache():
    """æ¸…ç©ºæ¨¡å‹ç¼“å­˜ - ç®€åŒ–ç‰ˆæœ¬ï¼Œç«‹å³åŒæ­¥çŠ¶æ€"""
    try:
        from services.model_preload_manager import get_model_manager
        model_manager = get_model_manager()
        
        if model_manager:
            model_manager.clear_cache()
            logger.info("âœ… æ‰‹åŠ¨æ¸…ç©ºæ¨¡å‹ç¼“å­˜æˆåŠŸ")
            return {
                "success": True,
                "message": "æ¨¡å‹ç¼“å­˜å·²æ¸…ç©º",
                "cache_version": model_manager.get_preload_status().get("cache_version", 0)
            }
        else:
            return {
                "success": False,
                "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"
            }
            
    except Exception as e:
        logger.error(f"âŒ æ¸…ç©ºæ¨¡å‹ç¼“å­˜å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {str(e)}"
        }

@app.post("/api/models/preload/reset")
async def reset_preload_attempts():
    """é‡ç½®é¢„åŠ è½½å¤±è´¥è®¡æ•°"""
    try:
        from services.model_preload_manager import get_model_manager
        model_manager = get_model_manager()

        if model_manager:
            model_manager.reset_preload_attempts()
            logger.info("æ‰‹åŠ¨é‡ç½®é¢„åŠ è½½å¤±è´¥è®¡æ•°æˆåŠŸ")
            return {
                "success": True,
                "message": "é¢„åŠ è½½å¤±è´¥è®¡æ•°å·²é‡ç½®"
            }
        else:
            return {
                "success": False,
                "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"
            }
    except Exception as e:
        logger.error(f"âŒ é‡ç½®é¢„åŠ è½½å¤±è´¥è®¡æ•°å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"é‡ç½®å¤±è´¥: {str(e)}"
        }

# ========== é»˜è®¤é¢„åŠ è½½æ¨¡å‹é…ç½®API ==========

@app.get("/api/models/preload/config")
async def get_default_preload_config():
    """è·å–é»˜è®¤é¢„åŠ è½½æ¨¡å‹é…ç½®"""
    try:
        from services.user_config_service import get_user_config_service
        from services.model_manager_service import get_model_manager

        user_config = get_user_config_service()
        model_manager = get_model_manager()

        # è·å–ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹
        user_selected = user_config.get_default_preload_model()

        # è·å–æ‰€æœ‰readyçš„æ¨¡å‹
        ready_models = model_manager.get_ready_whisper_models() if model_manager else []

        # è·å–ä½“ç§¯æœ€å¤§çš„readyæ¨¡å‹
        largest_model = model_manager.get_largest_ready_model() if model_manager else None

        # ç¡®å®šå®é™…ä¼šä½¿ç”¨çš„æ¨¡å‹
        actual_model = user_selected if user_selected and user_selected in ready_models else largest_model

        return {
            "success": True,
            "data": {
                "user_selected": user_selected,  # ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹
                "largest_model": largest_model,  # ä½“ç§¯æœ€å¤§çš„readyæ¨¡å‹
                "actual_model": actual_model,    # å®é™…ä¼šä½¿ç”¨çš„æ¨¡å‹
                "ready_models": ready_models     # æ‰€æœ‰readyçš„æ¨¡å‹åˆ—è¡¨
            }
        }
    except Exception as e:
        logger.error(f"âŒ è·å–é»˜è®¤é¢„åŠ è½½é…ç½®å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"è·å–é…ç½®å¤±è´¥: {str(e)}"
        }

@app.post("/api/models/preload/config")
async def set_default_preload_model(request: dict):
    """è®¾ç½®é»˜è®¤é¢„åŠ è½½æ¨¡å‹"""
    try:
        from services.user_config_service import get_user_config_service

        model_id = request.get("model_id")
        user_config = get_user_config_service()

        success = user_config.set_default_preload_model(model_id)

        if success:
            logger.info(f"âœ… è®¾ç½®é»˜è®¤é¢„åŠ è½½æ¨¡å‹: {model_id}")
            return {
                "success": True,
                "message": f"é»˜è®¤é¢„åŠ è½½æ¨¡å‹å·²è®¾ç½®ä¸º: {model_id or 'è‡ªåŠ¨é€‰æ‹©'}"
            }
        else:
            return {
                "success": False,
                "message": "è®¾ç½®å¤±è´¥"
            }
    except Exception as e:
        logger.error(f"âŒ è®¾ç½®é»˜è®¤é¢„åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"è®¾ç½®å¤±è´¥: {str(e)}"
        }

# ========== æ¨¡å‹åŠ è½½/å¸è½½API ==========

@app.post("/api/models/cache/unload")
async def unload_model(request: dict):
    """å¸è½½æŒ‡å®šæ¨¡å‹"""
    try:
        from services.model_preload_manager import get_model_manager as get_preload_manager

        model_id = request.get("model_id")
        device = request.get("device", "cuda")
        compute_type = request.get("compute_type", "float16")

        if not model_id:
            return {
                "success": False,
                "message": "ç¼ºå°‘model_idå‚æ•°"
            }

        preload_manager = get_preload_manager()
        if not preload_manager:
            return {
                "success": False,
                "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"
            }

        preload_manager.evict_model(model_id, device, compute_type)
        logger.info(f"âœ… å¸è½½æ¨¡å‹: {model_id}")

        return {
            "success": True,
            "message": f"æ¨¡å‹ {model_id} å·²å¸è½½"
        }
    except Exception as e:
        logger.error(f"âŒ å¸è½½æ¨¡å‹å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"å¸è½½å¤±è´¥: {str(e)}"
        }

@app.post("/api/models/preload/load-specific")
async def load_specific_model(request: dict):
    """åŠ è½½æŒ‡å®šæ¨¡å‹"""
    try:
        from services.model_preload_manager import get_model_manager as get_preload_manager, PreloadConfig
        from models.job_models import JobSettings
        import torch

        model_id = request.get("model_id")

        if not model_id:
            return {
                "success": False,
                "message": "ç¼ºå°‘model_idå‚æ•°"
            }

        preload_manager = get_preload_manager()
        if not preload_manager:
            return {
                "success": False,
                "message": "æ¨¡å‹ç®¡ç†å™¨æœªåˆå§‹åŒ–"
            }

        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        from services.model_manager_service import get_model_manager
        model_mgr = get_model_manager()
        status, local_path, detail = model_mgr._check_whisper_model_exists(model_id)

        if status != "ready":
            return {
                "success": False,
                "message": f"æ¨¡å‹æœªå°±ç»ª: {status}"
            }

        # å‡†å¤‡åŠ è½½å‚æ•°
        device = "cuda" if torch.cuda.is_available() else "cpu"
        settings = JobSettings(
            model=model_id,
            compute_type="float16",
            device=device
        )

        # åŠ è½½æ¨¡å‹
        logger.info(f"ğŸ”„ å¼€å§‹åŠ è½½æ¨¡å‹: {model_id}")
        model = await asyncio.get_event_loop().run_in_executor(
            None,
            preload_manager.get_model,
            settings
        )

        if model:
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_id}")
            return {
                "success": True,
                "message": f"æ¨¡å‹ {model_id} åŠ è½½æˆåŠŸ"
            }
        else:
            return {
                "success": False,
                "message": "æ¨¡å‹åŠ è½½å¤±è´¥"
            }
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"åŠ è½½å¤±è´¥: {str(e)}"
        }

@app.post("/api/shutdown")
async def shutdown_server():
    """ä¼˜é›…å…³é—­æœåŠ¡å™¨"""
    try:
        logger.info("æ”¶åˆ°å…³é—­æœåŠ¡å™¨è¯·æ±‚")

        # æ¸…ç†èµ„æº
        from services.model_preload_manager import get_model_manager
        model_manager = get_model_manager()
        if model_manager:
            model_manager.clear_cache()
            logger.info("å·²æ¸…ç†æ¨¡å‹ç¼“å­˜")
        
        # è¿”å›æˆåŠŸå“åº”
        response = {
            "success": True,
            "message": "æœåŠ¡å™¨æ­£åœ¨ä¼˜é›…å…³é—­"
        }
        
        # å¼‚æ­¥å…³é—­æœåŠ¡å™¨
        import asyncio
        import os
        asyncio.create_task(delayed_shutdown())
        
        return response
        
    except Exception as e:
        logger.error(f"å…³é—­æœåŠ¡å™¨å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "message": f"å…³é—­æœåŠ¡å™¨å¤±è´¥: {str(e)}"
        }

async def delayed_shutdown():
    """å»¶è¿Ÿå…³é—­æœåŠ¡å™¨ï¼Œç»™å“åº”æ—¶é—´è¿”å›"""
    await asyncio.sleep(1)  # ç­‰å¾…1ç§’è®©å“åº”è¿”å›
    logger.info("æœåŠ¡å™¨å³å°†å…³é—­...")
    import os
    os._exit(0)

if __name__ == "__main__":
    import uvicorn
    # ç›´æ¥ä¼ å…¥ appï¼Œå…³é—­ reloadï¼Œç¡®ä¿ä½¿ç”¨å½“å‰æ–‡ä»¶å†…å®šä¹‰çš„åº”ç”¨å®ä¾‹
    uvicorn.run(app, host="127.0.0.1", port=8000, reload=False,
                limit_max_requests=1000, limit_concurrency=50)
