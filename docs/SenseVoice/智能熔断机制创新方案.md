# 智能熔断机制创新方案

## 背景

### 现状
在进行音频转录前会进行初步的人声分离，但由于只进行采样点分析且只使用最小的模型，可能会出现BGM未清理干净的结果，于是引入在SenseVoice转录时如果低置信度频率达到阈值则熔断回退到人声分离使用更大的模型进行。同时在SenseVoice转录时置信度低的片段会挑出来使用Whisper进行二次转录生成多份结果，在未来校对时使用LLM语义分析进一步修正。

### 核心问题

置信度低可能原因：没清理干净BGM或者转录质量，或者两者兼有。只要判断失误，错误的调用大模型，时间成本就会大大增加。是否有更好的判断机制？单纯的对置信度的分布或者频率进行分析意义不大，需要结合工程经验、最新研究成果，给出一个完美的、具有创新意义并且可靠性高、速度快、性能要求低的新方案。

---

## 一、现有参考方案分析

### 参考方案1（三角定位 + 分诊矩阵）

**优点**：
- 利用 SenseVoice 原生 BGM 标签（零成本）
- SNR 作为物理指标直观
- 决策树逻辑清晰易调试

**缺点**：
- SNR 的能量分布法过于简化，混响/低频底噪场景误判率高
- 文本熵只能检测极端情况（复读机/乱码），大部分"低置信但有效"的结果会漏检
- 阈值硬编码（10dB, 15dB），无法适应不同音频类型

### 参考方案2（加权评分器）

**优点**：
- 引入更多特征维度（谱平坦度）
- 加权机制比硬阈值灵活
- 引入 `QUICK_DENOISE` 作为廉价中间层

**缺点**：
- 权重需要大量人工调参，且不同场景权重应不同
- 特征之间有冗余（SNR 和谱平坦度高度相关）
- **核心缺陷**：没有直接检测"音乐"，只是间接推断

---

## 二、创新方案

基于最新研究和工程实践，提出以下几个更优的方案：

---

### 方案 A：谐波比快速检测（Harmonic-Percussive Energy Ratio）

**核心洞察**：音乐和语音在频谱上有本质区别——音乐有明显的**谐波结构**（频谱上的水平线），而语音虽有谐波但更动态。通过计算**谐波能量占比**，可以直接判断是否存在持续性背景音乐。

**原理**：
- 使用 librosa 的 HPSS（Harmonic-Percussive Source Separation）的**轻量级版本**，只计算能量比，不做分离
- 音乐背景通常 `harmonic_ratio > 0.6`，纯语音 `< 0.4`

**实现**：

```python
import numpy as np

def calculate_harmonic_ratio(audio_chunk: np.ndarray, sr: int = 16000) -> float:
    """
    快速计算谐波能量占比（无需librosa）
    原理：谐波在频谱上呈现窄带，噪声/语音更宽带
    耗时: < 10ms (纯numpy)
    """
    # 1. 计算短时傅里叶变换
    n_fft = 512
    hop = 256
    # 简化：只取一帧代表
    frame = audio_chunk[:n_fft] * np.hanning(n_fft)
    spectrum = np.abs(np.fft.rfft(frame))

    # 2. 找频谱峰值（谐波分量）
    from scipy.signal import find_peaks
    peaks, _ = find_peaks(spectrum, height=np.mean(spectrum) * 2, distance=5)

    # 3. 计算峰值能量占总能量的比例
    peak_energy = np.sum(spectrum[peaks] ** 2) if len(peaks) > 0 else 0
    total_energy = np.sum(spectrum ** 2)

    harmonic_ratio = peak_energy / (total_energy + 1e-10)
    return harmonic_ratio
```

**决策逻辑**：
| 谐波比 | 含义 | 动作 |
|--------|------|------|
| > 0.55 | 存在持续音乐 | 升级 Demucs |
| 0.35-0.55 | 可能有轻微背景 | Quick Denoise |
| < 0.35 | 干净语音 | 切 Whisper |

**优势**：
- 直接检测音乐存在，比 SNR/BGM 标签更准确
- 纯数学计算，10ms 内完成
- 可替代参考方案中不准确的 SNR 估计

---

### 方案 B：多臂老虎机自适应决策（MAB-Based Adaptive Arbiter）

**核心洞察**：固定权重和阈值无法适应不同类型的视频。根据业界实践，多臂老虎机(MAB)可以在线学习最优策略，自动调整而无需人工调参。

**原理**：
- 每种"救援策略"视为一个"臂"（Demucs、Whisper、Quick Denoise、Pass）
- 根据实际效果（后续置信度是否提升）更新每个臂的奖励估计
- 使用 UCB1 或 Thompson Sampling 平衡探索与利用

**实现**：
```python
import numpy as np
from dataclasses import dataclass, field
from typing import Dict
from enum import Enum

class RescueAction(Enum):
    PASS = "pass"
    QUICK_DENOISE = "quick_denoise"
    SEPARATE_VOCALS = "separate"
    SWITCH_MODEL = "switch_model"

@dataclass
class MABContext:
    """上下文特征向量（用于Contextual Bandit）"""
    snr_bin: int          # SNR分桶: 0=低, 1=中, 2=高
    has_bgm_tag: bool
    harmonic_ratio_bin: int  # 谐波比分桶

    def to_key(self) -> str:
        return f"{self.snr_bin}_{int(self.has_bgm_tag)}_{self.harmonic_ratio_bin}"

class AdaptiveArbiter:
    """
    基于 UCB1 的自适应决策器
    每个(context, action)组合维护独立的统计
    """
    def __init__(self):
        self.actions = list(RescueAction)
        # {context_key: {action: (total_reward, count)}}
        self.stats: Dict[str, Dict[RescueAction, tuple]] = {}
        self.total_pulls = 0

    def _get_ucb_score(self, reward_sum: float, count: int) -> float:
        if count == 0:
            return float('inf')  # 未探索过，优先尝试
        avg_reward = reward_sum / count
        exploration_bonus = np.sqrt(2 * np.log(self.total_pulls + 1) / count)
        return avg_reward + exploration_bonus

    def decide(self, ctx: MABContext) -> RescueAction:
        """选择最优动作"""
        key = ctx.to_key()
        if key not in self.stats:
            self.stats[key] = {a: (0.0, 0) for a in self.actions}

        # 计算每个动作的UCB分数
        scores = {}
        for action in self.actions:
            reward_sum, count = self.stats[key][action]
            scores[action] = self._get_ucb_score(reward_sum, count)

        self.total_pulls += 1
        return max(scores, key=scores.get)

    def update(self, ctx: MABContext, action: RescueAction, reward: float):
        """根据实际效果更新统计"""
        key = ctx.to_key()
        old_sum, old_count = self.stats[key][action]
        self.stats[key][action] = (old_sum + reward, old_count + 1)
```

**奖励函数设计**：
```python
def calculate_reward(before_conf: float, after_conf: float, time_cost: float) -> float:
    """
    奖励 = 置信度提升 - 时间惩罚
    """
    conf_gain = max(0, after_conf - before_conf)  # 只奖励提升
    time_penalty = time_cost / 10.0  # 归一化时间成本
    return conf_gain - time_penalty * 0.5
```

**优势**：
- 自动学习最优策略，无需人工调参
- 能适应不同类型的视频/音频
- 冷启动后 100-200 个样本即可收敛
- 支持持久化统计数据，跨会话累积学习

---

### 方案 C：频谱指纹快速音乐检测（Spectral Fingerprint Music Detector）

**核心洞察**：真正的"音乐"有独特的频谱特征——**稳定的频谱质心 + 低 ZCR 方差 + 高谱对比度**。这三个特征的组合可以非常准确地检测背景音乐，且计算成本极低。

**原理**（基于经典信号处理研究）：
- **ZCR 方差**：音乐的 ZCR 曲线比语音稳定得多
- **谱质心稳定性**：音乐的谱质心波动小
- **谱对比度**：音乐在各频带有明显的峰谷差异

**实现**：
```python
def extract_music_fingerprint(audio_chunk: np.ndarray, sr: int = 16000) -> dict:
    """
    提取音乐指纹特征（全部纯numpy，< 15ms）
    """
    frame_size = 512
    hop = 256
    num_frames = (len(audio_chunk) - frame_size) // hop + 1

    zcrs = []
    centroids = []

    for i in range(num_frames):
        frame = audio_chunk[i*hop : i*hop + frame_size]

        # ZCR
        zcr = np.sum(np.abs(np.diff(np.signbit(frame)))) / len(frame)
        zcrs.append(zcr)

        # 谱质心
        spectrum = np.abs(np.fft.rfft(frame * np.hanning(frame_size)))
        freqs = np.fft.rfftfreq(frame_size, 1/sr)
        centroid = np.sum(freqs * spectrum) / (np.sum(spectrum) + 1e-10)
        centroids.append(centroid)

    return {
        "zcr_variance": np.var(zcrs),          # 音乐 < 0.001, 语音 > 0.005
        "centroid_std": np.std(centroids),     # 音乐 < 500Hz, 语音 > 1000Hz
        "zcr_mean": np.mean(zcrs),             # 音乐 ~0.05, 语音 ~0.1
    }

def is_music_dominant(features: dict) -> tuple[bool, float]:
    """
    判断是否音乐主导
    返回: (is_music, confidence)
    """
    # 特征融合评分
    music_score = 0.0

    if features["zcr_variance"] < 0.002:
        music_score += 0.4
    if features["centroid_std"] < 600:
        music_score += 0.3
    if features["zcr_mean"] < 0.08:
        music_score += 0.3

    return (music_score > 0.5, music_score)
```

**优势**：
- 基于经典信号处理研究，理论基础扎实
- 纯 numpy 实现，15ms 内完成
- 可以检测"隐性"音乐（没有被 SenseVoice 识别为 BGM 标签的）

---

### 方案 D：CTC 后验熵分解（Entropy Decomposition from CTC Posteriors）

**核心洞察**：参考 NVIDIA NeMo 的研究，Tsallis/Renyi 熵比单纯的 softmax 概率更能反映真实置信度。更进一步，可以**分解**熵来判断问题来源。

**原理**：
- **总熵** = 声学熵 + 语言熵
- 如果**声学熵高、语言熵低**：说明音频质量差（噪音） -> 分离
- 如果**声学熵低、语言熵高**：说明模型不认识词汇 -> 换模型
- 如果**两者都高**：说明是极端困难场景

**实现思路**（需要修改 SenseVoice 输出）：
```python
def analyze_entropy_decomposition(
    acoustic_logits: np.ndarray,  # 声学模型输出
    lm_scores: np.ndarray,        # 语言模型分数（如有）
    q: float = 1/3                # Tsallis 熵指数（NVIDIA推荐）
) -> dict:
    """
    Tsallis 熵计算
    """
    def tsallis_entropy(probs, q):
        if abs(q - 1.0) < 1e-6:
            return -np.sum(probs * np.log(probs + 1e-10))
        return (1 - np.sum(probs ** q)) / (q - 1)

    # 对 acoustic logits 计算熵
    acoustic_probs = np.exp(acoustic_logits) / np.sum(np.exp(acoustic_logits))
    acoustic_entropy = tsallis_entropy(acoustic_probs, q)

    # 如果有 LM 分数，也计算语言熵
    # ...

    return {
        "acoustic_entropy": acoustic_entropy,
        "is_noisy_audio": acoustic_entropy > 2.5,
        "needs_better_model": acoustic_entropy < 1.5 and lm_scores is not None
    }
```

**限制**：需要访问 SenseVoice 的原始 logits 输出，可能需要修改推理代码。

---

### 方案 E：滑动窗口历史学习（Sliding Window Context Learning）

**核心洞察**：同一视频/音频文件内，音频特征往往是**连续变化**的。如果前几个片段都需要分离，后续片段大概率也需要。利用**局部历史**可以大大提高判断准确性。

**实现**：
```python
from collections import deque
from dataclasses import dataclass

@dataclass
class SegmentHistory:
    action_taken: RescueAction
    was_effective: bool  # 置信度是否提升
    features: dict

class ContextualPredictor:
    """
    基于滑动窗口的上下文预测器
    """
    def __init__(self, window_size: int = 5):
        self.history: deque = deque(maxlen=window_size)

    def predict_likely_action(self) -> tuple[RescueAction, float]:
        """
        根据历史预测最可能需要的动作
        """
        if len(self.history) < 2:
            return None, 0.0  # 历史不足

        # 统计最近成功的动作
        effective_actions = [h.action_taken for h in self.history if h.was_effective]
        if not effective_actions:
            return None, 0.0

        # 找最频繁的有效动作
        from collections import Counter
        most_common = Counter(effective_actions).most_common(1)[0]
        action, count = most_common
        confidence = count / len(self.history)

        return action, confidence

    def add_result(self, segment_history: SegmentHistory):
        self.history.append(segment_history)
```

**决策融合**：
```python
def smart_decide(features, mab_arbiter, context_predictor):
    # 1. 检查上下文预测
    ctx_action, ctx_conf = context_predictor.predict_likely_action()

    # 2. 如果上下文高度一致（置信度 > 0.8），直接采用
    if ctx_conf > 0.8:
        return ctx_action

    # 3. 否则用 MAB 决策
    return mab_arbiter.decide(features)
```

**优势**：
- 利用局部相关性，减少重复计算
- 对于"全程 BGM"的视频特别有效
- 几乎零成本

---

## 三、综合推荐方案

综合以上分析，推荐**组合方案**：

```
                    +-----------------------------------+
                    |     低置信度片段进入              |
                    +-----------------+-----------------+
                                      |
                    +-----------------v-----------------+
                    |  第一层：历史上下文检查           |
                    |  (方案E - 零成本)                |
                    |  置信度>0.8? -> 直接采用历史策略  |
                    +-----------------+-----------------+
                                      | 否
                    +-----------------v-----------------+
                    |  第二层：快速音乐指纹检测         |
                    |  (方案C - 15ms)                  |
                    |  music_score > 0.6? -> Demucs    |
                    +-----------------+-----------------+
                                      | 否
                    +-----------------v-----------------+
                    |  第三层：谐波比 + BGM标签         |
                    |  (方案A - 10ms)                  |
                    |  二次确认是否有音乐              |
                    +-----------------+-----------------+
                                      |
                    +-----------------v-----------------+
                    |  第四层：MAB自适应决策            |
                    |  (方案B - 0ms)                   |
                    |  根据特征组合选择最优策略         |
                    +-----------------+-----------------+
                                      |
                    +-----------------v-----------------+
                    |  执行并记录结果                   |
                    |  更新 MAB 统计 + 历史窗口         |
                    +-----------------------------------+
```

### 核心代码框架

```python
class SmartCircuitBreaker:
    def __init__(self):
        self.context_predictor = ContextualPredictor(window_size=5)
        self.mab_arbiter = AdaptiveArbiter()

    async def decide(self, segment, result, audio_chunk) -> RescueAction:
        # === 第一层：历史上下文 ===
        ctx_action, ctx_conf = self.context_predictor.predict_likely_action()
        if ctx_conf > 0.8:
            return ctx_action

        # === 第二层：音乐指纹检测 ===
        music_features = extract_music_fingerprint(audio_chunk)
        is_music, music_conf = is_music_dominant(music_features)
        if is_music and music_conf > 0.6:
            return RescueAction.SEPARATE_VOCALS

        # === 第三层：谐波比确认 ===
        harmonic_ratio = calculate_harmonic_ratio(audio_chunk)
        has_bgm_tag = "<|BGM|>" in getattr(result, "raw_text", "")

        if has_bgm_tag or harmonic_ratio > 0.55:
            return RescueAction.SEPARATE_VOCALS

        # === 第四层：MAB决策 ===
        snr = calculate_quick_snr(audio_chunk)
        ctx = MABContext(
            snr_bin=0 if snr < 10 else (1 if snr < 20 else 2),
            has_bgm_tag=has_bgm_tag,
            harmonic_ratio_bin=0 if harmonic_ratio < 0.35 else (1 if harmonic_ratio < 0.55 else 2)
        )
        return self.mab_arbiter.decide(ctx)
```

---

## 四、方案对比总结

| 方案 | 创新点 | 成本 | 准确性 | 实现难度 | 推荐度 |
|------|--------|------|--------|----------|--------|
| **A: 谐波比** | 直接检测音乐特征 | 10ms | 高 | 低 | 5/5 |
| **B: MAB自适应** | 在线学习，无需调参 | 0ms | 随时间提升 | 中 | 5/5 |
| **C: 音乐指纹** | 多特征融合，理论基础强 | 15ms | 很高 | 低 | 4/5 |
| **D: 熵分解** | 利用模型内部信息 | 需修改推理 | 理论最高 | 高 | 3/5 |
| **E: 历史上下文** | 零成本，利用局部相关性 | 0ms | 场景依赖 | 很低 | 5/5 |

**最终建议**：**A + B + E 组合**是最优选择，兼顾了准确性、成本和可维护性。

---

## 五、参考资料

- [NVIDIA: Entropy-Based Methods for ASR Confidence Estimation](https://developer.nvidia.com/blog/entropy-based-methods-for-word-level-asr-confidence-estimation/)
- [USC: Speech/Music Discrimination Features](https://sipi.usc.edu/~ortega/WaveletPapers/speech_music1.pdf)
- [Multi-Armed Bandits for Model Serving - Pluralsight](https://medium.com/data-science-and-machine-learning-at-pluralsight/multi-armed-bandits-for-model-serving-and-experimentation-ab71d4441afb)
- [Harmonic-Percussive Source Separation - ILSP](http://mir.ilsp.gr/harmonic_percussive_separation.html)
- [Zero Crossing Rate - ScienceDirect](https://www.sciencedirect.com/topics/engineering/zero-crossing-rate)
- [Circuit Breaker Pattern - Microservices.io](https://microservices.io/patterns/reliability/circuit-breaker.html)
- [Spectral Descriptors - MATLAB](https://www.mathworks.com/help/audio/ug/spectral-descriptors.html)
- [Confidence Estimation for ASR - arXiv](https://arxiv.org/abs/2010.11428)
- [Online Model Selection with MAB - arXiv](https://arxiv.org/abs/2101.10385)
