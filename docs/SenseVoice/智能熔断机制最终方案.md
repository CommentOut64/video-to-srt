# 智能熔断机制开发文档 (Smart Circuit Breaker)

**版本**: v1.0 | **核心策略**: 频谱指纹 (Spectral Fingerprint) + 历史惯性 (History Inertia)

## 1\. 设计理念与目标

本模块充当音频转录流程中的“分诊台”。在调用昂贵的大模型（如 Demucs 分离或 Whisper 转录）之前，对音频切片进行极低成本的预判。

  * **核心痛点解决**：
      * **低响度 BGM 检测**：传统音量阈值法无法检测被掩盖的微弱 BGM，导致时间戳漂移。本方案通过**频域稳定性**（而非响度）来捕捉隐性 BGM。
      * **资源极简**：完全移除 GPU 依赖，纯 CPU 运行，显存占用忽略不计。
      * **决策稳定性**：利用音频的时间连续性，避免对同一段 BGM 反复进行不一致的“分离/不分离”决策。

-----

## 2\. 系统要求与环境

  * **运行环境**：
      * CPU: 任意现代 CPU（单核占用极低）
      * RAM: \< 100MB 额外占用
      * GPU: **不需要**
  * **依赖库**：
      * Python 3.8+
      * `numpy` (唯一核心依赖)
      * `dataclasses`, `collections` (Python 标准库)

-----

## 3\. 核心算法逻辑

### 3.1 方案 C：频谱指纹检测 (The Sensor)

利用音乐与语音在频域上的本质区别：

  * **ZCR 方差 (ZCR Variance)**: 音乐（即使是打击乐）在短时窗口内的过零率变化通常比无规律的语音更平滑。
  * **谱质心标准差 (Spectral Centroid Std)**: 音乐通常有固定的调性或持续的频段，其能量中心波动较小；语音的能量中心随元音辅音变化剧烈跳动。
  * **逻辑**：通过计算这几个指标，得出一个 0\~1 的 `music_score`。分数越高，越像音乐。

### 3.2 方案 E：滑动窗口历史 (The Memory)

利用音频的**时间局部相关性**：

  * **惯性原则**：如果过去 5 个切片中有 3 个判定为“含 BGM”，则当前切片极大概率也含 BGM。
  * **阻尼机制**：如果当前切片极度干净（`music_score` 极低），则强制打破惯性，防止错误延续。

-----

## 4\. 完整代码实现

将以下代码保存为 `circuit_breaker.py`。

```python
import numpy as np
from collections import deque
from dataclasses import dataclass
from typing import Tuple, Optional

# ==========================================
# 核心配置参数 (可在此处统一调整)
# ==========================================
CONFIG = {
    "SR": 16000,                # 采样率，必须与输入音频一致
    "N_FFT": 512,               # FFT窗口大小
    "HOP_LENGTH": 256,          # 步长
    "HISTORY_SIZE": 5,          # 历史窗口大小 (方案E)
    "HISTORY_THRESHOLD": 0.6,   # 历史惯性触发阈值 (3/5 = 0.6)
    "MUSIC_SCORE_THRESHOLD": 0.35, # 单帧检测阈值 (方案C)，设低以捕捉微弱BGM
    "RESET_SCORE": 0.15         # 极度干净的阈值，低于此值强制打断惯性
}

@dataclass
class DecisionResult:
    should_separate: bool       # 最终决策：是否需要调用Demucs
    confidence: float           # 置信度/评分
    reason: str                 # 决策来源: "history", "fingerprint", "clean_reset"

class SpectralFingerprintDetector:
    """
    方案 C 实现：纯 Numpy 音频特征提取器
    """
    def compute_score(self, audio_chunk: np.ndarray) -> float:
        # 1. 预处理：防止静音报错，添加极小值
        if len(audio_chunk) < CONFIG["N_FFT"]:
            return 0.0
        y = audio_chunk + 1e-10

        # 2. 计算基础特征
        # Zero Crossing Rate (ZCR)
        zcr = np.abs(np.diff(np.signbit(y))).sum() / (len(y) - 1)
        
        # 分帧计算频谱特征 (为了速度，不做完整的STFT，只做简化的帧分析)
        num_frames = (len(y) - CONFIG["N_FFT"]) // CONFIG["HOP_LENGTH"] + 1
        if num_frames <= 0: return 0.0
        
        # 提取关键帧特征 (ZCR波动率 & 质心波动率)
        frame_zcrs = []
        centroids = []
        
        # 为了极速，最多只取 50 帧进行采样分析（均匀分布）
        step = max(1, num_frames // 50)
        
        for i in range(0, num_frames, step):
            start = i * CONFIG["HOP_LENGTH"]
            end = start + CONFIG["N_FFT"]
            frame = y[start:end] * np.hanning(CONFIG["N_FFT"])
            
            # 帧 ZCR
            frame_zcr = np.sum(np.abs(np.diff(np.signbit(frame)))) / len(frame)
            frame_zcrs.append(frame_zcr)
            
            # 帧 频谱质心
            spectrum = np.abs(np.fft.rfft(frame))
            freqs = np.fft.rfftfreq(CONFIG["N_FFT"], 1/CONFIG["SR"])
            centroid = np.sum(freqs * spectrum) / (np.sum(spectrum) + 1e-10)
            centroids.append(centroid)
            
        # 3. 计算统计量 (核心判据)
        zcr_var = np.var(frame_zcrs)       # 音乐通常 < 0.002
        centroid_std = np.std(centroids)   # 音乐通常 < 600Hz
        
        # 4. 融合打分 (Score 越高越像音乐)
        score = 0.0
        # 判据1: ZCR 极度稳定 (最重要的BGM特征)
        if zcr_var < 0.0015: score += 0.5
        elif zcr_var < 0.003: score += 0.2
        
        # 判据2: 频率中心稳定
        if centroid_std < 500: score += 0.4
        elif centroid_std < 900: score += 0.2
        
        # 归一化限制
        return min(1.0, score)

class SmartArbiter:
    """
    方案 C + E 集成控制器
    """
    def __init__(self):
        self.detector = SpectralFingerprintDetector()
        self.history = deque(maxlen=CONFIG["HISTORY_SIZE"])
        
    def decide(self, audio_chunk: np.ndarray) -> DecisionResult:
        """
        主入口函数
        """
        # 1. 计算当前片段的物理评分
        music_score = self.detector.compute_score(audio_chunk)
        
        # 2. 检查是否极度干净 (Reset机制)
        if music_score < CONFIG["RESET_SCORE"]:
            self._update_history(False)
            return DecisionResult(False, music_score, "clean_reset")

        # 3. 计算历史惯性 (方案 E)
        history_positive_count = sum(self.history)
        history_confidence = history_positive_count / CONFIG["HISTORY_SIZE"] if self.history else 0.0
        
        if len(self.history) >= 3 and history_confidence >= CONFIG["HISTORY_THRESHOLD"]:
            # 惯性触发：即便当前分数稍低，也倾向于分离
            should_separate = True
            reason = "history_inertia"
        else:
            # 无惯性：完全依赖当前检测
            should_separate = music_score > CONFIG["MUSIC_SCORE_THRESHOLD"]
            reason = "fingerprint"
            
        # 4. 更新历史并返回
        self._update_history(should_separate)
        return DecisionResult(should_separate, music_score, reason)

    def _update_history(self, decision: bool):
        self.history.append(decision)

    def reset(self):
        """处理新视频文件时需调用"""
        self.history.clear()
```

-----

## 5\. 接入与集成指南

### 5.1 数据流接入点

该模块应放置在 VAD（语音活动检测）切片之后，ASR（语音转写）推理之前。

**伪代码示例**：

```python
# 初始化裁判 (全局复用)
arbiter = SmartArbiter()

# --- 处理新文件前 ---
arbiter.reset() 

# --- 遍历音频切片 ---
for chunk in audio_chunks:
    # chunk 必须是 float32 或 int16 转 float 的 numpy 数组
    # 且采样率需为 16000Hz
    
    # 1. 询问裁判
    decision = arbiter.decide(chunk)
    
    # 2. 根据裁判结果分流
    if decision.should_separate:
        print(f"检测到BGM ({decision.reason}, score={decision.confidence:.2f}) -> 调用 Demucs")
        processed_chunk = run_demucs_separation(chunk)
        final_text = run_sensevoice(processed_chunk)
    else:
        print(f"音频干净 ({decision.reason}, score={decision.confidence:.2f}) -> 直接转录")
        final_text = run_sensevoice(chunk)
```

### 5.2 输入数据要求

1.  **格式**：必须是 `numpy.ndarray`。
2.  **类型**：推荐 `float32` (-1.0 到 1.0)。如果是 `int16`，建议先除以 32768 转为 float，否则 FFT 计算能量时数值会过大。
3.  **采样率**：代码默认 `16000`。如果你的系统是 `44100` 或 `48000`，请在 `CONFIG` 中修改 `SR` 参数，否则频率计算会出错。
4.  **时长**：建议切片长度在 **2秒 到 30秒** 之间。
      * \< 1秒：特征样本太少，方差计算不准。
      * > 30秒：惯性机制的更新频率太低，反应迟钝。

-----

## 6\. 调参规则手册 (Tuning Guide)

根据实际测试情况，可调整 `CONFIG` 中的参数。

### 场景 A：必须极致干净，宁可多花时间 (激进模式)

  * **调整目标**：抓住哪怕最轻微的 BGM。
  * **操作**：
      * `MUSIC_SCORE_THRESHOLD`: **0.30** (降低门槛)
      * `HISTORY_THRESHOLD`: **0.5** (只要有一半历史是BGM，就继续分离)
      * `RESET_SCORE`: **0.10** (只有极其完美的静音才允许打断惯性)

### 场景 B：由于机器太慢，只想处理严重的 BGM (性能模式)

  * **调整目标**：只分离那种“动次打次”明显盖过人声的片段。
  * **操作**：
      * `MUSIC_SCORE_THRESHOLD`: **0.65** (提高门槛)
      * `HISTORY_THRESHOLD`: **0.8** (需高度确信才利用惯性)

### 场景 C：低音量 BGM 漏检 (当前痛点)

  * **调整目标**：虽然 `music_score` 只有 0.4，但确实有背景音。
  * **核心技巧**：保持 `MUSIC_SCORE_THRESHOLD` 在 **0.35** 左右，但增加 `HISTORY_SIZE` 到 **8**。这意味着系统会“记住”更久之前的 BGM 状态，防止中间因为 BGM 变弱而突然中断分离。

-----

## 7\. 常见问题排查 (Troubleshooting)

| 现象 | 可能原因 | 解决方案 |
| :--- | :--- | :--- |
| **报错 `Divide by zero`** | 输入了完全静音的片段 (全0数组) | 代码已在 `compute_score` 开头加入 `+ 1e-10` 保护，无需担心。 |
| **所有音频都被判定为 BGM** | 输入音频是 `int16` 格式但未归一化 | 检查输入数值范围。如果是 +/- 30000 级别，请先除以 32768.0。 |
| **说话停顿处被误判为 BGM** | 环境底噪太稳定 (如风扇声) | 适当调高 `RESET_SCORE` 到 0.2，或者稍微调高 `zcr_var` 的判定标准。 |
| **惯性关不掉 (后面全是干净的还在分)** | `RESET_SCORE` 设置太低 | 将 `RESET_SCORE` 调高到 0.2 或 0.25，让系统更容易从“分离模式”跳出。 |

-----

## 8\. 下一步建议

1.  **本地预热**：选取 3 个典型视频（一个纯人声，一个纯音乐，一个混合且BGM很小），运行上述代码，观察 `print` 输出的 `score` 值。
2.  **校准阈值**：根据这 3 个视频的 score 分布，微调 `MUSIC_SCORE_THRESHOLD`。这是唯一需要动的地方。
3.  **封装**：将此模块封装为独立的 Python 文件，在你的主程序中 `import SmartArbiter` 即可使用。