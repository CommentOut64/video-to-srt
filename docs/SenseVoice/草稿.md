### 🏛️ 宏观架构：三层漏斗模型

1.  **物理层 (Physical Layer)**：音频清洗与切分（FFmpeg / Demucs / VAD）。
2.  **声学层 (Acoustic Layer)**：极速转录与置信度筛查（SenseVoice + Whisper 补刀）。
3.  **语义层 (Semantic Layer)**：上下文理解、纠错与翻译（LLM）。

-----

### 🛠️ 详细工作流步骤 (Step-by-Step)

#### 第一阶段：物理层 - 预处理与分段

*目标：把视频变成干净的、机器爱听的短音频片段。*

**1. 音频提取与格式化 (FFmpeg)**

  * **输入**：用户上传的视频文件 (mp4, mkv, mov 等)。
  * **操作**：调用 FFmpeg 提取音频，统一转换为 **16kHz 单声道 WAV**（这是所有 ASR 模型的最佳输入标准）。
  * **命令**：
    ```bash
    ffmpeg -i input_video.mp4 -vn -acodec pcm_s16le -ac 1 -ar 16000 -map 0:a:0 temp_audio.wav
    ```

**2. 人声分离 (Vocal Separation)**

  * **引擎**：**Demucs (htdemucs)**
  * **策略**：
      * 如果用户选“快速模式”：跳过此步。
      * **默认**：使用 `htdemucs`。相比 `mdx_extra`，它在 GPU 上快 3-4 倍，且对语音识别精度无损。
  * **操作**：
      * 输入 `temp_audio.wav`。
      * 保留 `vocals.wav`，丢弃 `drums`, `bass`, `other`。
  * **输出**：纯净人声文件 `vocals.wav`。

**3. 语音活动检测 (VAD)**

  * **引擎**：**Silero VAD v4/v5**
  * **输入**：`vocals.wav`
  * **关键参数**：
      * `threshold`: 0.5 (标准)
      * `min_speech_duration_ms`: 250ms (避免极短噪音)
      * `min_silence_duration_ms`: 500ms (保证句子间有足够停顿)
      * `speech_pad_ms`: **200ms** (前后各加 200ms 缓冲，防止切掉首尾音节)。
  * **输出**：一个列表 `VAD_Segments = [{start: 0.5, end: 4.2}, {start: 5.1, end: 8.8}, ...]`。

-----

#### 第二阶段：声学层 - 转录与过滤

*目标：以最快速度生成初稿，并找出“听不清”的地方。*

**4. 极速转录 (Primary Transcription)**

  * **引擎**：**SenseVoice-Small** (INT8 量化版)
  * **操作**：
      * 遍历 `VAD_Segments`。
      * **批量推理 (Batch Inference)**：为了速度，不要一段一段送，建议把多个短段拼成一个 Batch 送入 GPU。
  * **获取数据**：
      * `text`: 文本内容。
      * `timestamp`: 字级/词级时间戳（SenseVoice 原生支持）。
      * **`confidence_score`**: 平均置信度分数 (0.0 - 1.0)。
  * **输出**：`Draft_Subtitles_List`。

**5. 质量门控 (Quality Gate) —— *逻辑分支点***

  * **逻辑**：设定阈值 `CONFIDENCE_THRESHOLD = 0.6`。
  * **分类**：
      * **A 类 (High Confidence)**：分数 \>= 0.6。直接进入下一阶段。
      * **B 类 (Low Confidence)**：分数 \< 0.6。标记为 `needs_review`，并将该段的时间轴加入 `Retry_Queue`。

**6. 声学回捞 (Acoustic Refinement) —— *串行化处理***

  * **检查**：如果 `Retry_Queue` 为空，跳过此步。
  * **操作**：
    1.  **卸载 SenseVoice** (释放显存)。
    2.  **加载 Whisper Large-v3** (或 Medium)。
    3.  **针对性重录**：只处理 `Retry_Queue` 中的时间段。
    4.  **数据更新**：将 Whisper 识别出的文本作为 `candidate_text_2` 存入字幕对象中。
    5.  **卸载 Whisper**。

-----

#### 第三阶段：语义层 - 智能校对与翻译 (可选)

*目标：利用 LLM 的“大脑”修复“耳朵”听错的内容，并进行翻译。*

**7. LLM 批处理 (The Judge & Translator)**

  * **触发条件**：用户开启“AI 校对”或“翻译”功能。
  * **策略**：**滑动窗口 (Sliding Window)**。
      * 不要一句一句发，太慢且没上下文。
      * 建议 **5 句为一个 Batch** 发送给 LLM。
  * **Prompt 构造 (核心)**：
      * **输入**：
        ```json
        [
          {"id": 1, "text": "上一句的文本..."},
          {"id": 2, "text": "SenseVoice结果", "candidate": "Whisper结果(如果有)", "flag": "low_conf"},
          {"id": 3, "text": "下一句的文本..."}
        ]
        ```
      * **指令**：
        > "你是一个字幕专家。请根据上下文修正第 2 句的 OCR 错误或同音字错误（参考 candidate）。如果 flag 存在，请重点检查。然后将第 2 句翻译成中文。返回 JSON。"
  * **执行**：LLM 返回修正后的原文和译文。

-----

#### 第四阶段：封装与导出

**8. 时间轴对齐与封装**

  * **时间轴来源**：
      * 始终优先使用 **SenseVoice** 的时间戳（因为它是原生对齐的，且不受 Whisper 幻觉影响）。
      * 除非 LLM 判定 SenseVoice 整个句子都丢了，才考虑使用 Whisper 的时间戳。
  * **格式化**：生成 SRT / ASS 文件。
  * **UI 渲染**：
      * 正常行：显示白色。
      * LLM 修正过的行：显示绿色高亮。
      * 依然存疑的行：显示黄色警告。

-----

### 📊 数据流图 (Data Flow)

```mermaid
graph TD
    Input(视频文件) --> FFmpeg[FFmpeg: 转16k WAV]
    FFmpeg --> Separation{是否分离人声?}
    
    Separation -- 是 (默认) --> Demucs[Htdemucs: 提取 Vocals]
    Separation -- 否 (极速) --> Vocals[WAV音频]
    Demucs --> Vocals
    
    Vocals --> VAD[Silero VAD: 切分片段]
    VAD --> SenseVoice[SenseVoice: 转录 + 时间戳 + 置信度]
    
    SenseVoice --> Gate{置信度检测}
    Gate -- High (>0.6) --> Draft[初稿池]
    Gate -- Low (<0.6) --> Queue[重审队列]
    
    Queue -- 队列非空 --> LoadWhisper[加载 Whisper (串行)]
    LoadWhisper --> WhisperRec[Whisper: 二次识别]
    WhisperRec --> Draft
    
    Draft --> Feature{用户需求?}
    
    Feature -- 仅转录 --> Export[生成 SRT]
    Feature -- 校对/翻译 --> BatchLLM[构造 Context Batch]
    
    BatchLLM --> LLM[LLM: 语义修正 + 翻译]
    LLM --> Merge[合并结果]
    Merge --> Export
```

### ⏱️ 性能预估 (以 10分钟视频为例)

| 步骤 | 引擎 | 耗时 (RTX 3060) | 显存占用 |
| :--- | :--- | :--- | :--- |
| **1. 预处理** | FFmpeg | \< 5秒 | CPU |
| **2. 分离** | Htdemucs | \~30秒 | \~1.5 GB |
| **3. VAD** | Silero | \< 2秒 | \< 0.5 GB |
| **4. 转录** | SenseVoice | **\< 10秒** | \< 1.0 GB |
| **5. 重审** | Whisper | \~20秒 (仅针对 5% 片段) | \~4.0 GB (串行) |
| **6. LLM** | Qwen-7B (Int4) | \~60秒 (取决于语速) | \~5.5 GB (串行) |
| **总计** | | **约 2 分钟** | **峰值 6GB (安全)** |

### 🚀 总结

这个工作流的精髓在于：

1.  **Htdemucs 替换 mdx\_extra**：解决了前处理的瓶颈。
2.  **SenseVoice 替换 Faster-Whisper**：解决了对齐不准和转录慢的瓶颈。
3.  **串行化 Whisper**：解决了显存爆炸的问题，只在必要时“召回”重武器。
4.  **LLM 融合**：一次性解决纠错和翻译，避免重复调用。

这就是目前 2025 年能做到的**最强本地字幕架构**。










1.选项C：可配置，默认完全信任频谱检测，实现频谱预判 + Demucs验证备用，要确保Demucs使用更大的模型比如mdx_extra
2.任何时刻只加载一个模型
3.保留原有的全局BGM检测方法，将其作为一个备选方案，默认不启用但留一个设置启用的接口
4.设置菜单将来会同时使用预设和高级设置，所以既要有预设又要保留所有参数的接口
5.渐进式集成，但不要分的太细，要合理分配，不需要太多测试
6.接受对核心服务的大幅重构，同上需要分阶段，不需要A/B测试









### 一、 存在的问题总结

1. **进度条逻辑僵化**：当前设计给转录阶段分配了固定的权重（如 70%）。这无法区分“仅运行 SenseVoice”和“SenseVoice + Whisper 补刀”两种场景。若开启补刀，进度条可能在 SenseVoice 跑完后停滞或回跳；若关闭补刀，进度条可能过早走完。
2. **缺乏句级切分（语义粒度缺失）**：目前设计直接将 VAD 切分的片段（可能长达 30 秒）作为一个字幕块输出。SenseVoice 虽然有字级时间戳，但缺失将这些字根据标点和停顿组合成短句的逻辑，导致用户看到的是“一大坨”文字。
3. **熔断机制割裂**：人声分离（Demucs）的升级逻辑与 ASR（Whisper）的补刀逻辑是分离的。系统可能在背景噪音极大的情况下直接让 Whisper 补刀（效果依然差），而不是先升级分离模型去除噪音。
4. **VAD 角色定位模糊**：在 SenseVoice 自带时间戳的情况下，VAD 容易被误解为多余。实际上 VAD 需要从“字幕切分者”转变为“资源调度者”。

------

### 二、 简要改进建议

#### 1. 进度条构成：动态权重策略

需根据用户配置动态调整 `config.py` 中的阶段权重：

- **场景 A：仅 SenseVoice（极速模式）**
  - **构成**：`Transcribe (SenseVoice)`: **90%** | `Format/Export`: **10%**
  - **逻辑**：SenseVoice 逐个 Batch 完成时，进度条线性增长至 90%。
- **场景 B：启用 Whisper 补刀（高精模式）**
  - **构成**：`Primary Transcribe (SenseVoice)`: **70%** | `Fallback Refine (Whisper)`: **20%** | `Format/Export`: **10%**
  - **逻辑**：
    1. SenseVoice 跑完全程，进度条到达 70%。
    2. 系统统计进入 `Retry_Queue` 的片段数量。
    3. Whisper 处理队列时，填充剩下的 20%。

#### 2. 字幕生成：贪心启发式分句 (Greedy Heuristic)

在 SenseVoice 输出结果后，**必须**插入一个后处理步骤，利用字级时间戳生成句级字幕：

- **算法逻辑**：遍历 SenseVoice 输出的字/词列表。
- **切分条件**（优先级从高到低）：
  1. **标点符号**：遇到 `。？！` 必切。
  2. **长停顿**：相邻字间隔 > 0.4s 必切。
  3. **强制长度**：单句超过 5秒 或 30个字 强制切分。
- **代码位置**：嵌入在 `_transcribe_segment_sensevoice` 返回结果之前。

#### 3. 熔断机制

详见F:\video_to_srt_gpu\docs\SenseVoice\最终方案\智能熔断机制集成改进方案.md

#### 4. VAD 重新定义与新流式机制

- **VAD 的新定义（物理切分）**：
  - **不再负责**决定字幕长相。
  - **核心职责**：
    1. **防幻觉**：过滤纯静音，防止 SenseVoice 瞎猜。
    2. **显存保护**：将长音频切为 15-30s 的 Chunk，防止 OOM。
    3. **并行加速**：为 GPU 提供 Batch 推理的原材料。
- **新流式推送机制（语义推送）**：
  - **后端处理**：VAD 切出 Chunk (15s) -> SenseVoice 转录 -> **分句算法切成 3 个短句**。
  - **SSE 推送**：后端不再推送一个大的 VAD 片段，而是**循环推送这 3 个短句**。
  - **前端表现**：用户看到字幕是按句子（2-5秒）一句句蹦出来的，而不是等了 15 秒突然出来一大段。
  
批处理策略 (Batching):
参数: batch_size_s=60 (按时长分批)。
VAD 分段设置:
参数: max_single_segment_time=30000 (30秒)。
逆文本标准化 (ITN):
参数: use_itn=True。
说明: 开启 ITN 可以自动将识别结果中的数字、时间等转换为标准格式（如 "一二三" 转为 "123"），提升字幕可读性。

重要补充：
1.Faster-Whisper 模型管理策略：SenseVoice在转录开始时加载，并在完成转录后常驻显存/内存（但提供配置项可以在完成队列中所有任务后立即卸载），直到手动关闭程序或者检测到内存/显存不足或者进入下一阶段（二次转录、翻译、校对等）时卸载。
2.当前是预加载VAD+预加载Whisper模型，现在改为启动时预加载默认人声分离模型。
3.特别注意，SenseVoice必须使用ONNX Runtime (INT8 量化)
4.如果硬件检测时未检测到gpu则默认关闭人声分离或仅在检测到强 BGM 时使用最轻量的 htdemucs 模型
