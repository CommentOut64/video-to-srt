问题 1：对齐后如何处理用户已修改的文本？

**核心原则：用户修改优先级 \> AI 修改优先级。**

如果用户在 AI 对齐完成前动手改了字，说明用户认为 AI 是错的。此时 AI 回传的“修正后文本”应该被丢弃，但 AI 回传的“修正后时间轴”通常仍然有价值（除非用户把整句话都改了）。

#### 解决方案：引入 `is_modified` (脏标记) 机制

在前端的数据结构中，为每一个 segment 增加一个状态标记。

**1. 前端数据结构设计**

```javascript
// 前端 store 中的单个片段对象
{
  id: 0,              // 唯一索引，与后端对应
  start: 10.5,        // 开始时间
  end: 15.2,          // 结束时间
  text: "原来的文字",  // 内容
  is_modified: false  // 【关键】是否被用户修改过
}
```

**2. 交互逻辑**

  * **用户编辑时**：一旦用户在输入框里敲了键盘，立即将该 segment 的 `is_modified` 设为 `true`。
  * **对齐数据到达时**：前端执行一个“智能合并”函数，而不是简单的覆盖。

**3. 智能合并算法 (Merge Logic)**

```javascript
function mergeAlignedData(currentList, alignedData) {
  return currentList.map(item => {
    // 在对齐结果中找到对应 ID 的项
    const alignedItem = alignedData.find(a => a.id === item.id);
    
    if (!alignedItem) return item;

    // 【策略核心】
    if (item.is_modified) {
      // 情况 A: 用户改过文本
      // 1. 保留用户的 text (绝不覆盖用户的劳动成果)
      // 2. 更新 start/end 时间戳 (通常 WhisperX 的时间戳更准，即使字改了，语音长度没变)
      // 3. 丢弃 alignedItem.text (因为那是基于旧识别结果的)
      return {
        ...item,
        start: alignedItem.start,
        end: alignedItem.end,
        // text: item.text  <-- 保持原样
      };
    } else {
      // 情况 B: 用户没碰过
      // 完全信任 AI，覆盖所有属性
      return {
        ...item,
        start: alignedItem.start,
        end: alignedItem.end,
        text: alignedItem.text
      };
    }
  });
}
```

#### 进阶方案（未来考虑）：后端重对齐

如果你追求完美，当 `is_modified === true` 时，除了保留文本，还可以在后台悄悄发一个请求，把用户修改后的文本发给 WhisperX 的对齐接口，针对这一小段音频**重新跑一次对齐**。这样时间轴和文字就都完美了。但在 V2.0 阶段，上述的“脏标记”方案性价比最高。

-----

### 问题 2：大数据量下的性能优化？

如果在 2 小时电影的字幕列表（可能 2000+ 行）上进行全量 DOM 更新，界面一定会卡死。

#### 解决方案：虚拟滚动 (Virtual Scrolling) + 离线计算

**1. 虚拟滚动 (必选)**

这是解决长列表性能问题的唯一正解。

  * **原理**：无论数据有多少条，浏览器只渲染**当前屏幕可见区域**（比如 20 条）以及上下缓冲区的 DOM 节点。用户滚动时，动态替换这 20 个节点的内容。
  * **技术选型**：
      * **Vue**: 使用 `vue-virtual-scroller` 或 `vue-virtual-scroll-list`。
      * **React**: 使用 `react-window` 或 `react-virtuoso`。

**2. Web Worker 处理合并 (可选，视数据量而定)**

当对齐完成，后端瞬间推过来 2000 条数据时，前端执行上面的 `mergeAlignedData` 遍历逻辑可能会占用 JS 主线程几十毫秒，导致 UI 掉帧。

  * **优化**：将 `mergeAlignedData` 逻辑放入 **Web Worker** 中执行。
  * **流程**：
    1.  后端 SSE 推送大数据。
    2.  主线程把 `currentList` 和 `newData` 丢给 Worker。
    3.  Worker 计算出最终的 `mergedList`。
    4.  Worker 把结果发回主线程。
    5.  主线程替换 Store 数据。

**3. 避免全量 Reactivity (Vue/React 特有)**

不要把所有数据都变成深层响应式（Deep Reactive）。

  * **Vue 3**：使用 `shallowRef` 来包裹这个巨大的列表。只有当整个列表被替换时才触发更新，而不是列表里某对象的某属性变了就触发。
  * **React**：使用 `useMemo` 确保只有数据引用变化时才重新渲染列表组件。

-----

### 总结：流式输出与更新的最佳实践流程

1.  **流式阶段 (Streaming)**:
      * 后端：分段转录，每转录完一段，WebSocket 推送 `{type: 'segment', data: {id: 1, text: '...', status: 'pending'}}`。
      * 前端：收到消息，`list.push(data)`。用户此时看到的是不断增长的列表。
      
2.  **用户修改阶段**:

      * 用户修改 ID 5 的文本。
      * 前端：`list[5].text = newText`; `list[5].is_modified = true`。

3.  **对齐更新阶段 (Alignment Update)**:
      * 后端：全部跑完对齐，发送 `{type: 'full_update', data: [全量已对齐数据]}`。
      * 前端：
          * (Worker线程) 读取当前 `list` 和 `new_data`。
          * (Worker线程) 遍历比对：如果 `item.is_modified`，保留 `text` 更新 `time`；否则全覆盖。
          * (主线程) `this.list = merged_result` (触发虚拟列表刷新)。
      * **视觉效果**：用户会看到所有没改过的文本瞬间变得更精准（比如标点符号变了、错别字修正了），时间轴微调了，但他自己改过的那几行字稳如泰山，依然保留着。

### 合并”尽可能快？

用户点击“导出”时，最怕转圈圈。要实现“秒级”导出，核心策略是：**不要把计算留到最后一刻（Lazy），而是预先计算（Eager）。**

#### 策略一：Eager Merge (急切合并) 

不要等到用户点击“导出”按钮时才去合并“用户编辑的文本”和“AI 对齐的时间轴”。

**在“对齐完成”事件到达的那一毫秒，立即在前端完成合并。**

1.  **流程**：
      * **T0 时间点**：SSE 推送 `{type: 'aligned_complete', data: [...]}` 到达前端。
      * **T1 时间点 (自动)**：前端立刻调用 Web Worker 或主线程执行 `mergeAlignedData(currentList, alignedData)`（就是上一轮对话中提到的那个函数）。
      * **T2 时间点**：前端 Store 中的数据被更新为“已合并的最终状态”。此时用户在界面上看到文字微调了一次。
      * **T3 时间点**：用户点击“导出 SRT”按钮。
      * **T4 时间点**：因为数据已经是现成的了，前端直接将 Store 里的 JSON 发送给后端保存，或者直接在前端生成 Blob 下载。**耗时：0ms。**

#### 策略二：前端主导导出 (Frontend-Driven Export)

既然你是在做桌面软件（Localhost 环境），网络传输开销几乎为零。不要让后端去维护“最终的文字状态”。

  * **误区**：用户修改了文字 -\> 调 API 存到后端 -\> 后端更新数据库 -\> 导出时后端生成文件。这样做太慢，且后端状态管理复杂。
  * **正解**：**前端是“真理之源” (Source of Truth)**。
      * 所有的编辑操作都只在前端内存（Vue/React Store）中发生。
      * 当用户点击“导出”时，前端把**整份**最新的 JSON 列表 POST 给后端。
      * 后端拿到 JSON，只负责一件事：**写入磁盘**。

**代码实现流程：**

1.  **前端 (Export 动作)**

<!-- end list -->

```javascript
async function exportToSRT() {
    // 1. 此时 list 已经是合并好的最终数据（由策略一保证）
    const finalData = this.subtitleList; 
    
    // 2. 直接把几千条数据发给后端
    // 本机回环传输 1MB JSON 数据只需要 <10ms
    await axios.post(`/api/jobs/${jobId}/export`, {
        format: 'srt',
        segments: finalData // 把前端看到的最终结果发回去
    });
}
```

2.  **后端 (无脑写入)**

<!-- end list -->

```python
class ExportRequest(BaseModel):
    format: str
    segments: List[dict] # 接收前端传来的最终数据

@app.post("/api/jobs/{job_id}/export")
def export_subtitles(job_id: str, req: ExportRequest):
    # 后端不需要去读数据库或 checkpoint
    # 后端完全信任前端传来的 segments
    output_path = get_job_path(job_id) / f"output.{req.format}"
    
    if req.format == 'srt':
        SubtitleExporter.to_srt(req.segments, output_path)
    # ...
    
    return {"path": str(output_path)}
```

#### 策略三：性能兜底 (针对超大文本量)

虽然两小时电影的字幕只有 2000 行左右，JS 处理起来是毫秒级的，但如果你担心性能，可以加一道保险：

  * **Web Worker 预处理**：在收到 SSE 对齐数据时，如果数据量 \> 5000 条，启动 Worker 进行合并。
  * **Diff 更新**：如果对齐结果和当前草稿差别不大（比如大部分时间戳没变），Vue/React 的虚拟列表机制会自动复用 DOM，不会卡顿。

### 总结

1.  **通信协议**：**坚决使用 SSE**。它完美契合你的“单向进度推送”需求，代码量少一半。
2.  **导出性能**：
      * **时机**：对齐数据一到，**立刻**在前端合并，不要等点击导出。
      * **数据流**：**前端主导**。导出时，前端把合并好的 JSON 扔给后端写文件。不要让后端去猜测前端改了什么，直接把结果给它即可。

这样设计，用户点击“导出”按钮时，感觉就是**瞬间完成**。