# Video-to-SRT GPU é¡¹ç›®æ¶æ„ä¸æŠ€æœ¯è¯¦è§£

> **é¡¹ç›®ç‰ˆæœ¬**: v2.0
> **æ–‡æ¡£ç”Ÿæˆæ—¶é—´**: 2025-11-17
> **é¡¹ç›®è·¯å¾„**: F:\video_to_srt_gpu

---

## ğŸ“‹ ç›®å½•

- [ä¸€ã€é¡¹ç›®æ¦‚è¿°](#ä¸€é¡¹ç›®æ¦‚è¿°)
- [äºŒã€æŠ€æœ¯æ ˆè¯¦è§£](#äºŒæŠ€æœ¯æ ˆè¯¦è§£)
- [ä¸‰ã€é¡¹ç›®æ¶æ„è®¾è®¡](#ä¸‰é¡¹ç›®æ¶æ„è®¾è®¡)
- [å››ã€ç›®å½•ç»“æ„è¯¦è§£](#å››ç›®å½•ç»“æ„è¯¦è§£)
- [äº”ã€æ ¸å¿ƒåŠŸèƒ½å®ç°](#äº”æ ¸å¿ƒåŠŸèƒ½å®ç°)
- [å…­ã€è§†é¢‘è½¬å½•å®Œæ•´æµç¨‹](#å…­è§†é¢‘è½¬å½•å®Œæ•´æµç¨‹)
- [ä¸ƒã€å‰ç«¯æŠ€æœ¯å®ç°](#ä¸ƒå‰ç«¯æŠ€æœ¯å®ç°)
- [å…«ã€åç«¯æŠ€æœ¯å®ç°](#å…«åç«¯æŠ€æœ¯å®ç°)
- [ä¹ã€æ€§èƒ½ä¼˜åŒ–æœºåˆ¶](#ä¹æ€§èƒ½ä¼˜åŒ–æœºåˆ¶)
- [åã€éƒ¨ç½²ä¸è¿ç»´](#åéƒ¨ç½²ä¸è¿ç»´)

---

## ä¸€ã€é¡¹ç›®æ¦‚è¿°

### 1.1 é¡¹ç›®ç®€ä»‹

**Video-to-SRT GPU** æ˜¯ä¸€ä¸ªåŸºäº **WhisperX** çš„é«˜æ€§èƒ½è§†é¢‘å­—å¹•ç”Ÿæˆå·¥å…·ï¼Œé‡‡ç”¨ç°ä»£åŒ–å‰åç«¯åˆ†ç¦»æ¶æ„ã€‚è¯¥é¡¹ç›®ä»ç®€å•çš„å‘½ä»¤è¡Œè„šæœ¬æ¼”è¿›ä¸ºåŠŸèƒ½å®Œå–„çš„ Web åº”ç”¨ç¨‹åºï¼Œæ”¯æŒ GPU åŠ é€Ÿã€å®æ—¶è¿›åº¦ç›‘æ§å’Œå¤šä»»åŠ¡å¹¶å‘å¤„ç†ã€‚

### 1.2 æ ¸å¿ƒç‰¹æ€§

- âœ… **å‰åç«¯åˆ†ç¦»æ¶æ„** - Vue.js 3 + FastAPIï¼Œç°ä»£åŒ– Web åº”ç”¨
- âœ… **GPU åŠ é€Ÿ** - æ”¯æŒ NVIDIA CUDAï¼Œè½¬å½•é€Ÿåº¦æå‡ 10 å€ä»¥ä¸Š
- âœ… **å®æ—¶è¿›åº¦ç›‘æ§** - è‡ªé€‚åº”è½®è¯¢æœºåˆ¶ï¼Œå®æ—¶åé¦ˆè½¬å½•è¿›åº¦
- âœ… **æ™ºèƒ½æ¨¡å‹é¢„åŠ è½½** - å¯åŠ¨æ—¶è‡ªåŠ¨é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
- âœ… **CPU äº²å’Œæ€§ä¼˜åŒ–** - ç»‘å®šç‰¹å®š CPU æ ¸å¿ƒï¼Œæå‡å¤„ç†æ•ˆç‡
- âœ… **å¹¶å‘ä»»åŠ¡å¤„ç†** - æ”¯æŒå¤šä»»åŠ¡åŒæ—¶è¿è¡Œï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æº
- âœ… **æ™ºèƒ½éŸ³é¢‘åˆ†æ®µ** - åŸºäºé™éŸ³æ£€æµ‹çš„æ™ºèƒ½åˆ†å‰²ï¼Œé¿å…å¥å­æ–­è£‚
- âœ… **å¤šæ ¼å¼æ”¯æŒ** - æ”¯æŒå‡ ä¹æ‰€æœ‰ä¸»æµè§†é¢‘/éŸ³é¢‘æ ¼å¼
- âœ… **ç”¨æˆ·å‹å¥½ç•Œé¢** - ç›´è§‚çš„ Web UIï¼Œæ‹–æ‹½ä¸Šä¼ ï¼Œå®æ—¶åé¦ˆ

### 1.3 é¡¹ç›®æ¼”è¿›å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | ä¸»è¦å˜æ›´ |
|------|------|---------|
| **v1.1.0** | 2024-06-18 | åˆå§‹ç‰ˆæœ¬ - å‘½ä»¤è¡Œç•Œé¢ï¼ŒåŸºç¡€è½¬å½•åŠŸèƒ½ |
| **v2.0.0** | 2025-08-18 | é‡å¤§æ¶æ„å‡çº§ - å‰åç«¯åˆ†ç¦»ã€Web UIã€æ¨¡å‹é¢„åŠ è½½ã€å¹¶å‘å¤„ç†ã€æ€§èƒ½ä¼˜åŒ– |

---

## äºŒã€æŠ€æœ¯æ ˆè¯¦è§£

### 2.1 åç«¯æŠ€æœ¯æ ˆ

| æŠ€æœ¯/åº“ | ç‰ˆæœ¬ | ç”¨é€” | è¯´æ˜ |
|---------|------|------|------|
| **Python** | 3.10+ | æ ¸å¿ƒè¯­è¨€ | ä¸»è¦å¼€å‘è¯­è¨€ |
| **FastAPI** | æœ€æ–° | Web æ¡†æ¶ | é«˜æ€§èƒ½å¼‚æ­¥ API æ¡†æ¶ï¼Œè‡ªåŠ¨ç”Ÿæˆ OpenAPI æ–‡æ¡£ |
| **Uvicorn** | æœ€æ–° | ASGI æœåŠ¡å™¨ | ç”Ÿäº§çº§å¼‚æ­¥æœåŠ¡å™¨ |
| **WhisperX** | 3.3.4 | è¯­éŸ³è¯†åˆ«æ ¸å¿ƒ | ä¼˜åŒ–ç‰ˆ OpenAI Whisperï¼Œæ”¯æŒè¯çº§å¯¹é½ |
| **PyTorch** | 2.5.1 (CUDA 11.8) | æ·±åº¦å­¦ä¹ æ¡†æ¶ | GPU åŠ é€Ÿæ¨ç† |
| **faster-whisper** | 1.1.1 | Whisper ä¼˜åŒ–å®ç° | CTranslate2 åŠ é€Ÿæ¨ç† |
| **pyannote.audio** | 3.3.2 | è¯´è¯äººåˆ†ç¦» | æä¾›å¯¹é½æ¨¡å‹ |
| **pydub** | æœ€æ–° | éŸ³é¢‘å¤„ç† | éŸ³é¢‘åˆ†æ®µå’Œé™éŸ³æ£€æµ‹ |
| **psutil** | æœ€æ–° | ç³»ç»Ÿç›‘æ§ | ç¡¬ä»¶æ£€æµ‹å’Œ CPU äº²å’Œæ€§ç®¡ç† |
| **FFmpeg** | ç³»ç»Ÿå®‰è£… | éŸ³è§†é¢‘å¤„ç† | æå–éŸ³é¢‘æµ |

### 2.2 å‰ç«¯æŠ€æœ¯æ ˆ

| æŠ€æœ¯/åº“ | ç‰ˆæœ¬ | ç”¨é€” | è¯´æ˜ |
|---------|------|------|------|
| **Vue.js** | 3.4.0 | å‰ç«¯æ¡†æ¶ | ä½¿ç”¨ Composition API (`<script setup>`) |
| **Vite** | 5.2.0 | æ„å»ºå·¥å…· | å¿«é€Ÿçƒ­æ›´æ–°ï¼ŒES æ¨¡å—åŸç”Ÿæ”¯æŒ |
| **Element Plus** | 2.11.1 | UI ç»„ä»¶åº“ | åŸºäº Vue 3 çš„ç»„ä»¶åº“ |
| **Axios** | 1.7.0 | HTTP å®¢æˆ·ç«¯ | API è¯·æ±‚å°è£… |
| **Node.js** | 16+ | JavaScript è¿è¡Œç¯å¢ƒ | å‰ç«¯æ„å»ºç¯å¢ƒ |

### 2.3 AI æ¨¡å‹

| æ¨¡å‹ | å¤§å° | å‚æ•°é‡ | æ¨èæ˜¾å­˜ | ç”¨é€” |
|------|------|---------|----------|------|
| **tiny** | 39 MB | 39M | 1 GB | å¿«é€Ÿè½¬å½•ï¼Œç²¾åº¦è¾ƒä½ |
| **base** | 74 MB | 74M | 1 GB | å¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦ |
| **small** | 244 MB | 244M | 2 GB | æ—¥å¸¸ä½¿ç”¨æ¨è |
| **medium** | 769 MB | 769M | 5 GB | é«˜ç²¾åº¦è½¬å½• |
| **large** | 1550 MB | 1550M | 10 GB | æœ€é«˜ç²¾åº¦ |
| **large-v2** | 1550 MB | 1550M | 10 GB | ä¼˜åŒ–çš„å¤§æ¨¡å‹ |
| **large-v3** | 1550 MB | 1550M | 10 GB | æœ€æ–°å¤§æ¨¡å‹ |

---

## ä¸‰ã€é¡¹ç›®æ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„

**æ¶æ„æ¨¡å¼**: å‰åç«¯åˆ†ç¦» + å¾®æœåŠ¡åŒ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æµè§ˆå™¨å®¢æˆ·ç«¯ (Browser)                      â”‚
â”‚                                                               â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚    â”‚         Vue.js 3 å‰ç«¯åº”ç”¨ (Port: 5174)             â”‚   â”‚
â”‚    â”‚                                                    â”‚   â”‚
â”‚    â”‚  - æ–‡ä»¶é€‰æ‹©/ä¸Šä¼                                      â”‚   â”‚
â”‚    â”‚  - å‚æ•°é…ç½®                                         â”‚   â”‚
â”‚    â”‚  - å®æ—¶è¿›åº¦ç›‘æ§                                      â”‚   â”‚
â”‚    â”‚  - ç»“æœä¸‹è½½                                         â”‚   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                         â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ HTTP/RESTful API
                          â”‚ (JSON æ•°æ®äº¤æ¢)
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FastAPI åç«¯æœåŠ¡å™¨ (Port: 8000)                     â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              API è·¯ç”±å±‚ (Routes Layer)                  â”‚ â”‚
â”‚  â”‚  - transcription_routes.py (è½¬å½•ä»»åŠ¡ç®¡ç†)               â”‚ â”‚
â”‚  â”‚  - file_routes.py (æ–‡ä»¶ç®¡ç†)                           â”‚ â”‚
â”‚  â”‚  - hardware_routes.py (ç¡¬ä»¶æ£€æµ‹)                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            ä¸šåŠ¡æœåŠ¡å±‚ (Services Layer)                  â”‚ â”‚
â”‚  â”‚  - TranscriptionService (è½¬å½•ä¸šåŠ¡é€»è¾‘)                 â”‚ â”‚
â”‚  â”‚  - FileService (æ–‡ä»¶æ“ä½œ)                             â”‚ â”‚
â”‚  â”‚  - HardwareService (ç¡¬ä»¶æ£€æµ‹)                         â”‚ â”‚
â”‚  â”‚  - ModelPreloadManager (æ¨¡å‹ç®¡ç†)                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           æ ¸å¿ƒå¤„ç†å±‚ (Core Processing)                  â”‚ â”‚
â”‚  â”‚  - TranscriptionProcessor (è½¬å½•å¤„ç†å™¨)                 â”‚ â”‚
â”‚  â”‚  - CPUAffinityManager (CPU äº²å’Œæ€§ç®¡ç†)                â”‚ â”‚
â”‚  â”‚  - MemoryMonitor (å†…å­˜ç›‘æ§)                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                   â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              AI æ¨¡å‹å±‚ (AI Models)                      â”‚ â”‚
â”‚  â”‚  - WhisperX Models (è¯­éŸ³è¯†åˆ«æ¨¡å‹)                      â”‚ â”‚
â”‚  â”‚  - Align Models (è¯çº§å¯¹é½æ¨¡å‹)                         â”‚ â”‚
â”‚  â”‚  - Model Cache & LRU ç¼“å­˜                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  GPU / CUDA      â”‚
                â”‚  (ç¡¬ä»¶åŠ é€Ÿå±‚)     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 æ•°æ®æµå‘

```
ç”¨æˆ·æ“ä½œ â†’ å‰ç«¯ Vue ç»„ä»¶ â†’ API Service å±‚ â†’ HTTP è¯·æ±‚
    â†’ FastAPI è·¯ç”± â†’ ä¸šåŠ¡æœåŠ¡ â†’ æ ¸å¿ƒå¤„ç†å™¨
    â†’ AI æ¨¡å‹æ¨ç† â†’ GPU åŠ é€Ÿè®¡ç®—
    â†’ ç»“æœè¿”å› â†’ çŠ¶æ€æ›´æ–° â†’ å‰ç«¯è½®è¯¢è·å– â†’ ç•Œé¢å±•ç¤º
```

### 3.3 æ¨¡å—ä¾èµ–å…³ç³»

```
main.py (åº”ç”¨å…¥å£)
  â”‚
  â”œâ”€> api/routes/ (è·¯ç”±æ¨¡å—)
  â”‚     â”œâ”€> transcription_routes.py
  â”‚     â”œâ”€> file_routes.py
  â”‚     â””â”€> hardware_routes.py
  â”‚
  â”œâ”€> services/ (æœåŠ¡æ¨¡å—)
  â”‚     â”œâ”€> transcription_service.py
  â”‚     â”œâ”€> file_service.py
  â”‚     â”œâ”€> hardware_service.py
  â”‚     â””â”€> model_preload_manager.py
  â”‚
  â”œâ”€> models/ (æ•°æ®æ¨¡å‹)
  â”‚     â”œâ”€> job_models.py
  â”‚     â””â”€> hardware_models.py
  â”‚
  â””â”€> processor.py (æ ¸å¿ƒå¤„ç†å™¨)
        â”œâ”€> TranscriptionProcessor
        â””â”€> CPUAffinityManager
```

---

## å››ã€ç›®å½•ç»“æ„è¯¦è§£

```
F:/video_to_srt_gpu/
â”‚
â”œâ”€â”€ frontend/                          # Vue.js å‰ç«¯åº”ç”¨
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/                # Vue ç»„ä»¶
â”‚   â”‚   â”‚   â”œâ”€â”€ file-management/       # æ–‡ä»¶ç®¡ç†ç»„ä»¶
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ FileSelector.vue   # æ–‡ä»¶é€‰æ‹©å™¨ï¼ˆåˆ—è¡¨/ä¸Šä¼ ï¼‰
â”‚   â”‚   â”‚   â”œâ”€â”€ hardware/              # ç¡¬ä»¶çŠ¶æ€ç»„ä»¶
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ HardwareDialog.vue # ç¡¬ä»¶ä¿¡æ¯å¯¹è¯æ¡†
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ HardwareStatus.vue # ç¡¬ä»¶çŠ¶æ€å¡ç‰‡
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ HardwareStatusBar.vue # ç¡¬ä»¶çŠ¶æ€æ 
â”‚   â”‚   â”‚   â”œâ”€â”€ models/                # æ¨¡å‹ç®¡ç†ç»„ä»¶
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ModelPreloadStatus.vue # æ¨¡å‹é¢„åŠ è½½çŠ¶æ€
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ModelStatusButton.vue  # æ¨¡å‹çŠ¶æ€æŒ‰é’®
â”‚   â”‚   â”‚   â””â”€â”€ transcription/         # è½¬å½•åŠŸèƒ½ç»„ä»¶
â”‚   â”‚   â”‚       â”œâ”€â”€ ProgressDisplay.vue    # è¿›åº¦æ˜¾ç¤º
â”‚   â”‚   â”‚       â””â”€â”€ TranscriptionSettings.vue # å‚æ•°è®¾ç½®
â”‚   â”‚   â”œâ”€â”€ services/                  # API æœåŠ¡å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ api.js                 # Axios åŸºç¡€é…ç½®
â”‚   â”‚   â”‚   â”œâ”€â”€ fileService.js         # æ–‡ä»¶æœåŠ¡ API
â”‚   â”‚   â”‚   â”œâ”€â”€ hardwareService.js     # ç¡¬ä»¶æœåŠ¡ API
â”‚   â”‚   â”‚   â””â”€â”€ transcriptionService.js # è½¬å½•æœåŠ¡ API
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ helpers.js             # å·¥å…·å‡½æ•°ï¼ˆæ ¼å¼åŒ–ã€æ˜ å°„ç­‰ï¼‰
â”‚   â”‚   â”œâ”€â”€ App.vue                    # ä¸»åº”ç”¨ç»„ä»¶ï¼ˆçŠ¶æ€ä¸­å¿ƒï¼‰
â”‚   â”‚   â”œâ”€â”€ main.js                    # åº”ç”¨å…¥å£ï¼ˆæŒ‚è½½ Vueï¼‰
â”‚   â”‚   â””â”€â”€ style.css                  # å…¨å±€æ ·å¼
â”‚   â”œâ”€â”€ index.html                     # HTML æ¨¡æ¿
â”‚   â”œâ”€â”€ package.json                   # npm ä¾èµ–é…ç½®
â”‚   â”œâ”€â”€ vite.config.js                 # Vite æ„å»ºé…ç½®ï¼ˆå«ä»£ç†ï¼‰
â”‚   â””â”€â”€ node_modules/                  # å‰ç«¯ä¾èµ–åŒ…
â”‚
â”œâ”€â”€ backend/                           # FastAPI åç«¯æœåŠ¡
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ api/
â”‚       â”‚   â””â”€â”€ routes/                # API è·¯ç”±
â”‚       â”‚       â”œâ”€â”€ file_routes.py             # æ–‡ä»¶ç®¡ç† API
â”‚       â”‚       â”œâ”€â”€ hardware_routes.py         # ç¡¬ä»¶æ£€æµ‹ API
â”‚       â”‚       â””â”€â”€ transcription_routes.py    # è½¬å½•ä»»åŠ¡ API
â”‚       â”œâ”€â”€ config/                    # é…ç½®æ¨¡å—
â”‚       â”‚   â””â”€â”€ model_config.py        # æ¨¡å‹é…ç½®ï¼ˆé»˜è®¤æ¨¡å‹åˆ—è¡¨ç­‰ï¼‰
â”‚       â”œâ”€â”€ models/                    # æ•°æ®æ¨¡å‹ï¼ˆPydantic/Dataclassï¼‰
â”‚       â”‚   â”œâ”€â”€ hardware_models.py     # ç¡¬ä»¶ä¿¡æ¯æ¨¡å‹
â”‚       â”‚   â””â”€â”€ job_models.py          # ä»»åŠ¡çŠ¶æ€æ¨¡å‹
â”‚       â”œâ”€â”€ services/                  # ä¸šåŠ¡æœåŠ¡å±‚
â”‚       â”‚   â”œâ”€â”€ file_service.py        # æ–‡ä»¶æ“ä½œæœåŠ¡
â”‚       â”‚   â”œâ”€â”€ hardware_service.py    # ç¡¬ä»¶æ£€æµ‹æœåŠ¡
â”‚       â”‚   â”œâ”€â”€ model_preload_manager.py # æ¨¡å‹é¢„åŠ è½½ç®¡ç†å™¨
â”‚       â”‚   â””â”€â”€ transcription_service.py     # è½¬å½•ä¸šåŠ¡æœåŠ¡
â”‚       â”œâ”€â”€ main.py                    # FastAPI åº”ç”¨å…¥å£
â”‚       â””â”€â”€ processor.py               # æ ¸å¿ƒè½¬å½•å¤„ç†å™¨
â”‚
â”œâ”€â”€ input/                             # è¾“å…¥æ–‡ä»¶ç›®å½•ï¼ˆå­˜æ”¾å¾…è½¬å½•è§†é¢‘/éŸ³é¢‘ï¼‰
â”œâ”€â”€ output/                            # è¾“å‡ºæ–‡ä»¶ç›®å½•ï¼ˆå­˜æ”¾ç”Ÿæˆçš„å­—å¹•ï¼‰
â”œâ”€â”€ jobs/                              # ä»»åŠ¡å·¥ä½œç›®å½•ï¼ˆæ¯ä¸ªä»»åŠ¡ç‹¬ç«‹å­ç›®å½•ï¼‰
â”‚   â””â”€â”€ {job_id}/                      # ä»»åŠ¡éš”ç¦»ç›®å½•
â”‚       â”œâ”€â”€ {original_file}            # åŸå§‹æ–‡ä»¶å‰¯æœ¬
â”‚       â”œâ”€â”€ audio.wav                  # æå–çš„éŸ³é¢‘
â”‚       â”œâ”€â”€ segment_0.wav              # éŸ³é¢‘åˆ†æ®µæ–‡ä»¶
â”‚       â”œâ”€â”€ segment_1.wav
â”‚       â””â”€â”€ {filename}.srt             # ç”Ÿæˆçš„å­—å¹•æ–‡ä»¶
â”œâ”€â”€ temp/                              # ä¸´æ—¶æ–‡ä»¶ç›®å½•
â”‚
â”œâ”€â”€ simple_launcher.py                 # ç®€åŒ–å¯åŠ¨å™¨ï¼ˆæ¨èï¼‰
â”œâ”€â”€ launcher_debug.py                  # è°ƒè¯•å¯åŠ¨å™¨
â”œâ”€â”€ ä¸€é”®å¯åŠ¨_ç®€åŒ–ç‰ˆ.bat                 # Windows æ‰¹å¤„ç†å¯åŠ¨å™¨
â”œâ”€â”€ æ¸…ç†è¿›ç¨‹.bat                        # è¿›ç¨‹æ¸…ç†å·¥å…·
â”‚
â”œâ”€â”€ requirements.txt                   # Python ä¾èµ–é…ç½®
â”œâ”€â”€ README.md                          # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ CHANGELOG.md                       # ç‰ˆæœ¬å˜æ›´æ—¥å¿—
â”œâ”€â”€ REFACTOR_SUMMARY.md               # é‡æ„æ€»ç»“æ–‡æ¡£
â”œâ”€â”€ LICENSE                            # MIT å¼€æºåè®®
â”œâ”€â”€ .gitignore                         # Git å¿½ç•¥é…ç½®
â””â”€â”€ video_to_srt_old.py               # æ—§ç‰ˆå•æ–‡ä»¶è„šæœ¬ï¼ˆå·²åºŸå¼ƒï¼‰
```

---

## äº”ã€æ ¸å¿ƒåŠŸèƒ½å®ç°

### 5.1 åŠŸèƒ½æ¸…å•

| åŠŸèƒ½æ¨¡å— | åŠŸèƒ½æè¿° | å®ç°ä½ç½® |
|---------|---------|---------|
| **æ–‡ä»¶ç®¡ç†** | æ–‡ä»¶ä¸Šä¼ ã€åˆ—è¡¨å±•ç¤ºã€åˆ é™¤ | `frontend/src/components/file-management/FileSelector.vue` |
| **ä»»åŠ¡åˆ›å»º** | ä¸ºæœ¬åœ°/ä¸Šä¼ æ–‡ä»¶åˆ›å»ºè½¬å½•ä»»åŠ¡ | `backend/app/processor.py:create_job()` |
| **ä»»åŠ¡å¯åŠ¨** | å¯åŠ¨è½¬å½•å¤„ç†æµç¨‹ | `backend/app/processor.py:start_job()` |
| **ä»»åŠ¡å–æ¶ˆ** | å–æ¶ˆæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ | `backend/app/processor.py:cancel_job()` |
| **è¿›åº¦ç›‘æ§** | å®æ—¶æŸ¥è¯¢ä»»åŠ¡è¿›åº¦å’ŒçŠ¶æ€ | å‰ç«¯è½®è¯¢ + åç«¯çŠ¶æ€ç®¡ç† |
| **ç»“æœä¸‹è½½** | ä¸‹è½½ç”Ÿæˆçš„ SRT å­—å¹•æ–‡ä»¶ | `GET /api/download/{job_id}` |
| **ç»“æœå¤åˆ¶** | å¤åˆ¶å­—å¹•åˆ°è§†é¢‘æºç›®å½• | `POST /api/copy-result/{job_id}` |
| **ç¡¬ä»¶æ£€æµ‹** | æ£€æµ‹ CPUã€GPUã€å†…å­˜ç­‰ä¿¡æ¯ | `backend/app/services/hardware_service.py` |
| **æ¨¡å‹é¢„åŠ è½½** | å¯åŠ¨æ—¶é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹ | `backend/app/services/model_preload_manager.py` |
| **ç¼“å­˜ç®¡ç†** | LRU æ¨¡å‹ç¼“å­˜ï¼Œè‡ªåŠ¨é©±é€ | `ModelPreloadManager._evict_lru_model()` |
| **CPU äº²å’Œæ€§** | ç»‘å®šç‰¹å®š CPU æ ¸å¿ƒ | `backend/app/processor.py:CPUAffinityManager` |
| **ä¼˜åŒ–å»ºè®®** | æ ¹æ®ç¡¬ä»¶æ¨èæœ€ä¼˜é…ç½® | `backend/app/services/hardware_service.py:CoreOptimizer` |

### 5.2 æ”¯æŒçš„æ–‡ä»¶æ ¼å¼

#### è¾“å…¥æ ¼å¼

**è§†é¢‘æ ¼å¼**ï¼ˆ15 ç§ï¼‰:
```
.mp4, .avi, .mkv, .mov, .wmv, .flv, .webm, .m4v, .mpg, .mpeg, .3gp, .ts, .vob, .ogv, .divx
```

**éŸ³é¢‘æ ¼å¼**ï¼ˆ9 ç§ï¼‰:
```
.mp3, .wav, .flac, .aac, .ogg, .m4a, .wma, .opus, .ape
```

#### è¾“å‡ºæ ¼å¼

**å­—å¹•æ ¼å¼**: SRT (SubRip)
- æ ‡å‡†æ—¶é—´ç æ ¼å¼: `HH:MM:SS,mmm`
- æ”¯æŒå¥å­çº§åˆ«æ—¶é—´æˆ³ï¼ˆé»˜è®¤ï¼‰
- æ”¯æŒè¯çº§åˆ«æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰

---

## å…­ã€è§†é¢‘è½¬å½•å®Œæ•´æµç¨‹

### 6.1 ç«¯åˆ°ç«¯æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. å‰ç«¯: POST /api/upload    â”‚
â”‚    - æ–‡ä»¶ç±»å‹éªŒè¯             â”‚
â”‚    - FormData ä¸Šä¼            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. åç«¯: ä¿å­˜æ–‡ä»¶åˆ° input/   â”‚
â”‚    - è‡ªåŠ¨é‡å‘½åå¤„ç†åŒåæ–‡ä»¶    â”‚
â”‚    - åˆ›å»º Job å¯¹è±¡           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. å‰ç«¯: ç”¨æˆ·é…ç½®å‚æ•°         â”‚
â”‚    - é€‰æ‹©æ¨¡å‹                â”‚
â”‚    - è®¾ç½®è®¾å¤‡ (CUDA/CPU)     â”‚
â”‚    - é…ç½®æ‰¹å¤§å°              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. å‰ç«¯: POST /api/start     â”‚
â”‚    - å‘é€ job_id å’Œè®¾ç½®      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. åç«¯: å¯åŠ¨è½¬å½•çº¿ç¨‹                      â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚    â”‚ é˜¶æ®µ 1: æå–éŸ³é¢‘ (5% æƒé‡)          â”‚â”‚
â”‚    â”‚  - FFmpeg æå–                     â”‚â”‚
â”‚    â”‚  - æ ¼å¼: å•å£°é“, 16kHz, WAV        â”‚â”‚
â”‚    â”‚  - å‘½ä»¤: ffmpeg -i input -ac 1 ... â”‚â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚             â”‚                             â”‚
â”‚             â–¼                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚    â”‚ é˜¶æ®µ 2: æ™ºèƒ½åˆ†æ®µ (10% æƒé‡)         â”‚â”‚
â”‚    â”‚  - 60ç§’åŸºç¡€åˆ†æ®µ                     â”‚â”‚
â”‚    â”‚  - é™éŸ³æ£€æµ‹ä¼˜åŒ–åˆ†å‰²ç‚¹                â”‚â”‚
â”‚    â”‚  - é¿å…å¥å­ä¸­é—´æ–­è£‚                  â”‚â”‚
â”‚    â”‚  - ç”Ÿæˆ segment_N.wav              â”‚â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚             â”‚                             â”‚
â”‚             â–¼                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚    â”‚ é˜¶æ®µ 3: è½¬å½•å¤„ç† (80% æƒé‡)         â”‚â”‚
â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚    â”‚  â”‚ 3.1 åŠ è½½ Whisper æ¨¡å‹        â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - ä»ç¼“å­˜è·å–æˆ–æ–°å»º           â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - GPU é¢„çƒ­ï¼ˆç©ºè·‘ä¸€æ¬¡ï¼‰       â”‚ â”‚â”‚
â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚    â”‚             â”‚                      â”‚â”‚
â”‚    â”‚             â–¼                      â”‚â”‚
â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚    â”‚  â”‚ 3.2 é€æ®µè½¬å½•                 â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - è°ƒç”¨ model.transcribe()   â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - è‡ªåŠ¨è¯­è¨€æ£€æµ‹               â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - è·å–åˆæ­¥è½¬å½•ç»“æœ           â”‚ â”‚â”‚
â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚    â”‚             â”‚                      â”‚â”‚
â”‚    â”‚             â–¼                      â”‚â”‚
â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚    â”‚  â”‚ 3.3 è¯çº§å¯¹é½                 â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - åŠ è½½ Align æ¨¡å‹           â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - whisperx.align()          â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - ç²¾ç¡®æ ¡å‡†æ—¶é—´æˆ³             â”‚ â”‚â”‚
â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚    â”‚             â”‚                      â”‚â”‚
â”‚    â”‚             â–¼                      â”‚â”‚
â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚    â”‚  â”‚ 3.4 æ—¶é—´åç§»æ ¡æ­£              â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - åŠ ä¸Šæ®µèµ·å§‹åç§»é‡           â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - start += seg_start_offset â”‚ â”‚â”‚
â”‚    â”‚  â”‚  - end += seg_start_offset   â”‚ â”‚â”‚
â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚             â”‚                             â”‚
â”‚             â–¼                             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚    â”‚ é˜¶æ®µ 4: ç”Ÿæˆ SRT (5% æƒé‡)          â”‚â”‚
â”‚    â”‚  - åˆå¹¶æ‰€æœ‰æ®µçš„ç»“æœ                  â”‚â”‚
â”‚    â”‚  - æ ¼å¼åŒ–æ—¶é—´æˆ³                     â”‚â”‚
â”‚    â”‚  - å†™å…¥ .srt æ–‡ä»¶                   â”‚â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. å‰ç«¯: è½®è¯¢çŠ¶æ€             â”‚
â”‚    - æ¯ 1.5s è¯·æ±‚ä¸€æ¬¡         â”‚
â”‚    - GET /api/status/{id}    â”‚
â”‚    - æ˜¾ç¤ºè¿›åº¦å’Œé˜¶æ®µ           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. ä»»åŠ¡å®Œæˆ                  â”‚
â”‚    - status: "finished"      â”‚
â”‚    - åœæ­¢è½®è¯¢                â”‚
â”‚    - æ˜¾ç¤ºä¸‹è½½æŒ‰é’®             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. ç”¨æˆ·ä¸‹è½½å­—å¹•               â”‚
â”‚    - GET /api/download/{id}  â”‚
â”‚    - æµè§ˆå™¨ä¸‹è½½ .srt æ–‡ä»¶     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 æ ¸å¿ƒç®—æ³•è¯¦è§£

#### 6.2.1 éŸ³é¢‘æå–

**ä½ç½®**: `backend/app/processor.py:379-384`

```python
def _extract_audio(self, input_file: str, audio_out: str) -> bool:
    """ä½¿ç”¨ FFmpeg æå–éŸ³é¢‘"""
    # å‚æ•°è¯´æ˜:
    # -y: è¦†ç›–è¾“å‡ºæ–‡ä»¶
    # -i: è¾“å…¥æ–‡ä»¶
    # -vn: ä¸åŒ…å«è§†é¢‘æµ
    # -ac 1: å•å£°é“ï¼ˆWhisper è¦æ±‚ï¼‰
    # -ar 16000: é‡‡æ ·ç‡ 16kHzï¼ˆWhisper è¦æ±‚ï¼‰
    # -acodec pcm_s16le: PCM 16ä½ç¼–ç 

    cmd = [
        'ffmpeg', '-y', '-i', input_file,
        '-vn',                    # ä»…éŸ³é¢‘
        '-ac', '1',               # å•å£°é“
        '-ar', '16000',           # 16kHz é‡‡æ ·ç‡
        '-acodec', 'pcm_s16le',   # PCM ç¼–ç 
        audio_out
    ]

    proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return proc.returncode == 0 and os.path.exists(audio_out)
```

#### 6.2.2 æ™ºèƒ½éŸ³é¢‘åˆ†æ®µ

**ä½ç½®**: `backend/app/processor.py:386-414`

```python
def _split_audio(self, audio_path: str) -> List[Dict]:
    """åŸºäºé™éŸ³æ£€æµ‹çš„æ™ºèƒ½åˆ†æ®µ"""

    # é…ç½®å‚æ•°
    SEGMENT_LEN_MS = 60_000         # 60ç§’åŸºç¡€åˆ†æ®µé•¿åº¦
    SILENCE_SEARCH_MS = 2_000       # åœ¨åˆ†å‰²ç‚¹å‰ 2 ç§’æœç´¢é™éŸ³
    MIN_SILENCE_LEN_MS = 300        # æœ€å°é™éŸ³é•¿åº¦ 300ms
    SILENCE_THRESH_DBFS = -40       # é™éŸ³é˜ˆå€¼ -40dB

    audio = AudioSegment.from_wav(audio_path)
    length = len(audio)
    segments = []
    pos = 0
    idx = 0

    while pos < length:
        end = min(pos + SEGMENT_LEN_MS, length)

        # é™éŸ³æœç´¢ä¼˜åŒ–
        if end < length and (end - pos) > SILENCE_SEARCH_MS:
            search_start = max(pos, end - SILENCE_SEARCH_MS)
            search_chunk = audio[search_start:end]

            try:
                # æ£€æµ‹é™éŸ³ç‚¹
                silences = silence.detect_silence(
                    search_chunk,
                    min_silence_len=MIN_SILENCE_LEN_MS,
                    silence_thresh=SILENCE_THRESH_DBFS
                )

                if silences:
                    # åœ¨ç¬¬ä¸€ä¸ªé™éŸ³ç‚¹åˆ†å‰²
                    silence_start = silences[0][0]
                    new_end = search_start + silence_start
                    if new_end - pos > MIN_SILENCE_LEN_MS:
                        end = new_end  # ä¼˜åŒ–åˆ†å‰²ç‚¹
            except Exception:
                pass  # é™éŸ³æ£€æµ‹å¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤åˆ†å‰²ç‚¹

        # å¯¼å‡ºåˆ†æ®µ
        chunk = audio[pos:end]
        seg_file = os.path.join(os.path.dirname(audio_path), f'segment_{idx}.wav')
        chunk.export(seg_file, format='wav')

        segments.append({
            'file': seg_file,
            'start_ms': pos  # è®°å½•èµ·å§‹ä½ç½®ï¼ˆç”¨äºåç»­æ—¶é—´åç§»æ ¡æ­£ï¼‰
        })

        pos = end
        idx += 1

    return segments
```

**ä¼˜åŒ–æ•ˆæœ**:
- âœ… é¿å…åœ¨è¯´è¯ä¸­é—´åˆ†å‰²ï¼Œä¿æŒå¥å­å®Œæ•´æ€§
- âœ… æé«˜è½¬å½•è´¨é‡å’Œå­—å¹•è¿è´¯æ€§
- âœ… å‡å°‘æ—¶é—´æˆ³å¯¹é½é”™è¯¯

#### 6.2.3 åˆ†æ®µè½¬å½•ä¸å¯¹é½

**ä½ç½®**: `backend/app/processor.py:455-485`

```python
def _transcribe_segment(self, seg: Dict, model, job: JobState, align_cache: Dict):
    """å•ä¸ªéŸ³é¢‘æ®µçš„è½¬å½•å’Œå¯¹é½"""

    # 1. åŠ è½½éŸ³é¢‘
    audio = whisperx.load_audio(seg['file'])

    # 2. Whisper è½¬å½•ï¼ˆåˆæ­¥è¯†åˆ«ï¼‰
    rs = model.transcribe(
        audio,
        batch_size=job.settings.batch_size,
        verbose=False,
        language=job.language  # å¦‚æœå·²æ£€æµ‹åˆ°è¯­è¨€åˆ™ä½¿ç”¨
    )

    if not rs or 'segments' not in rs:
        return None

    # 3. è¯­è¨€æ£€æµ‹ï¼ˆé¦–æ¬¡è½¬å½•æ—¶ï¼‰
    if not job.language and 'language' in rs:
        job.language = rs['language']

    lang = job.language or rs.get('language')

    # 4. åŠ è½½å¯¹é½æ¨¡å‹ï¼ˆè¯çº§å¯¹é½ï¼‰
    if lang not in align_cache:
        am, meta = self._get_align_model(lang, job.settings.device)
        align_cache[lang] = (am, meta)

    am, meta = align_cache[lang]

    # 5. è¯çº§å¯¹é½ï¼ˆç²¾ç¡®æ—¶é—´æˆ³ï¼‰
    aligned = whisperx.align(
        rs['segments'],  # åˆæ­¥è½¬å½•ç»“æœ
        am,              # å¯¹é½æ¨¡å‹
        meta,            # æ¨¡å‹å…ƒæ•°æ®
        audio,           # åŸå§‹éŸ³é¢‘
        job.settings.device
    )

    # 6. æ—¶é—´åç§»æ ¡æ­£ï¼ˆå…³é”®ï¼ï¼‰
    # å› ä¸ºæ¯ä¸ªæ®µéƒ½æ˜¯ä» 0 ç§’å¼€å§‹ï¼Œéœ€è¦åŠ ä¸Šæ®µåœ¨æ•´ä¸ªéŸ³é¢‘ä¸­çš„èµ·å§‹åç§»
    start_offset = seg['start_ms'] / 1000.0  # æ¯«ç§’è½¬ç§’
    final = {'segments': []}

    if 'segments' in aligned:
        for s in aligned['segments']:
            if 'start' in s:
                s['start'] += start_offset
            if 'end' in s:
                s['end'] += start_offset
            final['segments'].append(s)

    # è¯çº§æ—¶é—´æˆ³ä¹Ÿéœ€è¦æ ¡æ­£
    if 'word_segments' in aligned:
        final['word_segments'] = []
        for w in aligned['word_segments']:
            if 'start' in w:
                w['start'] += start_offset
            if 'end' in w:
                w['end'] += start_offset
            final['word_segments'].append(w)

    del audio  # é‡Šæ”¾éŸ³é¢‘å†…å­˜
    return final
```

#### 6.2.4 SRT å­—å¹•ç”Ÿæˆ

**ä½ç½®**: `backend/app/processor.py:498-524`

```python
def _generate_srt(self, results: List[Dict], path: str, word_level: bool):
    """ç”Ÿæˆ SRT æ ¼å¼å­—å¹•æ–‡ä»¶"""

    lines = []
    n = 1  # å­—å¹•åºå·

    for r in results:
        if not r:
            continue

        entries = []

        # è¯çº§æ—¶é—´æˆ³æ¨¡å¼
        if word_level and r.get('word_segments'):
            for w in r['word_segments']:
                if w.get('start') is not None and w.get('end') is not None:
                    txt = (w.get('word') or '').strip()
                    if txt:
                        entries.append({
                            'start': w['start'],
                            'end': w['end'],
                            'text': txt
                        })

        # å¥å­çº§æ—¶é—´æˆ³æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        elif r.get('segments'):
            for s in r['segments']:
                if s.get('start') is not None and s.get('end') is not None:
                    txt = (s.get('text') or '').strip()
                    if txt:
                        entries.append({
                            'start': s['start'],
                            'end': s['end'],
                            'text': txt
                        })

        # å†™å…¥ SRT æ ¼å¼
        for e in entries:
            if e['end'] <= e['start']:
                continue  # è·³è¿‡æ— æ•ˆæ—¶é—´æˆ³

            lines.append(str(n))  # åºå·
            lines.append(f"{self._format_ts(e['start'])} --> {self._format_ts(e['end'])}")  # æ—¶é—´æˆ³
            lines.append(e['text'])  # å­—å¹•æ–‡æœ¬
            lines.append("")  # ç©ºè¡Œ
            n += 1

    # å†™å…¥æ–‡ä»¶
    with open(path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

def _format_ts(self, sec: float) -> str:
    """æ ¼å¼åŒ–æ—¶é—´æˆ³ä¸º SRT æ ¼å¼: HH:MM:SS,mmm"""
    if sec < 0:
        sec = 0

    ms = int(round(sec * 1000))
    h = ms // 3600000
    ms %= 3600000
    m = ms // 60000
    ms %= 60000
    s = ms // 1000
    ms %= 1000

    return f"{h:02}:{m:02}:{s:02},{ms:03}"
```

**SRT æ ¼å¼ç¤ºä¾‹**:
```
1
00:00:00,000 --> 00:00:03,520
è¿™æ˜¯ç¬¬ä¸€å¥å­—å¹•

2
00:00:03,520 --> 00:00:07,120
è¿™æ˜¯ç¬¬äºŒå¥å­—å¹•
```

### 6.3 è¿›åº¦è®¡ç®—æœºåˆ¶

**ä½ç½®**: `backend/app/processor.py:282-301`

```python
# é˜¶æ®µæƒé‡é…ç½®
PHASE_WEIGHTS = {
    "extract": 5,      # éŸ³é¢‘æå–å  5%
    "split": 10,       # éŸ³é¢‘åˆ†æ®µå  10%
    "transcribe": 80,  # è½¬å½•å¤„ç†å  80%ï¼ˆä¸»è¦è€—æ—¶ï¼‰
    "srt": 5           # SRT ç”Ÿæˆå  5%
}

def _update_progress(self, job: JobState, phase: str, ratio: float, message: str = ''):
    """æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆåŠ æƒè®¡ç®—ï¼‰"""

    # è®¡ç®—å·²å®Œæˆé˜¶æ®µçš„ç´¯è®¡æƒé‡
    done_weight = sum(PHASE_WEIGHTS[k] for k in PHASE_WEIGHTS if k < phase)

    # å½“å‰é˜¶æ®µæƒé‡
    current_weight = PHASE_WEIGHTS[phase]

    # æ€»è¿›åº¦ = å·²å®Œæˆ + (å½“å‰é˜¶æ®µ Ã— å½“å‰è¿›åº¦)
    total_progress = done_weight + current_weight * ratio

    # æ›´æ–°ä»»åŠ¡çŠ¶æ€
    job.phase = phase
    job.progress = total_progress
    if message:
        job.message = message
```

**ç¤ºä¾‹è®¡ç®—**:
```
å‡è®¾å½“å‰åœ¨ transcribe é˜¶æ®µï¼Œå·²å¤„ç† 50% çš„æ®µï¼š

done_weight = extract(5) + split(10) = 15
current_weight = transcribe(80)
ratio = 0.5

total_progress = 15 + 80 * 0.5 = 15 + 40 = 55%
```

---

## ä¸ƒã€å‰ç«¯æŠ€æœ¯å®ç°

### 7.1 ç»„ä»¶æ¶æ„

#### 7.1.1 ä¸»åº”ç”¨ç»„ä»¶ (App.vue)

**è®¾è®¡æ¨¡å¼**: å®¹å™¨-å±•ç¤ºæ¨¡å¼ (Container-Presenter Pattern)

**æ ¸å¿ƒèŒè´£**:
- é›†ä¸­çŠ¶æ€ç®¡ç†ï¼ˆæ‰€æœ‰çŠ¶æ€éƒ½åœ¨è¿™é‡Œï¼‰
- ä¸šåŠ¡é€»è¾‘å¤„ç†ï¼ˆæ–‡ä»¶ä¸Šä¼ ã€ä»»åŠ¡å¯åŠ¨ã€è½®è¯¢ç­‰ï¼‰
- å­ç»„ä»¶åè°ƒä¸æ•°æ®æµæ§åˆ¶

**çŠ¶æ€ç®¡ç†**:

```javascript
// æ–‡ä»¶é€‰æ‹©ç›¸å…³çŠ¶æ€
const availableFiles = ref([])        // å¯ç”¨æ–‡ä»¶åˆ—è¡¨
const selectedFile = ref(null)        // å½“å‰é€‰ä¸­æ–‡ä»¶
const loadingFiles = ref(false)       // åŠ è½½æ–‡ä»¶åˆ—è¡¨ä¸­
const creating = ref(false)           // åˆ›å»ºä»»åŠ¡ä¸­
const uploading = ref(false)          // ä¸Šä¼ ä¸­
const uploadProgress = ref(0)         // ä¸Šä¼ è¿›åº¦ 0-100
const showUpload = ref(false)         // æ˜¾ç¤ºä¸Šä¼ æ¨¡å¼

// è½¬å½•ä»»åŠ¡ç›¸å…³çŠ¶æ€
const jobId = ref("")                 // ä»»åŠ¡ ID
const status = ref("")                // ä»»åŠ¡çŠ¶æ€: queued/processing/finished/failed/canceled
const progress = ref(0)               // ä»»åŠ¡è¿›åº¦ 0-100
const phase = ref("")                 // å½“å‰é˜¶æ®µ: extract/split/transcribe/srt
const statusText = ref("è¯·å…ˆé€‰æ‹©æ–‡ä»¶") // çŠ¶æ€æ–‡æœ¬
const processing = ref(false)         // å¤„ç†ä¸­æ ‡å¿—
const starting = ref(false)           // å¯åŠ¨ä¸­æ ‡å¿—
const canceling = ref(false)          // å–æ¶ˆä¸­æ ‡å¿—
const canRestart = ref(false)         // å¯é‡æ–°è½¬å½•æ ‡å¿—
const pollTimer = ref(null)           // è½®è¯¢å®šæ—¶å™¨

// è½¬å½•å‚æ•°ï¼ˆreactive å“åº”å¼å¯¹è±¡ï¼‰
const settings = reactive({
  model: "medium",                    // æ¨¡å‹: tiny/base/small/medium/large
  compute_type: "float16",            // è®¡ç®—ç±»å‹: float16/float32/int8
  device: "cuda",                     // è®¾å¤‡: cuda/cpu
  batch_size: 16,                     // æ‰¹å¤§å°: 1-64
  word_timestamps: false,             // æ˜¯å¦è¾“å‡ºè¯çº§æ—¶é—´æˆ³
})
```

#### 7.1.2 ç»„ä»¶å±‚æ¬¡ç»“æ„

```
App.vue (æ ¹ç»„ä»¶ - çŠ¶æ€ç®¡ç†ä¸­å¿ƒ)
  â”‚
  â”œâ”€> FileSelector (æ–‡ä»¶é€‰æ‹©ç»„ä»¶)
  â”‚     â”œâ”€ Props: availableFiles, selectedFile, showUpload, uploadProgress, ...
  â”‚     â””â”€ Emits: select-file, upload-file, refresh-files, toggle-mode, ...
  â”‚
  â”œâ”€> TranscriptionSettings (å‚æ•°è®¾ç½®ç»„ä»¶)
  â”‚     â”œâ”€ Props: settings, processing, starting, canRestart, ...
  â”‚     â”œâ”€ Emits: start-transcription, restart-transcription
  â”‚     â””â”€> ModelStatusButton (æ¨¡å‹çŠ¶æ€æŒ‰é’®)
  â”‚           â”œâ”€ è‡ªä¸»çŠ¶æ€ç®¡ç†
  â”‚           â””â”€ è‡ªä¸»è½®è¯¢æ›´æ–°
  â”‚
  â”œâ”€> ProgressDisplay (è¿›åº¦æ˜¾ç¤ºç»„ä»¶)
  â”‚     â”œâ”€ Props: progress, phase, status, language, downloadUrl, ...
  â”‚     â””â”€ Emits: cancel-job, copy-result
  â”‚
  â””â”€> HardwareDialog (ç¡¬ä»¶ä¿¡æ¯å¯¹è¯æ¡†)
        â””â”€ Props: visible
        â””â”€ Emits: update:visible
```

### 7.2 API æœåŠ¡å°è£…

#### 7.2.1 ä¸‰å±‚æ¶æ„

**1. åŸºç¡€å±‚ (api.js)**

```javascript
import axios from 'axios'

// åˆ›å»º Axios å®ä¾‹
const apiClient = axios.create({
  baseURL: '/api',           // æ‰€æœ‰è¯·æ±‚å‰ç¼€ /apiï¼ˆç”± Vite ä»£ç†åˆ°åç«¯ï¼‰
  timeout: 300000,           // 5 åˆ†é’Ÿè¶…æ—¶ï¼ˆè½¬å½•æ“ä½œå¾ˆæ…¢ï¼‰
  headers: {
    'Content-Type': 'application/json'
  }
})

// è¯·æ±‚æ‹¦æˆªå™¨ï¼ˆå¯æ·»åŠ  token ç­‰ï¼‰
apiClient.interceptors.request.use(
  config => config,
  error => Promise.reject(error)
)

// å“åº”æ‹¦æˆªå™¨ï¼ˆç»Ÿä¸€é”™è¯¯å¤„ç†ï¼‰
apiClient.interceptors.response.use(
  response => response,
  error => {
    console.error('APIè¯·æ±‚å¤±è´¥:', error)
    return Promise.reject(error)
  }
)

export default apiClient
```

**2. æœåŠ¡å±‚ (Service Classes)**

**FileService (fileService.js)**:
```javascript
import apiClient from './api'

export class FileService {
  /**
   * è·å– input ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨
   */
  static async getFiles() {
    const response = await apiClient.get('/files')
    return response.data
  }

  /**
   * åˆ é™¤æ–‡ä»¶
   */
  static async deleteFile(filename) {
    const response = await apiClient.delete(`/files/${filename}`)
    return response.data
  }

  /**
   * å®¢æˆ·ç«¯æ–‡ä»¶ç±»å‹éªŒè¯
   */
  static isSupportedFile(filename) {
    const lowerName = filename.toLowerCase()
    const videoExtensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mpg', '.mpeg', '.3gp', '.ts', '.vob', '.ogv', '.divx']
    const audioExtensions = ['.mp3', '.wav', '.flac', '.aac', '.ogg', '.m4a', '.wma', '.opus', '.ape']
    const supportedExtensions = [...videoExtensions, ...audioExtensions]
    return supportedExtensions.some(ext => lowerName.endsWith(ext))
  }
}
```

**TranscriptionService (transcriptionService.js)**:
```javascript
import apiClient from './api'

export class TranscriptionService {
  /**
   * ä¸Šä¼ æ–‡ä»¶å¹¶åˆ›å»ºä»»åŠ¡
   */
  static async uploadFile(file, onProgress) {
    const formData = new FormData()
    formData.append('file', file)

    const response = await apiClient.post('/upload', formData, {
      headers: { 'Content-Type': 'multipart/form-data' },
      onUploadProgress: (progressEvent) => {
        if (onProgress) {
          onProgress(progressEvent)  // è¿›åº¦å›è°ƒ
        }
      }
    })
    return response.data
  }

  /**
   * ä¸ºæœ¬åœ°æ–‡ä»¶åˆ›å»ºä»»åŠ¡
   */
  static async createJob(filename) {
    const response = await apiClient.post('/create-job', { filename })
    return response.data
  }

  /**
   * å¯åŠ¨è½¬å½•ä»»åŠ¡
   */
  static async startJob(jobId, settings) {
    const formData = new FormData()
    formData.append('job_id', jobId)
    formData.append('settings', JSON.stringify(settings))

    const response = await apiClient.post('/start', formData)
    return response.data
  }

  /**
   * æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
   */
  static async getJobStatus(jobId) {
    const response = await apiClient.get(`/status/${jobId}`)
    return response.data
  }

  /**
   * å–æ¶ˆä»»åŠ¡
   */
  static async cancelJob(jobId) {
    const response = await apiClient.post(`/cancel/${jobId}`)
    return response.data
  }

  /**
   * è·å–ä¸‹è½½ URL
   */
  static getDownloadUrl(jobId) {
    return `/api/download/${jobId}`
  }

  /**
   * å¤åˆ¶ç»“æœåˆ°æºç›®å½•
   */
  static async copyResult(jobId) {
    const response = await apiClient.post(`/copy-result/${jobId}`)
    return response.data
  }
}
```

#### 7.2.2 Vite ä»£ç†é…ç½®

**vite.config.js**:
```javascript
export default defineConfig({
  server: {
    port: 5174,
    proxy: {
      '/api': {
        target: 'http://127.0.0.1:8000',  // åç«¯åœ°å€
        changeOrigin: true,
        ws: true,  // æ”¯æŒ WebSocket
        secure: false,
        configure: (proxy) => {
          proxy.on('error', (err) => {
            console.error('[proxy error]', err.message)
          })
          proxy.on('proxyReq', (proxyReq, req, res) => {
            console.log('[proxy request]', req.method, req.url, '->', proxyReq.path)
          })
        }
      }
    }
  }
})
```

### 7.3 å®æ—¶è¿›åº¦æ›´æ–°æœºåˆ¶

#### 7.3.1 è‡ªé€‚åº”è½®è¯¢

**å®ç°ä½ç½®**: `frontend/src/App.vue`

```javascript
/**
 * è½®è¯¢ä»»åŠ¡çŠ¶æ€
 * - æ­£å¸¸æƒ…å†µ: 1.5s é—´éš”
 * - ç½‘ç»œé”™è¯¯: 2.5s é—´éš”ï¼ˆè‡ªåŠ¨é‡è¯•ï¼‰
 */
async function poll() {
  clearTimeout(pollTimer.value)

  if (!jobId.value) return

  try {
    const data = await TranscriptionService.getJobStatus(jobId.value)

    // æ›´æ–°çŠ¶æ€
    status.value = data.status
    progress.value = data.progress || 0
    statusText.value = data.message || data.status
    phase.value = data.phase || ""
    language.value = data.language || ""

    // ä»»åŠ¡å®Œæˆ
    if (status.value === "finished") {
      processing.value = false
      downloadUrl.value = TranscriptionService.getDownloadUrl(jobId.value)
      canRestart.value = true
      ElMessage.success('è½¬å½•å®Œæˆï¼')
      return  // åœæ­¢è½®è¯¢
    }

    // ä»»åŠ¡å¤±è´¥æˆ–å–æ¶ˆ
    if (status.value === "failed" || status.value === "canceled") {
      processing.value = false
      canRestart.value = true
      return  // åœæ­¢è½®è¯¢
    }

    // ç»§ç»­è½®è¯¢
    pollTimer.value = setTimeout(poll, 1500)  // 1.5ç§’åå†æ¬¡æŸ¥è¯¢

  } catch (e) {
    console.error('è½®è¯¢å¤±è´¥:', e)
    // ç½‘ç»œé”™è¯¯: å»¶é•¿é—´éš”åé‡è¯•
    pollTimer.value = setTimeout(poll, 2500)  // 2.5ç§’åé‡è¯•
  }
}
```

**ç‰¹ç‚¹**:
- âœ… **å¤±è´¥é‡è¯•**: ç½‘ç»œé”™è¯¯è‡ªåŠ¨é‡è¯•ï¼Œä¸ä¸­æ–­ç”¨æˆ·ä½“éªŒ
- âœ… **è‡ªé€‚åº”é—´éš”**: æ­£å¸¸ 1.5sï¼Œé”™è¯¯æ—¶ 2.5s
- âœ… **ä¼˜é›…åœæ­¢**: ä»»åŠ¡å®Œæˆæˆ–å¤±è´¥æ—¶è‡ªåŠ¨åœæ­¢è½®è¯¢
- âœ… **èµ„æºæ¸…ç†**: ç»„ä»¶å¸è½½æ—¶æ¸…é™¤å®šæ—¶å™¨

```javascript
onUnmounted(() => {
  if (pollTimer.value) {
    clearTimeout(pollTimer.value)
  }
})
```

#### 7.3.2 æ¨¡å‹é¢„åŠ è½½çš„é«˜çº§è½®è¯¢

**å®ç°ä½ç½®**: `frontend/src/components/models/ModelStatusButton.vue`

```javascript
/**
 * æ ¹æ®çŠ¶æ€è®¡ç®—è½®è¯¢é—´éš”
 */
function getAdaptiveInterval() {
  if (modelStatus.is_preloading) {
    return 1500  // é¢„åŠ è½½ä¸­: é«˜é¢‘æ›´æ–° (1.5s)
  }
  if (modelStatus.loaded_models > 0) {
    return 15000  // å·²åŠ è½½æ¨¡å‹: ä½é¢‘æ›´æ–° (15s)
  }
  return 5000  // é»˜è®¤: ä¸­é¢‘æ›´æ–° (5s)
}

/**
 * è‡ªé€‚åº”è½®è¯¢
 */
function startAdaptivePolling() {
  const poll = async () => {
    try {
      await updateModelStatus()
      const nextInterval = getAdaptiveInterval()
      pollTimer = setTimeout(poll, nextInterval)
    } catch (error) {
      console.error('æ¨¡å‹çŠ¶æ€æ›´æ–°å¤±è´¥:', error)
      pollTimer = setTimeout(poll, 8000)  // é”™è¯¯æ—¶é™ä½é¢‘ç‡
    }
  }
  poll()  // ç«‹å³æ‰§è¡Œä¸€æ¬¡
}
```

**æ™ºèƒ½ç‰¹æ€§**:
- âœ… **ä¸‰çº§è½®è¯¢é¢‘ç‡**: é¢„åŠ è½½ä¸­ 1.5s / ç©ºé—² 5s / å·²åŠ è½½ 15s
- âœ… **çŠ¶æ€å˜åŒ–æ£€æµ‹**: ä½¿ç”¨ `cache_version` æ£€æµ‹æ•°æ®å˜åŒ–
- âœ… **å†…å­˜ä¼˜åŒ–**: ç©ºé—²æ—¶é™ä½è½®è¯¢é¢‘ç‡ï¼ŒèŠ‚çœèµ„æº

### 7.4 ç»„ä»¶é€šä¿¡æœºåˆ¶

#### 7.4.1 Props Down / Events Up

**FileSelector ç»„ä»¶ç¤ºä¾‹**:

```vue
<script setup>
// Props å®šä¹‰ï¼ˆçˆ¶ç»„ä»¶ä¼ å…¥ï¼‰
const props = defineProps({
  showUpload: Boolean,
  availableFiles: Array,
  selectedFile: Object,
  loadingFiles: Boolean,
  creating: Boolean,
  uploading: Boolean,
  uploadProgress: Number,
  inputDirPath: String,
  jobId: String
})

// Events å®šä¹‰ï¼ˆå‘çˆ¶ç»„ä»¶å‘é€ï¼‰
const emits = defineEmits([
  'toggle-mode',
  'refresh-files',
  'select-file',
  'clear-selection',
  'create-job',
  'upload-file'
])

// ä½¿ç”¨äº‹ä»¶
function handleSelectFile(file) {
  emits('select-file', file)
}
</script>
```

**ä¼˜åŠ¿**: å•å‘æ•°æ®æµï¼Œæ•°æ®æµåŠ¨è·¯å¾„æ¸…æ™°ï¼Œæ˜“äºè°ƒè¯•å’Œç»´æŠ¤

---

## å…«ã€åç«¯æŠ€æœ¯å®ç°

### 8.1 FastAPI åº”ç”¨æ¶æ„

#### 8.1.1 åº”ç”¨å…¥å£ (main.py)

**å…³é”®ç‰¹æ€§**:

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import os

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="Video-to-SRT API", version="2.0")

# CORS ä¸­é—´ä»¶ï¼ˆå…è®¸è·¨åŸŸï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å¼€å‘ç¯å¢ƒå…è®¸æ‰€æœ‰æ¥æº
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
ROOT = os.path.dirname(os.path.abspath(__file__))
BASE_DIR = os.path.dirname(os.path.dirname(ROOT))
processor = None
_model_manager = None

# å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    global processor, _model_manager

    # åˆå§‹åŒ–ç›®å½•
    os.makedirs(os.path.join(BASE_DIR, "input"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "output"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "jobs"), exist_ok=True)
    os.makedirs(os.path.join(BASE_DIR, "temp"), exist_ok=True)

    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = TranscriptionProcessor(BASE_DIR)

    # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
    _model_manager = ModelPreloadManager()
    processor.set_model_manager(_model_manager)

    # å»¶è¿Ÿ 10 ç§’è‡ªåŠ¨é¢„åŠ è½½ï¼ˆç­‰å¾…å‰ç«¯å°±ç»ªï¼‰
    import threading
    def delayed_preload():
        import time
        time.sleep(10)
        if os.getenv("AUTO_PRELOAD_MODELS", "true").lower() != "false":
            asyncio.run(_model_manager.preload_models())

    threading.Thread(target=delayed_preload, daemon=True).start()

# å…³é—­äº‹ä»¶
@app.on_event("shutdown")
async def shutdown_event():
    global _model_manager
    if _model_manager:
        _model_manager.clear_cache()  # æ¸…ç†æ¨¡å‹ç¼“å­˜ï¼Œé‡Šæ”¾å†…å­˜
```

#### 8.1.2 API ç«¯ç‚¹åˆ†ç±»

**è½¬å½•ä»»åŠ¡ç®¡ç†**:
- `POST /api/upload` - ä¸Šä¼ æ–‡ä»¶å¹¶åˆ›å»ºä»»åŠ¡
- `POST /api/create-job` - ä¸ºæœ¬åœ°æ–‡ä»¶åˆ›å»ºä»»åŠ¡
- `POST /api/start` - å¯åŠ¨è½¬å½•
- `POST /api/cancel/{job_id}` - å–æ¶ˆä»»åŠ¡
- `GET /api/status/{job_id}` - æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
- `GET /api/download/{job_id}` - ä¸‹è½½å­—å¹•
- `POST /api/copy-result/{job_id}` - å¤åˆ¶å­—å¹•åˆ°æºç›®å½•

**æ–‡ä»¶ç®¡ç†**:
- `GET /api/files` - åˆ—å‡º input ç›®å½•æ–‡ä»¶
- `DELETE /api/files/{filename}` - åˆ é™¤æ–‡ä»¶

**ç¡¬ä»¶æ£€æµ‹**:
- `GET /api/hardware/basic` - è·å–ç¡¬ä»¶ä¿¡æ¯
- `GET /api/hardware/optimize` - è·å–ä¼˜åŒ–é…ç½®
- `GET /api/hardware/status` - è·å–å®Œæ•´ç¡¬ä»¶çŠ¶æ€

**æ¨¡å‹ç®¡ç†**:
- `GET /api/models/preload/status` - é¢„åŠ è½½çŠ¶æ€
- `GET /api/models/cache/status` - ç¼“å­˜çŠ¶æ€
- `POST /api/models/preload/start` - å¯åŠ¨é¢„åŠ è½½
- `POST /api/models/cache/clear` - æ¸…ç©ºç¼“å­˜
- `POST /api/models/preload/reset` - é‡ç½®å¤±è´¥è®¡æ•°

**ç³»ç»Ÿæ§åˆ¶**:
- `GET /api/ping` - å¥åº·æ£€æŸ¥
- `GET /api/cpu-info` - CPU ä¿¡æ¯
- `POST /api/shutdown` - ä¼˜é›…å…³é—­

### 8.2 æ¨¡å‹é¢„åŠ è½½å’Œç¼“å­˜æœºåˆ¶

#### 8.2.1 æ ¸å¿ƒæ•°æ®ç»“æ„

**ä½ç½®**: `backend/app/services/model_preload_manager.py`

```python
from dataclasses import dataclass
from collections import OrderedDict
import threading
import time

@dataclass
class ModelCacheInfo:
    """æ¨¡å‹ç¼“å­˜ä¿¡æ¯"""
    model: Any                          # æ¨¡å‹å¯¹è±¡
    key: Tuple[str, str, str]           # (model_name, compute_type, device)
    load_time: float                    # åŠ è½½è€—æ—¶(ç§’)
    last_used: float                    # æœ€åä½¿ç”¨æ—¶é—´æˆ³
    memory_size: int                    # ä¼°ç®—å†…å­˜å ç”¨(MB)

class ModelPreloadManager:
    def __init__(self):
        # LRU ç¼“å­˜å®¹å™¨
        self._whisper_cache: OrderedDict[Tuple[str, str, str], ModelCacheInfo] = OrderedDict()
        self._align_cache: OrderedDict[str, Tuple[model, meta, last_used]] = OrderedDict()

        # ç»Ÿä¸€é”æœºåˆ¶ï¼ˆé¿å…å¤šé”æ­»é”ï¼‰
        self._global_lock = threading.RLock()

        # ç¼“å­˜é…ç½®
        self.max_cache_size = 3  # æœ€å¤šç¼“å­˜ 3 ä¸ªæ¨¡å‹

        # é¢„åŠ è½½çŠ¶æ€
        self._preload_status = {
            "is_preloading": False,
            "progress": 0.0,
            "current_model": "",
            "loaded_models": [],
            "failed_models": [],
            "failed_attempts": 0,
            "max_retry_attempts": 3,
            "cache_version": time.time()  # ç¼“å­˜ç‰ˆæœ¬å·
        }

        # å†…å­˜ç›‘æ§å™¨
        self.memory_monitor = MemoryMonitor()
```

#### 8.2.2 LRU ç¼“å­˜ç­–ç•¥

```python
def get_model(self, settings: JobSettings):
    """è·å– Whisper æ¨¡å‹ï¼ˆLRU ç¼“å­˜ï¼‰"""
    key = (settings.model, settings.compute_type, settings.device)

    with self._global_lock:
        # ç¼“å­˜å‘½ä¸­
        if key in self._whisper_cache:
            info = self._whisper_cache[key]
            info.last_used = time.time()
            # ç§»åˆ°é˜Ÿåˆ—æœ«å°¾ï¼ˆæ ‡è®°ä¸ºæœ€è¿‘ä½¿ç”¨ï¼‰
            self._whisper_cache.move_to_end(key)
            return info.model

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦åŠ è½½
        return self._load_whisper_model(settings)

def _load_whisper_model(self, settings: JobSettings):
    """åŠ è½½ Whisper æ¨¡å‹"""
    with self._global_lock:
        # æ£€æŸ¥ç¼“å­˜å¤§å°ï¼Œé©±é€ LRU æ¨¡å‹
        while len(self._whisper_cache) >= self.max_cache_size:
            self._evict_lru_model()

        # æ£€æŸ¥å†…å­˜æ˜¯å¦å……è¶³
        if not self.memory_monitor.check_memory_available():
            raise RuntimeError("ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")

        # åŠ è½½æ¨¡å‹
        start_time = time.time()
        model = whisperx.load_model(
            settings.model,
            settings.device,
            compute_type=settings.compute_type
        )
        load_time = time.time() - start_time

        # é¢„çƒ­æ¨¡å‹ï¼ˆç©ºè·‘ä¸€æ¬¡ï¼‰
        self._warmup_model(model)

        # ä¼°ç®—å†…å­˜å ç”¨
        memory_size = self._estimate_model_size(settings.model)

        # ç¼“å­˜æ¨¡å‹
        info = ModelCacheInfo(
            model=model,
            key=(settings.model, settings.compute_type, settings.device),
            load_time=load_time,
            last_used=time.time(),
            memory_size=memory_size
        )
        self._whisper_cache[key] = info

        # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
        self._preload_status["cache_version"] = time.time()

        return model

def _evict_lru_model(self):
    """é©±é€æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹"""
    # OrderedDict çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯æœ€ä¹…æœªä½¿ç”¨çš„
    oldest_key = next(iter(self._whisper_cache))
    info = self._whisper_cache.pop(oldest_key)

    # é‡Šæ”¾æ¨¡å‹å†…å­˜
    del info.model
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # æ›´æ–°ç¼“å­˜ç‰ˆæœ¬å·
    self._preload_status["cache_version"] = time.time()
```

#### 8.2.3 é¢„åŠ è½½å¹‚ç­‰æ€§å®ç°

```python
async def preload_models(self, progress_callback=None):
    """é¢„åŠ è½½æ¨¡å‹ï¼ˆå¹‚ç­‰æ€§ä¿è¯ï¼‰"""

    with self._global_lock:
        # å¹‚ç­‰æ€§æ£€æŸ¥ - é˜²æ­¢é‡å¤é¢„åŠ è½½
        if self._preload_status["is_preloading"]:
            return {"success": True, "message": "é¢„åŠ è½½å·²åœ¨è¿›è¡Œä¸­"}

        # æ£€æŸ¥å¤±è´¥æ¬¡æ•°
        if self._preload_status["failed_attempts"] >= self._preload_status["max_retry_attempts"]:
            return {"success": False, "message": "é¢„åŠ è½½å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå·²åœæ­¢"}

        # è®¾ç½®é¢„åŠ è½½æ ‡å¿—
        self._preload_status["is_preloading"] = True
        self._preload_status["progress"] = 0.0
        self._preload_status["current_model"] = ""

    try:
        for idx, model_name in enumerate(self.config.default_models):
            # æ£€æŸ¥æ˜¯å¦å·²ç¼“å­˜
            key = (model_name, self.config.compute_type, self.config.device)
            if key in self._whisper_cache:
                continue  # è·³è¿‡å·²åŠ è½½çš„æ¨¡å‹

            # æ›´æ–°è¿›åº¦
            self._preload_status["current_model"] = model_name
            if progress_callback:
                progress_callback(idx / len(self.config.default_models), model_name)

            # åŠ è½½æ¨¡å‹
            settings = JobSettings(
                model=model_name,
                compute_type=self.config.compute_type,
                device=self.config.device
            )
            model = self._load_whisper_model(settings)

            # è®°å½•å·²åŠ è½½
            self._preload_status["loaded_models"].append(model_name)

        # é¢„åŠ è½½æˆåŠŸ
        self._preload_status["progress"] = 1.0
        return {"success": True, "message": "é¢„åŠ è½½å®Œæˆ"}

    except Exception as e:
        # é¢„åŠ è½½å¤±è´¥
        self._preload_status["failed_attempts"] += 1
        self._preload_status["failed_models"].append(self._preload_status["current_model"])
        return {"success": False, "message": f"é¢„åŠ è½½å¤±è´¥: {str(e)}"}

    finally:
        # æ¸…é™¤é¢„åŠ è½½æ ‡å¿—
        self._preload_status["is_preloading"] = False
        self._preload_status["cache_version"] = time.time()
```

### 8.3 CPU äº²å’Œæ€§ä¼˜åŒ–

#### 8.3.1 äº²å’Œæ€§ç®¡ç†å™¨

**ä½ç½®**: `backend/app/processor.py:42-167`

```python
import psutil
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class CPUAffinityConfig:
    """CPU äº²å’Œæ€§é…ç½®"""
    enabled: bool = False                     # æ˜¯å¦å¯ç”¨
    strategy: str = "auto"                    # ç­–ç•¥: auto/half/custom
    custom_cores: List[int] = field(default_factory=list)  # è‡ªå®šä¹‰æ ¸å¿ƒåˆ—è¡¨
    exclude_cores: List[int] = field(default_factory=list)  # æ’é™¤æ ¸å¿ƒåˆ—è¡¨

class CPUAffinityManager:
    """CPU æ ¸å¿ƒç»‘å®šç®¡ç†å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.original_affinity = None  # ä¿å­˜åŸå§‹è®¾ç½®

    def apply_cpu_affinity(self, config: CPUAffinityConfig) -> bool:
        """åº”ç”¨ CPU äº²å’Œæ€§è®¾ç½®"""
        try:
            # 1. ä¿å­˜åŸå§‹è®¾ç½®
            self.original_affinity = psutil.Process().cpu_affinity()

            # 2. è®¡ç®—ç›®æ ‡æ ¸å¿ƒ
            target_cores = self.calculate_optimal_cores(
                strategy=config.strategy,
                custom_cores=config.custom_cores,
                exclude_cores=config.exclude_cores
            )

            if not target_cores:
                return False

            # 3. åº”ç”¨ç»‘å®š
            psutil.Process().cpu_affinity(target_cores)
            self.logger.info(f"å·²åº”ç”¨ CPU äº²å’Œæ€§: {target_cores}")
            return True

        except Exception as e:
            self.logger.error(f"åº”ç”¨ CPU äº²å’Œæ€§å¤±è´¥: {e}")
            return False

    def restore_cpu_affinity(self) -> bool:
        """æ¢å¤åŸå§‹ CPU äº²å’Œæ€§è®¾ç½®"""
        if self.original_affinity is None:
            return False

        try:
            psutil.Process().cpu_affinity(self.original_affinity)
            self.logger.info(f"å·²æ¢å¤ CPU äº²å’Œæ€§: {self.original_affinity}")
            return True
        except Exception as e:
            self.logger.error(f"æ¢å¤ CPU äº²å’Œæ€§å¤±è´¥: {e}")
            return False

    def calculate_optimal_cores(
        self,
        strategy: str = "auto",
        custom_cores: List[int] = None,
        exclude_cores: List[int] = None
    ) -> List[int]:
        """è®¡ç®—æœ€ä¼˜æ ¸å¿ƒåˆ†é…"""

        # è·å–æ‰€æœ‰å¯ç”¨æ ¸å¿ƒ
        all_cores = list(range(psutil.cpu_count()))
        available_cores = [c for c in all_cores if c not in (exclude_cores or [])]
        cpu_count = len(available_cores)

        if strategy == "custom":
            # ç”¨æˆ·è‡ªå®šä¹‰æ ¸å¿ƒåˆ—è¡¨
            if not custom_cores:
                return []
            return [c for c in custom_cores if c in available_cores]

        elif strategy == "half":
            # ä½¿ç”¨å‰ 50% çš„æ ¸å¿ƒ
            half_count = max(1, cpu_count // 2)
            return available_cores[:half_count]

        else:  # "auto" é»˜è®¤ç­–ç•¥
            if cpu_count <= 4:
                # ä½ç«¯ç³»ç»Ÿ: ä½¿ç”¨å…¨éƒ¨æ ¸å¿ƒ
                return available_cores
            elif cpu_count <= 8:
                # ä¸­ç«¯ç³»ç»Ÿ: ç•™ 1 ä¸ªæ ¸å¿ƒç»™ç³»ç»Ÿ
                return available_cores[:-1]
            else:
                # é«˜ç«¯å¤šæ ¸: ä½¿ç”¨å‰ 75% æ ¸å¿ƒ
                use_count = int(cpu_count * 0.75)
                return available_cores[:use_count]
```

#### 8.3.2 ä»»åŠ¡çº§åˆ«åº”ç”¨

```python
def _run_pipeline(self, job: JobState):
    """è½¬å½•å¤„ç†æµç¨‹ï¼ˆåº”ç”¨ CPU äº²å’Œæ€§ï¼‰"""

    # ä»»åŠ¡å¼€å§‹å‰åº”ç”¨ CPU äº²å’Œæ€§
    cpu_applied = False
    if job.settings.cpu_affinity.enabled:
        cpu_applied = self.cpu_manager.apply_cpu_affinity(job.settings.cpu_affinity)
        if cpu_applied:
            self.logger.info(f"ä»»åŠ¡ {job.job_id} å·²åº”ç”¨ CPU äº²å’Œæ€§è®¾ç½®")

    try:
        # æ‰§è¡Œè½¬å½•æµç¨‹...
        self._extract_audio(...)
        self._split_audio(...)
        self._transcribe_segment(...)
        self._generate_srt(...)

    finally:
        # ä»»åŠ¡ç»“æŸåæ¢å¤ CPU äº²å’Œæ€§
        if cpu_applied:
            restored = self.cpu_manager.restore_cpu_affinity()
            if restored:
                self.logger.info(f"ä»»åŠ¡ {job.job_id} å·²æ¢å¤ CPU äº²å’Œæ€§è®¾ç½®")

        # æ¸…ç†å†…å­˜
        gc.collect()
```

**ä¼˜åŒ–æ•ˆæœ**:
- âœ… **å‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢**: ç»‘å®šç‰¹å®šæ ¸å¿ƒé¿å…çº¿ç¨‹è¿ç§»
- âœ… **ç¼“å­˜å‹å¥½**: L1/L2/L3 ç¼“å­˜å‘½ä¸­ç‡æå‡
- âœ… **é¿å… NUMA è·¨èŠ‚ç‚¹è®¿é—®**: åœ¨å¤šè·¯æœåŠ¡å™¨ä¸Šæ•ˆæœæ˜¾è‘—

### 8.4 ç¡¬ä»¶æ£€æµ‹æœåŠ¡

**ä½ç½®**: `backend/app/services/hardware_service.py`

```python
class CoreHardwareDetector:
    """æ ¸å¿ƒç¡¬ä»¶æ£€æµ‹å™¨"""

    def detect(self) -> HardwareInfo:
        """å¹¶è¡Œæ£€æµ‹å„ç¡¬ä»¶ç»„ä»¶"""
        gpu_info = self._detect_gpu()
        cpu_info = self._detect_cpu()
        memory_info = self._detect_memory()
        storage_info = self._detect_storage()

        return HardwareInfo(
            # GPU
            gpu_count=gpu_info["gpu_count"],
            gpu_name=gpu_info.get("gpu_name", "N/A"),
            gpu_memory_mb=gpu_info["gpu_memory_mb"],
            cuda_available=gpu_info["cuda_available"],
            cuda_version=gpu_info.get("cuda_version", "N/A"),

            # CPU
            cpu_cores=cpu_info["cpu_cores"],
            cpu_threads=cpu_info["cpu_threads"],
            cpu_name=cpu_info["cpu_name"],

            # Memory
            memory_total_mb=memory_info["memory_total_mb"],
            memory_available_mb=memory_info["memory_available_mb"],

            # Storage
            temp_space_available_gb=storage_info["temp_space_available_gb"]
        )

    def _detect_gpu(self) -> Dict:
        """GPU æ£€æµ‹"""
        if not torch.cuda.is_available():
            return {
                "cuda_available": False,
                "gpu_count": 0,
                "gpu_memory_mb": []
            }

        gpu_count = torch.cuda.device_count()
        gpu_memory_list = []
        gpu_names = []

        for i in range(gpu_count):
            device_props = torch.cuda.get_device_properties(i)
            memory_mb = device_props.total_memory // (1024 * 1024)
            gpu_name = device_props.name

            gpu_memory_list.append(memory_mb)
            gpu_names.append(gpu_name)

        return {
            "cuda_available": True,
            "gpu_count": gpu_count,
            "gpu_memory_mb": gpu_memory_list,
            "gpu_name": gpu_names[0] if gpu_names else "N/A",
            "cuda_version": torch.version.cuda or "N/A"
        }

    def _detect_cpu(self) -> Dict:
        """CPU æ£€æµ‹"""
        import platform

        return {
            "cpu_cores": psutil.cpu_count(logical=False),  # ç‰©ç†æ ¸å¿ƒ
            "cpu_threads": psutil.cpu_count(logical=True),  # é€»è¾‘çº¿ç¨‹
            "cpu_name": platform.processor() or "N/A"
        }
```

---

## ä¹ã€æ€§èƒ½ä¼˜åŒ–æœºåˆ¶

### 9.1 æ¨¡å‹é¢„åŠ è½½

**ç­–ç•¥**: å¯åŠ¨æ—¶è‡ªåŠ¨é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹ï¼Œå‡å°‘é¦–æ¬¡è½¬å½•ç­‰å¾…æ—¶é—´

**é…ç½®**:
```python
# backend/app/config/model_config.py
DEFAULT_MODELS = ["medium"]  # é»˜è®¤é¢„åŠ è½½ medium æ¨¡å‹
```

**æ•ˆæœ**:
- âŒ **æœªé¢„åŠ è½½**: é¦–æ¬¡è½¬å½•ç­‰å¾… 30-60 ç§’ï¼ˆæ¨¡å‹åŠ è½½ï¼‰
- âœ… **é¢„åŠ è½½å**: ç«‹å³å¼€å§‹è½¬å½•ï¼Œ0 ç­‰å¾…

### 9.2 LRU æ¨¡å‹ç¼“å­˜

**ç­–ç•¥**: ä½¿ç”¨ LRUï¼ˆæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ç®—æ³•ç®¡ç†æ¨¡å‹ç¼“å­˜

**é…ç½®**:
```python
MAX_CACHE_SIZE = 3  # æœ€å¤šç¼“å­˜ 3 ä¸ªæ¨¡å‹
```

**ç¤ºä¾‹åœºæ™¯**:
```
ç¼“å­˜çŠ¶æ€: [medium, small]  (2/3)

ç”¨æˆ·è¯·æ±‚ large:
  â†’ åŠ è½½ large æ¨¡å‹
  â†’ ç¼“å­˜æ›´æ–°: [medium, small, large]  (3/3)

ç”¨æˆ·è¯·æ±‚ mediumï¼ˆå‘½ä¸­ç¼“å­˜ï¼‰:
  â†’ ç›´æ¥ä½¿ç”¨ç¼“å­˜
  â†’ ç¼“å­˜æ›´æ–°: [small, large, medium]  (3/3, medium ç§»åˆ°æœ«å°¾)

ç”¨æˆ·è¯·æ±‚ tiny:
  â†’ ç¼“å­˜å·²æ»¡ï¼Œé©±é€ smallï¼ˆæœ€ä¹…æœªä½¿ç”¨ï¼‰
  â†’ åŠ è½½ tiny æ¨¡å‹
  â†’ ç¼“å­˜æ›´æ–°: [large, medium, tiny]  (3/3)
```

### 9.3 GPU åŠ é€Ÿ

**å¯¹æ¯”æµ‹è¯•**ï¼ˆ1 å°æ—¶è§†é¢‘ï¼Œmedium æ¨¡å‹ï¼‰:

| è®¾å¤‡ | è€—æ—¶ | åŠ é€Ÿæ¯” |
|------|------|--------|
| **CPU** (Intel i7-12700) | ~45 åˆ†é’Ÿ | 1x |
| **GPU** (RTX 4060 Laptop) | ~4 åˆ†é’Ÿ | **11x** |

**æ˜¾å­˜ä¼˜åŒ–**:
- è‡ªåŠ¨é€‰æ‹©è®¡ç®—ç²¾åº¦: `float16` (é»˜è®¤) / `float32` / `int8`
- åŠæ—¶æ¸…ç†æ˜¾å­˜: `torch.cuda.empty_cache()`

### 9.4 æ™ºèƒ½éŸ³é¢‘åˆ†æ®µ

**å¯¹æ¯”**:

| ç­–ç•¥ | åˆ†æ®µæ–¹å¼ | é—®é¢˜ |
|------|---------|------|
| **å›ºå®šåˆ†æ®µ** | æ¯ 60 ç§’å¼ºåˆ¶åˆ†å‰² | å¯èƒ½åœ¨å¥å­ä¸­é—´æ–­è£‚ï¼Œå½±å“è½¬å½•è´¨é‡ |
| **æ™ºèƒ½åˆ†æ®µ** | åœ¨é™éŸ³ç‚¹åˆ†å‰² | ä¿æŒå¥å­å®Œæ•´æ€§ï¼Œæé«˜è½¬å½•è´¨é‡ |

**ç¤ºä¾‹**:
```
åŸå§‹éŸ³é¢‘: "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„å¥å­ï¼Œå®ƒéœ€è¦......"

å›ºå®šåˆ†æ®µ (60s):
  æ®µ1: "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„å¥å­ï¼Œå®ƒéœ€è¦"  âŒ å¥å­æ–­è£‚
  æ®µ2: "......"

æ™ºèƒ½åˆ†æ®µ (é™éŸ³ç‚¹):
  æ®µ1: "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„å¥å­ï¼Œå®ƒéœ€è¦......"  âœ… å¥å­å®Œæ•´
  æ®µ2: (ä¸‹ä¸€å¥å®Œæ•´å¥å­)
```

### 9.5 å¹¶å‘ä»»åŠ¡å¤„ç†

**ç­–ç•¥**: æ¯ä¸ªè½¬å½•ä»»åŠ¡åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œ

```python
def start_job(self, job_id: str):
    job = self.get_job(job_id)
    job.status = "processing"

    # ç‹¬ç«‹çº¿ç¨‹æ‰§è¡Œ
    threading.Thread(
        target=self._run_pipeline,
        args=(job,),
        daemon=True
    ).start()
```

**æ•ˆæœ**:
- âœ… å¤šä»»åŠ¡å¹¶è¡Œè¿è¡Œï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æº
- âœ… ä»»åŠ¡éš”ç¦»ï¼Œäº’ä¸å½±å“

### 9.6 å†…å­˜ç›‘æ§

**ç­–ç•¥**: å®æ—¶ç›‘æ§ç³»ç»Ÿå†…å­˜ï¼Œè¶…è¿‡é˜ˆå€¼æ‹’ç»åŠ è½½æ¨¡å‹

```python
class MemoryMonitor:
    def check_memory_available(self, threshold: float = 0.85) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡æ˜¯å¦ä½äº 85%"""
        memory = psutil.virtual_memory()
        return memory.percent < (threshold * 100)
```

**æ•ˆæœ**:
- âœ… é˜²æ­¢ OOM (Out of Memory)
- âœ… ä¿æŠ¤ç³»ç»Ÿç¨³å®šæ€§

---

## åã€éƒ¨ç½²ä¸è¿ç»´

### 10.1 ç³»ç»Ÿè¦æ±‚

#### åŸºç¡€è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Windows 10/11, Linux, macOS
- **Python**: 3.8+
- **Node.js**: 16+
- **FFmpeg**: å·²å®‰è£…å¹¶åœ¨ PATH ä¸­
- **å†…å­˜**: å»ºè®® 8GB+

#### GPU åŠ é€Ÿè¦æ±‚ï¼ˆå¯é€‰ï¼‰
- **NVIDIA GPU**: æ”¯æŒ CUDA 11.8+
- **æ˜¾å­˜**: å»ºè®® 4GB+
- **é©±åŠ¨**: æœ€æ–° NVIDIA é©±åŠ¨
- **cuDNN**: ä¸ CUDA ç‰ˆæœ¬åŒ¹é…

### 10.2 å®‰è£…éƒ¨ç½²

#### 1. å…‹éš†é¡¹ç›®
```bash
git clone https://github.com/your-repo/video_to_srt_gpu.git
cd video_to_srt_gpu
```

#### 2. å®‰è£…åç«¯ä¾èµ–
```bash
pip install -r requirements.txt
```

#### 3. å®‰è£…å‰ç«¯ä¾èµ–
```bash
cd frontend
npm install
```

#### 4. é…ç½® FFmpeg
```bash
# Windows
# ä¸‹è½½ FFmpeg å¹¶æ·»åŠ åˆ° PATH

# Linux
sudo apt-get install ffmpeg

# macOS
brew install ffmpeg
```

### 10.3 å¯åŠ¨æ–¹å¼

#### æ–¹å¼ 1: ä¸€é”®å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰

**Windows**:
```bash
åŒå‡»: ä¸€é”®å¯åŠ¨_ç®€åŒ–ç‰ˆ.bat
```

**åŠŸèƒ½**:
- âœ… è‡ªåŠ¨æ¸…ç†ç«¯å£å ç”¨
- âœ… å¯åŠ¨åç«¯æœåŠ¡ï¼ˆç‹¬ç«‹çª—å£ï¼‰
- âœ… å¯åŠ¨å‰ç«¯æœåŠ¡ï¼ˆç‹¬ç«‹çª—å£ï¼‰
- âœ… è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
- âœ… åå°é¢„åŠ è½½æ¨¡å‹

#### æ–¹å¼ 2: Python å¯åŠ¨å™¨ï¼ˆè·¨å¹³å°ï¼‰

```bash
python simple_launcher.py
```

**ç‰¹ç‚¹**:
- âœ… è·¨å¹³å°å…¼å®¹ï¼ˆWindows/Linux/macOSï¼‰
- âœ… è‡ªåŠ¨ä¾èµ–æ£€æŸ¥
- âœ… è¿›ç¨‹ç®¡ç†
- âœ… ä¼˜é›…å…³é—­

#### æ–¹å¼ 3: æ‰‹åŠ¨å¯åŠ¨ï¼ˆå¼€å‘æ¨¡å¼ï¼‰

```bash
# 1. å¯åŠ¨åç«¯
cd backend
python app/main.py

# 2. å¯åŠ¨å‰ç«¯ï¼ˆæ–°ç»ˆç«¯ï¼‰
cd frontend
npm run dev

# 3. è®¿é—®
http://localhost:5174
```

### 10.4 ç¯å¢ƒå˜é‡

```bash
# ç¦ç”¨è‡ªåŠ¨é¢„åŠ è½½æ¨¡å‹
export AUTO_PRELOAD_MODELS=false

# æŒ‡å®š CUDA è®¾å¤‡
export CUDA_VISIBLE_DEVICES=0
```

### 10.5 ç”Ÿäº§éƒ¨ç½²å»ºè®®

#### åç«¯
```bash
# ä½¿ç”¨ Gunicorn + Uvicorn Workers
gunicorn backend.app.main:app \
  --workers 4 \
  --worker-class uvicorn.workers.UvicornWorker \
  --bind 0.0.0.0:8000
```

#### å‰ç«¯
```bash
# æ„å»ºç”Ÿäº§ç‰ˆæœ¬
cd frontend
npm run build

# ä½¿ç”¨ Nginx æ‰˜ç®¡
# é…ç½® Nginx åå‘ä»£ç†åˆ°åç«¯
```

#### Nginx é…ç½®ç¤ºä¾‹
```nginx
server {
    listen 80;
    server_name your-domain.com;

    # å‰ç«¯é™æ€æ–‡ä»¶
    location / {
        root /path/to/frontend/dist;
        try_files $uri $uri/ /index.html;
    }

    # åç«¯ API ä»£ç†
    location /api {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### 10.6 å¸¸è§é—®é¢˜

#### Q1: æ¨¡å‹ä¸‹è½½å¤±è´¥
**A**: WhisperX é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½å› ç½‘ç»œé—®é¢˜å¤±è´¥ã€‚è§£å†³æ–¹æ¡ˆ:
1. ä½¿ç”¨ä»£ç†: `export HTTP_PROXY=http://your-proxy:port`
2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° `~/.cache/whisper/`

#### Q2: CUDA Out of Memory
**A**: æ˜¾å­˜ä¸è¶³ã€‚è§£å†³æ–¹æ¡ˆ:
1. é™ä½ `batch_size` (é»˜è®¤ 16 â†’ 8 æˆ– 4)
2. é€‰æ‹©æ›´å°çš„æ¨¡å‹ (large â†’ medium â†’ small)
3. ä½¿ç”¨ `compute_type="int8"` é™ä½ç²¾åº¦

#### Q3: FFmpeg not found
**A**: æœªå®‰è£… FFmpeg æˆ–æœªæ·»åŠ åˆ° PATHã€‚è§£å†³æ–¹æ¡ˆ:
1. å®‰è£… FFmpeg: https://ffmpeg.org/download.html
2. æ·»åŠ åˆ°ç³»ç»Ÿ PATH

#### Q4: å‰ç«¯æ— æ³•è¿æ¥åç«¯
**A**: æ£€æŸ¥:
1. åç«¯æ˜¯å¦å¯åŠ¨: `curl http://localhost:8000/api/ping`
2. ç«¯å£æ˜¯å¦è¢«å ç”¨: `netstat -ano | findstr 8000`
3. é˜²ç«å¢™æ˜¯å¦é˜»æ­¢

---

## é™„å½•

### A. å…³é”®æ–‡ä»¶è·¯å¾„ç´¢å¼•

| åŠŸèƒ½æ¨¡å— | æ–‡ä»¶è·¯å¾„ |
|---------|---------|
| **åç«¯å…¥å£** | `backend/app/main.py` |
| **æ ¸å¿ƒå¤„ç†å™¨** | `backend/app/processor.py` |
| **æ¨¡å‹ç®¡ç†** | `backend/app/services/model_preload_manager.py` |
| **ç¡¬ä»¶æ£€æµ‹** | `backend/app/services/hardware_service.py` |
| **ä»»åŠ¡æ¨¡å‹** | `backend/app/models/job_models.py` |
| **å‰ç«¯å…¥å£** | `frontend/src/main.js` |
| **ä¸»åº”ç”¨ç»„ä»¶** | `frontend/src/App.vue` |
| **API æœåŠ¡** | `frontend/src/services/*.js` |
| **å¯åŠ¨å™¨** | `simple_launcher.py` |

### B. API ç«¯ç‚¹å®Œæ•´åˆ—è¡¨

| æ–¹æ³• | ç«¯ç‚¹ | è¯´æ˜ |
|------|------|------|
| POST | `/api/upload` | ä¸Šä¼ æ–‡ä»¶ |
| POST | `/api/create-job` | åˆ›å»ºä»»åŠ¡ |
| POST | `/api/start` | å¯åŠ¨è½¬å½• |
| POST | `/api/cancel/{job_id}` | å–æ¶ˆä»»åŠ¡ |
| GET | `/api/status/{job_id}` | æŸ¥è¯¢çŠ¶æ€ |
| GET | `/api/download/{job_id}` | ä¸‹è½½å­—å¹• |
| POST | `/api/copy-result/{job_id}` | å¤åˆ¶ç»“æœ |
| GET | `/api/files` | æ–‡ä»¶åˆ—è¡¨ |
| DELETE | `/api/files/{filename}` | åˆ é™¤æ–‡ä»¶ |
| GET | `/api/hardware/basic` | ç¡¬ä»¶ä¿¡æ¯ |
| GET | `/api/hardware/optimize` | ä¼˜åŒ–é…ç½® |
| GET | `/api/hardware/status` | å®Œæ•´ç¡¬ä»¶çŠ¶æ€ |
| GET | `/api/models/preload/status` | é¢„åŠ è½½çŠ¶æ€ |
| POST | `/api/models/preload/start` | å¯åŠ¨é¢„åŠ è½½ |
| GET | `/api/models/cache/status` | ç¼“å­˜çŠ¶æ€ |
| POST | `/api/models/cache/clear` | æ¸…ç©ºç¼“å­˜ |
| POST | `/api/models/preload/reset` | é‡ç½®å¤±è´¥è®¡æ•° |
| GET | `/api/ping` | å¥åº·æ£€æŸ¥ |
| GET | `/api/cpu-info` | CPU ä¿¡æ¯ |
| POST | `/api/shutdown` | ä¼˜é›…å…³é—­ |

### C. æ€§èƒ½ä¼˜åŒ–æ¸…å•

- âœ… æ¨¡å‹é¢„åŠ è½½ï¼ˆå‡å°‘é¦–æ¬¡ç­‰å¾…ï¼‰
- âœ… LRU æ¨¡å‹ç¼“å­˜ï¼ˆè‡ªåŠ¨ç®¡ç†å†…å­˜ï¼‰
- âœ… GPU åŠ é€Ÿï¼ˆ10x+ é€Ÿåº¦æå‡ï¼‰
- âœ… æ™ºèƒ½éŸ³é¢‘åˆ†æ®µï¼ˆæé«˜è½¬å½•è´¨é‡ï¼‰
- âœ… CPU äº²å’Œæ€§ä¼˜åŒ–ï¼ˆå‡å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢ï¼‰
- âœ… å¹¶å‘ä»»åŠ¡å¤„ç†ï¼ˆå……åˆ†åˆ©ç”¨ç¡¬ä»¶ï¼‰
- âœ… å†…å­˜ç›‘æ§ï¼ˆé˜²æ­¢ OOMï¼‰
- âœ… è‡ªé€‚åº”è½®è¯¢ï¼ˆé™ä½ç½‘ç»œå¼€é”€ï¼‰

### D. é¡¹ç›®ä¼˜åŠ¿æ€»ç»“

1. **ç°ä»£åŒ–æ¶æ„** - å‰åç«¯åˆ†ç¦»ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤
2. **é«˜æ€§èƒ½å¤„ç†** - GPU åŠ é€Ÿ + æ™ºèƒ½ç¼“å­˜ + CPU ä¼˜åŒ–
3. **ç”¨æˆ·å‹å¥½** - ç›´è§‚çš„ Web ç•Œé¢ï¼Œå®æ—¶è¿›åº¦åé¦ˆ
4. **æ™ºèƒ½ä¼˜åŒ–** - è‡ªåŠ¨æ¨¡å‹é¢„åŠ è½½ï¼Œè‡ªé€‚åº”èµ„æºåˆ†é…
5. **ç¨³å®šå¯é ** - å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
6. **å¼€æºå…è´¹** - MIT åè®®ï¼Œç¤¾åŒºå‹å¥½

---

**æ–‡æ¡£ç»“æŸ**
